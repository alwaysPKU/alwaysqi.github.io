---
layout: post
title: bounding box 回归
date: 2018-03-26
categories: [论文笔记]
tags: 论文笔记
---
<!--more-->

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>


仔细看一下rcnn系列的paper的话，还是会有很多的收获的，比如bounding box的回归部分。就有挺多的细节，这些细节为什么要这么做，其实是很关键的，理解背后的原理非常重要，这个blog结合了网上的别人的解释和自己的理解。图片多来自于网上。

### 为什么要做bounding box 回归

首先明白回归预测的都是连续值，比如实数，连续值是有个好处的，即用于较准是非常方便的，看下面的这个图

![avator](/images/bbox1.png)

其中绿色的是真实的物体的位置，但是如果我们的检测器学习之后初步预测到的是红色的区域怎么办，所以需要进一步的较准，而如何操作呢？答案就是回归，
回归并不是直接去预测bbox 的真实位置，而是预测的是偏移，网络的回归部分其实学到的是一个变换，即如何从红色的区域变成绿色的区域。但是即便是加了回归，最后预测的结果也未必就是绿色的区域，如果都是那或者是过拟合了，或者就是太好了。最后经过回归较准之后，即经过红色区域的变换，而到达一个和绿色的区域比较接近的一个bbox,
见下图

![avator](/images/bbox2.png)

在这个图中，P是proposal bbox, 或者直接理解成anchor，（我理解faster-rcnn中的proposal是在anchor的基础上经过RPN预测的偏移变换之后得到的，即第一次回归，在后面的stage中又做了一次回归。）
在ssd里面就直接预测了一次偏移,即只有一次回归，所以当成anchor理解或者proposal理解都行。


### 具体如何做的bbox 回归

这里要解决几个问题

* gt 是什么？

* 网络输出是什么？

* loss是 什么？

近照paper上面，是这样定义的

![avator](/images/bbox3.png)

紧接着就有两个问题，为什么要这样弄？

`tx, ty`那样比较容易理解，见下图的解释

![avator](/images/bbox4.png)

这张图上有两个人，一个离得近，一个离得远，如果网络的回归部分直接预测的是坐标差的话`xi-ai`， 那么显然离得近的这个坐标差要大一些，离得远的坐标差就会小一些，那他们对loss的贡献就不一样，这是由于scale带来的差异，所以为了去掉这种由于scale不一致而导致的最终loss有差异，就可以采用一种“标准化”的策略，上面的定义其实就是这种想法,即转化为去学习变化的比例。

同样的问题为什么w,h那里用了log, 一个基本的知识是`|log10|=|log0.1|`,这样的话，对于伸长`10`倍和缩小`10`倍所产生的loss实际上一样的，即对Loss的贡献是一样的，让网络平等的去学习伸或者缩。举个例子，如果不取log的话，假设gt的`(tw,th)=(2,3)`， 那么这时候可能预测的这个比例是`(2.5, 3.5)`的话就比较接近了,
但是如果gt是`(0.5, 0.3333)`的话，可能预测的需要是`(0.6, 0.23333)`的时候才会显得比较接近，但是看一下，前者产生的Loss是0.5,而后者是0.1这显然不一样。
造成这种的原因是由于放大或者缩小造成的，为了让两者一致取个log就会一致了。

参考的blog有[https://blog.csdn.net/zijin0802034/article/details/77685438](https://blog.csdn.net/zijin0802034/article/details/77685438)



$$x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}$$
