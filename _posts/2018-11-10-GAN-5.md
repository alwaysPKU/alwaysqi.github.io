---
layout: post
title: GAN-5 (WGAN 论文笔记)
date: 2018-11-10 
categories: [论文笔记]
tags: 论文笔记
---
<!--more-->

### WGAN 初衷

其实使用过GAN的人应该知道，训练GAN有很多头疼的问题。例如：GAN的训练对超参数特别敏感，需要精心设计。GAN中关于生成模型和判别模型的迭代也很有问题，按照通常理解，如果判别模型训练地很好，应该对生成的提高有很大作用，但实际中恰恰相反，如果将判别模型训练地很充分，生成模型甚至会变差。那么问题出在哪里呢？

在ICLR 2017大会上有一篇口头报告论文提出了这个问题产生的机理和解决办法。问题就出在目标函数的设计上。这篇文章的作者证明，GAN的本质其实是优化真实样本分布和生成样本分布之间的差异，并最小化这个差异。特别需要指出的是，优化的目标函数是两个分布上的Jensen-Shannon距离，但这个距离有这样一个问题，如果两个分布的样本空间并不完全重合，这个距离是无法定义的。

作者接着证明了“真实分布与生成分布的样本空间并不完全重合”是一个极大概率事件，并证明在一些假设条件下，可以从理论层面推导出一些实际中遇到的现象。

既然知道了问题的关键所在，那么应该如何解决问题呢？该文章提出了一种解决方案：使用Wasserstein距离代替Jensen-Shannon距离。并依据Wasserstein距离设计了相应的算法，即WGAN。新的算法与原始GAN相比，参数更加不敏感，训练过程更加平滑。

### 关键词
这个paper里面看着很数学化，里面有我之前比较喜欢的实分析中的测度论，变差，绝对连续，等等这些等出现了。

* Jensen-Shannon距离

这即两个KL散度的一个平均，因为KL散度自己不对称，所以不是是距离，但是两个加起来作个平均就对称了.

* Wasserstein 距离

![avator](/images/wgan1.png)

光有上面的漂亮的公式还是不行的，如果实现不了的话，也没有用，如何算其梯度呢，作者也作了证明。

![avator](/images/wgan2.png)

能算梯度之后，就可以进行参数更新，进行优化了。
算法流程如图，

![avator](/images/wgan3.png)

感觉这篇paper还是很好的，总共有30多页，相比之前看的不超过10页的这个算是长的了，不过大致了解了一下其想法，里面的许多实验细节和结论还没有去细细看，需要边自己做实验然后再对着Paper看。





