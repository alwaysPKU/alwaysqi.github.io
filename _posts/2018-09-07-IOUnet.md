---
layout: post
title: IouNet 论文笔记
date: 2018-09-07 11：55 +0800
categories: [论文笔记]
tags: 论文笔记
---
<!--more-->

### Abstract

论文链接: [https://arxiv.org/abs/1807.11590](https://arxiv.org/abs/1807.11590)

现代的目标检测器大都依赖于bbox的回归和NMS。虽然有类别的概率反应类别的置信度，但是并没有关于位置的置信度的。而这会使得bbox回归在迭代的过程中退化（指的是对应的class score），或者在NMS的时候被抑制。
这篇文章提出了IOU-net，来学习predict detectedd-bbox和gt-bbox之间的IOU，然后将之前通过class score的NMS，改成了用IOU来做；此外也还提出了一种 "optimization-based bbox refinement method",即以IOU作为目的。


### Introduction
很多现代的object detectors 是依赖于two-stage frameworks，它们把物体检测作为一个多任务的学习问题，先是区别前景和背景并且给它们一个适当的Label， 然后一组系数能够最大化检测框和gt框之间的IOU，最后才采用NMS技术把重复多余的检测给去掉。
但是之前可以把每个类别的概率来作为"分类置信度"，但是并没有”位置置信度“。
![004.png]({{ site.url }}/images/004.png)

这样会有两个问题，
- 以分类置信度来作为proposals的排名的时候，并没有考虑到位置的准确性。
- 没有位置置信度的话，不太好解释广泛使用的bbox的回归
并且有实验表明bbox的回归在迭代过程中并不是单调的，也就是说可能会变差，
如图所示


![avatar](/images/005.png)


然后这里提出的解决方案是：

![avatar](/images/006.png)


即
- IOU作为位置置信度比较自然，用它作为NMS中的排名机制。
- 相比于传统地bbox回归，这里提出一种基于优化的box refinement 的过程，在inference阶段，把IOU作为优化目标，并且实验表明可以在位置的准确性上面做到单调的提升。

### 之前NMS的问题
文中说用之前的NMS办法的话，会有一个问题，即

misaligned classification and localization accuracy

意思就是可能通过class prop选的最好的那个所对应的位置并不一定是最好的，即没有aligned。

也有改进NMS的，比如soft-NMS.之前nms是找到一个当前最好的之后，就去掉周围那些超过某个给定的IOU阈值的，而在soft-nms中，并不是删除而是把它的置信度给降低，这主要是考虑到这些框可能会在后面还会用到，实验证明Soft-NMS确实会提高recall。

也有一些不用nms和soft-nms的，而是用一种基于学习的算法，毕竟nms和soft-nms是不用学习的。

### 实验的证据

为了说明确实有不aligned的情况，作者做了实验

![avatar](/images/007.png)

发现其实class confidence 与IOU 并不怎么相关， 而 loc confidence 看上去倒是挺相关的。

然后从下图也可以看出来，如果没有loc置信度的话，会使得超过一半的IOU>0.9的检测框会在传统的NMS过程中被抑制， 而这会使检测的位置质量下降。

![avatar](/images/008.png)

并且从下图也可以看出基于优化的bbox refinement方法的好处

![avatar](/images/009.png)

从图上明显可以看出，基于优化的方法的结果的AP是一直在递增的，
而基于回归的方法会随着迭代次数出现下降的可能。

### IOU-Net
网络结构是这样的
![avatar](/images/010.png)
dashed部分是IOU-net，外面的是RPN和R-CNN.

### learning to predict IOU
值得注意的地方是：

![avatar](/images/011.png)

即用gt来做数据增强，随机一组变换参数， 然后结合gt就能产生一批候选，然后在这些候选中去掉那些IOU<0.5的。

IOU阶段这里的bbox并不是用RPN来产生的，RPN到后面联合的时候才用到。

### IOU-guided NMS

![avatar](/images/012.png)

意思就是先根据pred_IOU, 选出最大的pred_IOU对应的bbox，然后在这个bbox附近中去掉那些和这个bbox的IOU大于某个阈值的，在这个过程中同时要更新class-confidence为最大的，
不断地重复这个过程，直到所有的bbox全部用完。

### bbox refinement as an optimization procedure
优化过程如下：

![avatar](/images/013.png)

（-- 我并不是完全清楚其中的细节，等理解了再更新--）
目前我的理解是:要估计的IOU（det, gt），希望使得这个东西大一些，所以可以对bbox求梯度，然后朝着梯度的方向更新bbox，使得它变大，停止条件是T达到了，那么被放入A的bbox的条件我不是很理解，好像是变化不太明显的时候或者是超过了那个容忍度的时候。


###  precies ROI-Pooling

![avatar](/images/014.png)

从上图中可以看出其演化的过程，从最初的roi-pooling到后面用双线性插值的roi-align，再到这里的用双重积分的prRoi-pooling，

其中f的定义如下：

![avatar](/images/015.png)

意思就是(x,y)这点的值要用它周围的4个点来插值，具体推导见下面的：

![avatar](/images/016.png)

### joint training

![avatar](/images/017.png)

注意这里面有说对于IOU predictor用的是SmoothL1。

具体的细节还要针对代码看！
还会更新！








