---
layout: post
title: AI 日历
date: 2023-03-30
categories: [任意门]
tags: 任意门
---


| 研究方向 | 2012 | 2013 | 2014 | 2015 | 2016 | 2017 | 2018 | 2019 | 2020 | 2021 | 2022 | 2023 |
| ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ |
| 计算机视觉-CNN | [AlexNet](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) |  | [1.VGG](https://arxiv.org/pdf/1409.1556.pdf)  [2.GoogleNet](https://arxiv.org/pdf/1409.4842.pdf) | [ResNet](https://arxiv.org/pdf/1512.03385.pdf) |    |  [MobileNet](https://arxiv.org/pdf/1704.04861.pdf)  |    |  [EfficientNet](https://arxiv.org/pdf/1905.11946.pdf) |    |  [Non-deep networks](https://arxiv.org/pdf/2110.07641.pdf)  |    |    |  
| 计算机视觉 - 对比学习|  |  |    |    |    |    |  [1.InstDisc](https://arxiv.org/pdf/1805.01978.pdf)  [2.CPC](https://arxiv.org/pdf/1807.03748.pdf)  |  [1.InvaSpread](https://arxiv.org/pdf/1904.03436.pdf)  [2.CMC](https://arxiv.org/pdf/1906.05849.pdf)  [3.MoCov1](https://arxiv.org/pdf/1911.05722.pdf)  |  [1.SimCLRv1](https://arxiv.org/pdf/2002.05709.pdf)  [2.MoCov2](https://arxiv.org/pdf/2003.04297.pdf)  [3.SimCLRv2](https://arxiv.org/pdf/2006.10029.pdf)  [4.BYOL](https://arxiv.org/pdf/2006.07733.pdf)  [5.SWaV](https://arxiv.org/pdf/2006.09882.pdf)  [6.SimSiam](https://arxiv.org/pdf/2011.10566.pdf)  |  [1.MoCov3](https://arxiv.org/pdf/2104.02057.pdf)  [2.DINO](https://arxiv.org/pdf/2104.14294.pdf)  |    |    |  
| 计算机视觉 - Transformer |  |  |    |    |    |    |    |    |  [ViT](https://arxiv.org/pdf/2010.11929.pdf)  |  [1.Swin Transformer](https://arxiv.org/pdf/2103.14030.pdf)  [2.MLP-Mixer](https://arxiv.org/pdf/2105.01601.pdf)  [3.MAE](https://arxiv.org/pdf/2111.06377.pdf)  |   |    |  
| 生成模型 |  |  |  [GAN](https://papers.nips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf)  | [DCGAN](https://arxiv.org/pdf/1511.06434.pdf)   |  [1.pix2pix](https://arxiv.org/pdf/1611.07004.pdf)  [2.SRGAN](https://arxiv.org/pdf/1609.04802.pdf)  | [1.WGAN](https://arxiv.org/abs/1701.07875)  [2.CycleGAN](https://arxiv.org/abs/1703.10593)   |  [StyleGAN](https://arxiv.org/abs/1812.04948)  |  [StyleGAN2](https://arxiv.org/pdf/1912.04958.pdf)  |  [DDPM](https://arxiv.org/pdf/2006.11239.pdf)  |  [1.Improved DDPM](https://arxiv.org/pdf/2102.09672.pdf)  [2.Guided Diffusion Models](https://arxiv.org/pdf/2105.05233.pdf)  [3.StyleGAN3](https://arxiv.org/pdf/2106.12423.pdf) |  [1.Stable Diffusion](https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf)  [2.DALL.E 2](https://arxiv.org/pdf/2204.06125.pdf)  |    |  
| 计算机视觉 - Object Detection |  |  |  [R-CNN](https://arxiv.org/pdf/1311.2524v5.pdf)  |  [1.Fast R-CNN](http://arxiv.org/abs/1504.08083v2)  [2.Faster R-CNN](http://arxiv.org/abs/1506.01497v3)  |  [1.SSD](http://arxiv.org/abs/1512.02325v5)  [2.YOLO](http://arxiv.org/abs/1506.02640v5) |  [1.Mask R-CNN](http://arxiv.org/abs/1703.06870v3)  [2.YOLOv2](http://arxiv.org/abs/1612.08242v1)  |  [YOLOv3](http://arxiv.org/abs/1804.02767v1)  |  [CenterNet](https://arxiv.org/pdf/1904.07850.pdf)  |  [DETR](https://arxiv.org/pdf/2005.12872.pdf)  |    |    |    |  
| 计算机视觉 - 视频理解 |  |  |  [1.DeepVideo](https://cs.stanford.edu/people/karpathy/deepvideo/)  [2.Two-stream](https://arxiv.org/pdf/1406.2199.pdf)  [3.C3D](https://arxiv.org/pdf/1412.0767.pdf)  |  [Beyond-short-snippets](https://arxiv.org/pdf/1503.08909.pdf)  |  [1.Convolutional fusion](https://arxiv.org/pdf/1604.06573.pdf)  [2.TSN](https://arxiv.org/pdf/1608.00859.pdf)  |  [1.I3D](https://arxiv.org/pdf/1705.07750.pdf)  [2.R2+1D](https://arxiv.org/pdf/1711.11248.pdf)  [3.Non-local](https://arxiv.org/pdf/1711.07971.pdf)  |  [SlowFast](https://arxiv.org/pdf/1812.03982.pdf)  |    |    | [TimeSformer](https://arxiv.org/pdf/2102.05095.pdf)   |    |    |  
| 自然语言处理 - Transformer |  |  |    |    |    |  [Transformer](https://arxiv.org/abs/1706.03762)  |  [1.GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)  [2.BERT](https://arxiv.org/abs/1810.04805)  |  [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)  |  [GPT-3](https://arxiv.org/abs/2005.14165)  |    |  [chatGPT](https://openai.com/blog/chatgpt)  |    |  
| 多模态学习 |  |  |    |    |    |    |    |    |    |  [1.CLIP](https://openai.com/blog/clip/)  [2.ViLT](https://arxiv.org/pdf/2102.03334.pdf)  [3.ViLD](https://arxiv.org/pdf/2104.13921.pdf)  [4.GLIP](https://arxiv.org/pdf/2112.03857.pdf)  [5.CLIP4Clip](https://arxiv.org/pdf/2104.08860.pdf)  [6.ActionCLIP](https://arxiv.org/pdf/2109.08472.pdf)  [7.PointCLIP](https://arxiv.org/pdf/2112.02413.pdf)  |  [1.LSeg](https://arxiv.org/pdf/2201.03546.pdf)  [2.GroupViT](https://arxiv.org/pdf/2202.11094.pdf)  [3.CLIPasso](https://arxiv.org/pdf/2202.05822.pdf)  [4.DepthCLIP](https://arxiv.org/pdf/2207.01077.pdf) |  [GPT-4](https://openai.com/product/gpt-4)  |  
