---
layout: post
title: 梯度clip
date: 2018-12-23
categories: [torch]
tags: torch
---
<!--more-->

在训练模型的时候有时候会出现这种情况，即loss已经很小了，几乎接近于0，但是模型并不是太好，我之前就遇到过这个问题。分析了之后觉得是因为loss小的时候梯度也很小，这样就会导致学不到东西。
而梯度clip是一种解决的办法，它是为了防止梯度太大或者梯度太小的时候而做的，就像`torch.clamp` 的想法一样，如果小于某个阈值的话，为了不让其太小了，就人为地规定一个值，让其以这个值传下去，

在torch里面可以这样用。

`torch.nn.utils.clip_grad_norm` 里面指定阈值就可以了。而且可以指定哪些参数。






