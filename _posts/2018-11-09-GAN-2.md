---
layout: post
title: GAN-2 (GAN 论文笔记)
date: 2018-11-09 
categories: [论文笔记]
tags: 论文笔记
---
<!--more-->

GAN是14年发明的，作者是在读博士`Ian J.Goodfellow`, 敬佩！！！
之前先从代码的方式，快速了看到了gan的成果，不过只懂表象不懂原理是不行的，所以这次准备把paper读一下，放开其它的细节暂且不管，这次只记录上次里面用到的loss函数。
![avator](/images/gan3.png)
最关键的是为什么作者会选取这个作为优化的目标呢？
我自己的理解记录如下。
直观上看，为了使Discriminator效果好，最好是gt进来的时候输出是1，fake进来的时候输出0，那么这相当于是一个二分类的问题，所以用BCE比较自然，这就是上面的式子中先不看`min_G`的时候的那部分；现在再这样想一下，如果Discriminator已经固定，如何让Generator好呢，因为Generator这部分的输出一定是fake，所以这部分的loss对应在BCE的部分就是上式子中的第二项，而Generator的好意味着D认为其是真品的probablity非常接近于1，所以要优化的就是`min_G E[log(1-D(G(z)))]`,而这时候D认为已经固定，所以第一项对这时候优化G可以认为是没有影响，所以综合起来就是上面的式子。
有一些关于loss的改进，后面会慢慢地理解其进展。
