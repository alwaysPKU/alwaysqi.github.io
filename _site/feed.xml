<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>半甜不要腻</title>
    <description>welcome to my page</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 09 Jan 2019 15:19:39 +0800</pubDate>
    <lastBuildDate>Wed, 09 Jan 2019 15:19:39 +0800</lastBuildDate>
    <generator>Jekyll v3.8.3</generator>
    
      <item>
        <title>jekyll&amp;valine实现评论功能</title>
        <description>&lt;!--more--&gt;

&lt;p&gt;标签： 拾遗&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;要借助leancloud平台&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;注册leancloud&lt;/li&gt;
  &lt;li&gt;新建应用，名字任取&lt;/li&gt;
  &lt;li&gt;创建class，可取名comment，默认配置即可&lt;/li&gt;
  &lt;li&gt;include/comments.html添加代码&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;script &lt;/span&gt;&lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;//cdn1.lncld.net/static/js/3.0.4/av-min.js&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;script &lt;/span&gt;&lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'//unpkg.com/valine/dist/Valine.min.js'&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;id=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;comment&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;script&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;valine&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;Valine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;valine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;el&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'#comment'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;appId&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'App ID'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;//leancloud里找&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;appKey&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'App Key'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;//leancloud里找&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;notify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'/2019/01/supportcomment/'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'默认评论'&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
        <pubDate>Tue, 08 Jan 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/01/supportcomment/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/01/supportcomment/</guid>
        
        <category>拾遗</category>
        
        
        <category>拾遗</category>
        
      </item>
    
      <item>
        <title>jekyll支持latex公式</title>
        <description>&lt;!--more--&gt;

&lt;p&gt;标签： 拾遗&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;在incloud/head.html文件里加入以下代码：&lt;/p&gt;
&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;script &lt;/span&gt;&lt;span class=&quot;na&quot;&gt;type=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text/x-mathjax-config&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;MathJax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;Hub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;Config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;tex2jax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;skipTags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'script'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'noscript'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'style'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'textarea'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'pre'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;inlineMath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'$'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'$'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;script &lt;/span&gt;&lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML'&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;async&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;来个秀：
&lt;script type=&quot;math/tex&quot;&gt;\begin{multline} \lambda_\textbf{coord} \sum_{i = 0}^{S^2}     \sum_{j = 0}^{B}     \mathbb{𝟙}_{ij}^{\text{obj}}             \left[             \left(                 x_i - \hat{x}_i             \right)^2 +             \left(                 y_i - \hat{y}_i             \right)^2             \right] \\ + \lambda_\textbf{coord} \sum_{i = 0}^{S^2}     \sum_{j = 0}^{B}         \mathbb{𝟙}_{ij}^{\text{obj}}          \left[         \left(             \sqrt{w_i} - \sqrt{\hat{w}_i}         \right)^2 +         \left(             \sqrt{h_i} - \sqrt{\hat{h}_i}         \right)^2         \right] \\ + \sum_{i = 0}^{S^2}     \sum_{j = 0}^{B}         \mathbb{𝟙}_{ij}^{\text{obj}}         \left(             C_i - \hat{C}_i         \right)^2 \\ + \lambda_\textrm{noobj} \sum_{i = 0}^{S^2}     \sum_{j = 0}^{B}     \mathbb{𝟙}_{ij}^{\text{noobj}}         \left(             C_i - \hat{C}_i         \right)^2 \\ + \sum_{i = 0}^{S^2} \mathbb{𝟙}_i^{\text{obj}}     \sum_{c \in \textrm{classes}}         \left(             p_i(c) - \hat{p}_i(c)         \right)^2 \end{multline}&lt;/script&gt;&lt;/p&gt;

</description>
        <pubDate>Tue, 08 Jan 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/01/supportLatex/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/01/supportLatex/</guid>
        
        <category>拾遗</category>
        
        
        <category>拾遗</category>
        
      </item>
    
      <item>
        <title>全卷积神经网络(FCN)</title>
        <description>&lt;!--more--&gt;

&lt;p&gt;标签： CNN，深度学习，检测&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;FCN又称全卷积神经网络&lt;a href=&quot;https://link.jianshu.com/?t=https://arxiv.org/abs/1411.4038&quot;&gt;《Fully Convolutional Networks for Semantic Segmentation》&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;fcn的精髓&quot;&gt;FCN的精髓&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;1、把卷积层 -&amp;gt; 全连接层，看成是对一整张图片的卷积层运算。&lt;/li&gt;
    &lt;li&gt;2、把全连接层 -&amp;gt; 全连接层，看成是采用1*1大小的卷积核，进行卷积层运算。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;论文翻译&quot;&gt;论文翻译&lt;/h3&gt;
&lt;p&gt;先来原文的翻译（大部分来最于&lt;a href=&quot;https://www.cnblogs.com/xuanxufeng/p/6249834.html&quot;&gt;这里&lt;/a&gt;，进行些许微整理）
&lt;img src=&quot;/images/posts/FCN/title.jpg&quot; alt=&quot;title&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;摘要&quot;&gt;摘要&lt;/h4&gt;
&lt;p&gt;卷积网络在特征分层领域是非常强大的视觉模型。我们证明了经过端到端、像素到像素训练的卷积网络超过语义分割中最先进的技术。我们的核心观点是建立“全卷积”网络，输入任意尺寸，经过有效的推理和学习产生相应尺寸的输出。我们定义并指定全卷积网络的空间，解释它们在空间范围内dense prediction任务(预测每个像素所属的类别)和获取与先验模型联系的应用。我们改编当前的分类网络(AlexNet &lt;sup id=&quot;fnref:22&quot;&gt;&lt;a href=&quot;#fn:22&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; ,the VGG net &lt;sup id=&quot;fnref:34&quot;&gt;&lt;a href=&quot;#fn:34&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; , and GoogLeNet &lt;sup id=&quot;fnref:35&quot;&gt;&lt;a href=&quot;#fn:35&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; )到完全卷积网络和通过微调 &lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; 传递它们的学习表现到分割任务中。然后我们定义了一个跳跃式的架构，结合来自深、粗层的语义信息和来自浅、细层的表征信息来产生准确和精细的分割。我们的完全卷积网络成为了在PASCAL VOC最出色的分割方式（在2012年相对62.2%的平均IU提高了20%），NYUDv2，和SIFT Flow,对一个典型图像推理只需要花费不到0.2秒的时间。&lt;/p&gt;

&lt;h4 id=&quot;1-引言&quot;&gt;1. 引言&lt;/h4&gt;
&lt;p&gt;卷积网络在识别领域前进势头很猛。卷积网不仅全图式的分类上有所提高 &lt;sup id=&quot;fnref:22:1&quot;&gt;&lt;a href=&quot;#fn:22&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:34:1&quot;&gt;&lt;a href=&quot;#fn:34&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:35:1&quot;&gt;&lt;a href=&quot;#fn:35&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; ,也在结构化输出的局部任务上取得了进步。包括在目标检测边界框 &lt;sup id=&quot;fnref:32&quot;&gt;&lt;a href=&quot;#fn:32&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:12&quot;&gt;&lt;a href=&quot;#fn:12&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:19&quot;&gt;&lt;a href=&quot;#fn:19&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; 、部分和关键点预测 &lt;sup id=&quot;fnref:42&quot;&gt;&lt;a href=&quot;#fn:42&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:26&quot;&gt;&lt;a href=&quot;#fn:26&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt; 和局部通信 &lt;sup id=&quot;fnref:26:1&quot;&gt;&lt;a href=&quot;#fn:26&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:10&quot;&gt;&lt;a href=&quot;#fn:10&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt; 的进步。&lt;/p&gt;

&lt;p&gt;在从粗糙到精细推理的进展中下一步自然是对每一个像素进行预测。早前的方法已经将卷积网络用于语义分割 &lt;sup id=&quot;fnref:30&quot;&gt;&lt;a href=&quot;#fn:30&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:9&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:31&quot;&gt;&lt;a href=&quot;#fn:31&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:17&quot;&gt;&lt;a href=&quot;#fn:17&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:15&quot;&gt;&lt;a href=&quot;#fn:15&quot; class=&quot;footnote&quot;&gt;16&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:11&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt; ,其中每个像素被标记为其封闭对象或区域的类别，但是这些工作还是有缺点。
&lt;img src=&quot;/images/posts/FCN/1.1.png&quot; alt=&quot;1.1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我们证明了经过&lt;strong&gt;端到端&lt;/strong&gt;、像素到像素训练的的卷积网络超过语义分割中没有further machinery的技术。我们认为，这是第一次训练端到端(1)的FCN在像素级别的预测，而且来自监督式预处理(2)。全卷积在现有的网络基础上从任意尺寸的输入预测密集输出。学习和推理能在全图通过密集的前馈计算和反向传播一次执行。网内上采样层能在像素级别预测和通过下采样池化学习。&lt;/p&gt;

&lt;p&gt;这种方法非常有效，无论是渐进地还是完全地，消除了在其他方法中的并发问题。Patchwise训练是常见的 &lt;sup id=&quot;fnref:30:1&quot;&gt;&lt;a href=&quot;#fn:30&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:3:1&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:9:1&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:31:1&quot;&gt;&lt;a href=&quot;#fn:31&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:11:1&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt;，但是缺少了全卷积训练的有效性。我们的方法不是利用预处理或者后期处理解决并发问题，包括超像素 &lt;sup id=&quot;fnref:9:2&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:17:1&quot;&gt;&lt;a href=&quot;#fn:17&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt; ，proposals &lt;sup id=&quot;fnref:17:2&quot;&gt;&lt;a href=&quot;#fn:17&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:15:1&quot;&gt;&lt;a href=&quot;#fn:15&quot; class=&quot;footnote&quot;&gt;16&lt;/a&gt;&lt;/sup&gt;，或者对通过随机域事后细化或者局部分类 &lt;sup id=&quot;fnref:9:3&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:17:3&quot;&gt;&lt;a href=&quot;#fn:17&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt; 。我们的模型通过重新解释分类网到全卷积网络和微调它们的学习表现将最近在分类上的成功 &lt;sup id=&quot;fnref:22:2&quot;&gt;&lt;a href=&quot;#fn:22&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:34:2&quot;&gt;&lt;a href=&quot;#fn:34&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:35:2&quot;&gt;&lt;a href=&quot;#fn:35&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; 移植到dense prediction。与此相反，先前的工作应用的是小规模、没有超像素预处理的卷积网。&lt;/p&gt;

&lt;p&gt;语义分割面临在语义和位置的内在张力问题：全局信息解决的“是什么”，而局部信息解决的是“在哪里”。深层特征通过非线性的局部到全局金字塔编码了位置和语义信息。我们在4.2节(见图3）定义了一种利用集合了深、粗层的语义信息和浅、细层的表征信息的特征谱的跨层架构。&lt;/p&gt;

&lt;p&gt;在下一节，我们回顾深层分类网、FCNs和最近一些利用卷积网解决语义分割的相关工作。接下来的章节将解释FCN设计和密集预测权衡，介绍我们的网内上采样和多层结合架构，描述我们的实验框架。最后，我们展示了最先进技术在PASCAL VOC 2011-2, NYUDv2, 和SIFT Flow上的实验结果。&lt;/p&gt;

&lt;h4 id=&quot;2-相关工作&quot;&gt;2. 相关工作&lt;/h4&gt;

&lt;p&gt;我们的方法是基于最近深层网络在图像分类上的成功 &lt;sup id=&quot;fnref:22:3&quot;&gt;&lt;a href=&quot;#fn:22&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:34:3&quot;&gt;&lt;a href=&quot;#fn:34&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:35:3&quot;&gt;&lt;a href=&quot;#fn:35&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; 和转移学习。转移第一次被证明在各种视觉识别任务 &lt;sup id=&quot;fnref:5:1&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:41&quot;&gt;&lt;a href=&quot;#fn:41&quot; class=&quot;footnote&quot;&gt;18&lt;/a&gt;&lt;/sup&gt; ，然后是检测，不仅在实例还有融合proposal-classification模型的语义分割 &lt;sup id=&quot;fnref:12:1&quot;&gt;&lt;a href=&quot;#fn:12&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:17:4&quot;&gt;&lt;a href=&quot;#fn:17&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:15:2&quot;&gt;&lt;a href=&quot;#fn:15&quot; class=&quot;footnote&quot;&gt;16&lt;/a&gt;&lt;/sup&gt; 。我们现在重新构建和微调直接的、dense prediction语义分割的分类网。在这个框架里我们绘制FCNs的空间并将过去的或是最近的先验模型置于其中。&lt;/p&gt;

&lt;p&gt;全卷积网络据我们所知，第一次将卷积网扩展到任意尺寸的输入的是Matan等人 &lt;sup id=&quot;fnref:28&quot;&gt;&lt;a href=&quot;#fn:28&quot; class=&quot;footnote&quot;&gt;19&lt;/a&gt;&lt;/sup&gt; ,它将经典的LeNet &lt;sup id=&quot;fnref:23&quot;&gt;&lt;a href=&quot;#fn:23&quot; class=&quot;footnote&quot;&gt;20&lt;/a&gt;&lt;/sup&gt; 扩展到识别字符串的位数。因为他们的网络结构限制在一维的输入串，Matan等人利用译码器译码获得输出。Wolf和Platt &lt;sup id=&quot;fnref:40&quot;&gt;&lt;a href=&quot;#fn:40&quot; class=&quot;footnote&quot;&gt;21&lt;/a&gt;&lt;/sup&gt; 将卷积网输出扩展到来检测邮政地址块的四角得分的二维图。这些先前工作做的是推理和用于检测的全卷积式学习。Ning等人 &lt;sup id=&quot;fnref:30:2&quot;&gt;&lt;a href=&quot;#fn:30&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt; 定义了一种卷积网络用于秀丽线虫组织的粗糙的、多分类分割，基于全卷积推理。&lt;/p&gt;

&lt;p&gt;全卷积计算也被用在现在的一些多层次的网络结构中。Sermanet等人的滑动窗口检测 &lt;sup id=&quot;fnref:32:1&quot;&gt;&lt;a href=&quot;#fn:32&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; ，Pinherio 和Collobert的语义分割 &lt;sup id=&quot;fnref:31:2&quot;&gt;&lt;a href=&quot;#fn:31&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt; ，Eigen等人的图像修复 &lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;22&lt;/a&gt;&lt;/sup&gt; 都做了全卷积式推理。全卷积训练很少，但是被Tompson等人 &lt;sup id=&quot;fnref:38&quot;&gt;&lt;a href=&quot;#fn:38&quot; class=&quot;footnote&quot;&gt;23&lt;/a&gt;&lt;/sup&gt; 用来学习一种端到端的局部检测和姿态估计的空间模型非常有效，尽管他们没有解释或者分析这种方法。&lt;/p&gt;

&lt;p&gt;此外，He等人&lt;sup id=&quot;fnref:19:1&quot;&gt;&lt;a href=&quot;#fn:19&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;在特征提取时丢弃了分类网的无卷积部分。他们结合proposals和空间金字塔池来产生一个局部的、固定长度的特征用于分类。尽管快速且有效，但是这种混合模型不能进行端到端的学习。&lt;/p&gt;

&lt;p&gt;基于卷积网的dense prediction近期的一些工作已经将卷积网应用于dense prediction问题，包括Ning等人的语义分割 &lt;sup id=&quot;fnref:30:3&quot;&gt;&lt;a href=&quot;#fn:30&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt; ,Farabet等人 &lt;sup id=&quot;fnref:9:4&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt; 以及Pinheiro和Collobert &lt;sup id=&quot;fnref:31:3&quot;&gt;&lt;a href=&quot;#fn:31&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt; ；Ciresan等人的电子显微镜边界预测 &lt;sup id=&quot;fnref:3:2&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt; 以及Ganin和Lempitsky &lt;sup id=&quot;fnref:11:2&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt; 的通过混合卷积网和最邻近模型的处理自然场景图像;还有Eigen等人 &lt;sup id=&quot;fnref:6:1&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;22&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:7&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;24&lt;/a&gt;&lt;/sup&gt; 的图像修复和深度估计。这些方法的相同点包括如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;限制容量和接收域的小模型&lt;/li&gt;
  &lt;li&gt;patchwise训练 &lt;sup id=&quot;fnref:30:4&quot;&gt;&lt;a href=&quot;#fn:30&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:3:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:9:5&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:31:4&quot;&gt;&lt;a href=&quot;#fn:31&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:11:3&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;超像素投影的预处理，随机场正则化、滤波或局部分类 &lt;sup id=&quot;fnref:9:6&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:3:4&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:11:4&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;输入移位和dense输出的隔行交错输出 &lt;sup id=&quot;fnref:32:2&quot;&gt;&lt;a href=&quot;#fn:32&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:31:5&quot;&gt;&lt;a href=&quot;#fn:31&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:11:5&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;多尺度金字塔处理 &lt;sup id=&quot;fnref:9:7&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:31:6&quot;&gt;&lt;a href=&quot;#fn:31&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:11:6&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;饱和双曲线正切非线性 &lt;sup id=&quot;fnref:9:8&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:6:2&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;22&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:31:7&quot;&gt;&lt;a href=&quot;#fn:31&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;集成 &lt;sup id=&quot;fnref:3:5&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:11:7&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然而我们的方法确实没有这种机制。但是我们研究了patchwise训练 （3.4节）和从FCNs的角度出发的“shift-and-stitch”dense输出（3.2节）。我们也讨论了网内上采样（3.3节），其中Eigen等人&lt;sup id=&quot;fnref:7:1&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;24&lt;/a&gt;&lt;/sup&gt;的全连接预测是一个特例。&lt;/p&gt;

&lt;p&gt;和这些现有的方法不同的是，我们改编和扩展了深度分类架构，使用图像分类作为监督预处理，和从全部图像的输入和ground truths(用于有监督训练的训练集的分类准确性)通过全卷积微调进行简单且高效的学习。&lt;/p&gt;

&lt;p&gt;Hariharan等人 &lt;sup id=&quot;fnref:17:5&quot;&gt;&lt;a href=&quot;#fn:17&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt; 和Gupta等人 &lt;sup id=&quot;fnref:15:3&quot;&gt;&lt;a href=&quot;#fn:15&quot; class=&quot;footnote&quot;&gt;16&lt;/a&gt;&lt;/sup&gt; 也改编深度分类网到语义分割，但是也在混合proposal-classifier模型中这么做了。这些方法通过采样边界框和region proposal进行微调了R-CNN系统 &lt;sup id=&quot;fnref:12:2&quot;&gt;&lt;a href=&quot;#fn:12&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; ,用于检测、语义分割和实例分割。这两种办法都不能进行端到端的学习。他们分别在PASCAL VOC和NYUDv2实现了最好的分割效果，所以在第5节中我们直接将我们的独立的、端到端的FCN和他们的语义分割结果进行比较。&lt;/p&gt;

&lt;p&gt;我们通过跨层和融合特征来定义一种非线性的局部到整体的表述用来协调端到端。在现今的工作中Hariharan等人 &lt;sup id=&quot;fnref:18&quot;&gt;&lt;a href=&quot;#fn:18&quot; class=&quot;footnote&quot;&gt;25&lt;/a&gt;&lt;/sup&gt; 也在语义分割的混合模型中使用了多层。&lt;/p&gt;

&lt;h4 id=&quot;3-全卷积网络&quot;&gt;3. 全卷积网络&lt;/h4&gt;

&lt;p&gt;卷积网的每层数据是一个h&lt;em&gt;w&lt;/em&gt;d的三维数组，其中h和w是空间维度,d是特征或通道维数。第一层是像素尺寸为h*w、颜色通道数为d的图像。高层中的locations和图像中它们连通的locations相对应，被称为接收域。&lt;/p&gt;

&lt;p&gt;卷积网是以平移不变形作为基础的。其基本组成部分(卷积，池化和激励函数)作用在局部输入域，只依赖相对空间坐标。在特定层记X_ij为在坐标(i,j)的数据向量，在following layer有Y_ij，Y_ij的计算公式如下:
&lt;img src=&quot;/images/posts/FCN/3.1.png&quot; alt=&quot;3.1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其中k为卷积核尺寸，s是步长或下采样因素，f_ks决定了层的类型：一个卷积的矩阵乘或者是平均池化，用于最大池的最大空间值或者是一个激励函数的一个非线性elementwise，亦或是层的其他种类等等。当卷积核尺寸和步长遵从转换规则，这个函数形式被表述为如下形式：
&lt;img src=&quot;/images/posts/FCN/3.2.png&quot; alt=&quot;3.2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;当一个普通深度的网络计算一个普通的非线性函数，一个网络只有这种形式的层计算非线性滤波，我们称之为深度滤波或全卷积网络。FCN理应可以计算任意尺寸的输入并产生相应（或许重采样)空间维度的输出。一个实值损失函数有FCN定义了task。如果损失函数是一个最后一层的空间维度总和,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/FCN/3.3.png&quot; alt=&quot;3.3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;它的梯度将是它的每层空间组成梯度总和。所以在全部图像上的基于l的随机梯度下降计算将和基于l’的梯度下降结果一样，将最后一层的所有接收域作为minibatch（分批处理）。在这些接收域重叠很大的情况下，前反馈计算和反向传播计算整图的叠层都比独立的patch-by-patch有效的多。&lt;/p&gt;

&lt;p&gt;我们接下来将解释怎么将分类网络转换到能产生粗输出图的全卷积网络。对于像素级预测，我们需要连接这些粗略的输出结果到像素。3.2节描述了一种技巧，快速扫描&lt;sup id=&quot;fnref:13&quot;&gt;&lt;a href=&quot;#fn:13&quot; class=&quot;footnote&quot;&gt;26&lt;/a&gt;&lt;/sup&gt;因此被引入。我们通过将它解释为一个等价网络修正而获得了关于这个技巧的一些领悟。作为一个高效的替换，我们引入了去卷积层用于上采样见3.3节。在3.4节，我们考虑通过patchwise取样训练，便在4.3节证明我们的全图式训练更快且同样有效。&lt;/p&gt;

&lt;h5 id=&quot;31-改编分类用于dense-prediction&quot;&gt;3.1 改编分类用于dense prediction&lt;/h5&gt;

&lt;p&gt;典型的识别网络，包括LeNet &lt;sup id=&quot;fnref:23:1&quot;&gt;&lt;a href=&quot;#fn:23&quot; class=&quot;footnote&quot;&gt;20&lt;/a&gt;&lt;/sup&gt; , AlexNet &lt;sup id=&quot;fnref:22:4&quot;&gt;&lt;a href=&quot;#fn:22&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; , 和一些后继者 &lt;sup id=&quot;fnref:34:4&quot;&gt;&lt;a href=&quot;#fn:34&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, &lt;sup id=&quot;fnref:35:4&quot;&gt;&lt;a href=&quot;#fn:35&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; ，表面上采用的是固定尺寸的输入产生了非空间的输出。这些网络的全连接层有确定的位数并丢弃空间坐标。然而，这些全连接层也被看做是覆盖全部输入域的核卷积。需要将它们加入到可以采用任何尺寸输入并输出分类图的全卷积网络中。这种转换如图2所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/FCN/3.1.1.png&quot; alt=&quot;3.1.1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;此外，当作为结果的图在特殊的输入patches上等同于原始网络的估计，计算是高度摊销的在那些patches的重叠域上。例如，当AlexNet花费了1.2ms（在标准的GPU上)推算一个227&lt;em&gt;227图像的分类得分，全卷积网络花费22ms从一张500&lt;/em&gt;500的图像上产生一个10*10的输出网格，比朴素法快了5倍多。&lt;/p&gt;

&lt;p&gt;这些卷积化模式的空间输出图可以作为一个很自然的选择对于dense问题，比如语义分割。每个输出单元ground truth可用，正推法和逆推法都是直截了当的，都利用了卷积的固有的计算效率(和可极大优化性)。对于AlexNet例子相应的逆推法的时间为单张图像时间2.4ms，全卷积的10*10输出图为37ms，结果是相对于顺推法速度加快了。&lt;/p&gt;

&lt;p&gt;当我们将分类网络重新解释为任意输出尺寸的全卷积域输出图，输出维数也通过下采样显著的减少了。分类网络下采样使filter保持小规模同时计算要求合理。这使全卷积式网络的输出结果变得粗糙，通过输入尺寸因为一个和输出单元的接收域的像素步长等同的因素来降低它。&lt;/p&gt;

&lt;h5 id=&quot;32-shift-and-stitch是滤波稀疏&quot;&gt;3.2 Shift-and stitch是滤波稀疏&lt;/h5&gt;

&lt;p&gt;dense prediction能从粗糙输出中通过从输入的平移版本中将输出拼接起来获得。如果输出是因为一个因子f降低采样，平移输入的x像素到左边，y像素到下面，一旦对于每个(x,y)满足0&amp;lt;=x,y&amp;lt;=f.处理f^2个输入，并将输出交错以便预测和它们接收域的中心像素一致。&lt;/p&gt;

&lt;p&gt;尽管单纯地执行这种转换增加了f^2的这个因素的代价，有一个非常有名的技巧用来高效的产生完全相同的结果 &lt;sup id=&quot;fnref:13:1&quot;&gt;&lt;a href=&quot;#fn:13&quot; class=&quot;footnote&quot;&gt;26&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:32:3&quot;&gt;&lt;a href=&quot;#fn:32&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; ，这个在小波领域被称为多孔算法 &lt;sup id=&quot;fnref:27&quot;&gt;&lt;a href=&quot;#fn:27&quot; class=&quot;footnote&quot;&gt;27&lt;/a&gt;&lt;/sup&gt; 。考虑一个层（卷积或者池化）中的输入步长s,和后面的滤波权重为f_ij的卷积层（忽略不相关的特征维数）。设置更低层的输入步长到l上采样它的输出影响因子为s。然而，将原始的滤波和上采样的输出卷积并没有产生和shift-and-stitch相同的结果，因为原始的滤波只看得到（已经上采样）输入的简化的部分。为了重现这种技巧，通过扩大来稀疏滤波，如下:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/FCN/3.2.1.png&quot; alt=&quot;3.2.1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如果s能除以i和j，除非i和j都是0。重现该技巧的全网输出需要重复一层一层放大这个filter知道所有的下采样被移除。（在练习中，处理上采样输入的下采样版本可能会更高效。）&lt;/p&gt;

&lt;p&gt;在网内减少二次采样是一种折衷的做法：filter能看到更细节的信息，但是接受域更小而且需要花费很长时间计算。Shift-and -stitch技巧是另外一种折衷做法：输出更加密集且没有减小filter的接受域范围，但是相对于原始的设计filter不能感受更精细的信息。&lt;/p&gt;

&lt;p&gt;尽管我们已经利用这个技巧做了初步的实验，但是我们没有在我们的模型中使用它。正如在下一节中描述的，我们发现从上采样中学习更有效和高效，特别是接下来要描述的结合了跨层融合。&lt;/p&gt;

&lt;h5 id=&quot;33-上采样是向后向卷积&quot;&gt;3.3 上采样是向后向卷积&lt;/h5&gt;
&lt;p&gt;另一种连接粗糙输出到dense像素的方法就是插值法。比如，简单的双线性插值计算每个输出y_ij来自只依赖输入和输出单元的相对位置的线性图最近的四个输入。&lt;/p&gt;

&lt;p&gt;从某种意义上，伴随因子f的上采样是对步长为1/f的分数式输入的卷积操作。只要f是整数，一种自然的方法进行上采样就是向后卷积（有时称为去卷积）伴随输出步长为f。这样的操作实现是不重要的，因为它只是简单的调换了卷积的顺推法和逆推法。所以上采样在网内通过计算像素级别的损失的反向传播用于端到端的学习。&lt;/p&gt;

&lt;p&gt;需要注意的是去卷积滤波在这种层面上不需要被固定不变（比如双线性上采样）但是可以被学习。一堆反褶积层和激励函数甚至能学习一种非线性上采样。在我们的实验中，我们发现在网内的上采样对于学习dense prediction是快速且有效的。我们最好的分割架构利用了这些层来学习上采样用以微调预测，见4.2节。&lt;/p&gt;

&lt;h5 id=&quot;34-patchwise训练是一种损失采样&quot;&gt;3.4 patchwise训练是一种损失采样&lt;/h5&gt;
&lt;p&gt;在随机优化中，梯度计算是由训练分布支配的。patchwise 训练和全卷积训练能被用来产生任意分布，尽管他们相对的计算效率依赖于重叠域和minibatch的大小。在每一个由所有的单元接受域组成的批次在图像的损失之下（或图像的集合）整张图像的全卷积训练等同于patchwise训练。当这种方式比patches的均匀取样更加高效的同时，它减少了可能的批次数量。然而在一张图片中随机选择patches可能更容易被重新找到。限制基于它的空间位置随机取样子集产生的损失（或者可以说应用输入和输出之间的DropConnect mask &lt;sup id=&quot;fnref:39&quot;&gt;&lt;a href=&quot;#fn:39&quot; class=&quot;footnote&quot;&gt;28&lt;/a&gt;&lt;/sup&gt; ）排除来自梯度计算的patches。&lt;/p&gt;

&lt;p&gt;如果保存下来的patches依然有重要的重叠，全卷积计算依然将加速训练。如果梯度在多重逆推法中被积累，batches能包含几张图的patches。patcheswise训练中的采样能纠正分类失调 &lt;sup id=&quot;fnref:30:5&quot;&gt;&lt;a href=&quot;#fn:30&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:9:9&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:3:6&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt; 和减轻密集空间相关性的影响&lt;sup id=&quot;fnref:31:8&quot;&gt;&lt;a href=&quot;#fn:31&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:17:6&quot;&gt;&lt;a href=&quot;#fn:17&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt;。在全卷积训练中，分类平衡也能通过给损失赋权重实现，对损失采样能被用来标识空间相关。&lt;/p&gt;

&lt;p&gt;我们研究了4.3节中的伴有采样的训练，没有发现对于dense prediction它有更快或是更好的收敛效果。全图式训练是有效且高效的。&lt;/p&gt;

&lt;h4 id=&quot;4-分割架构&quot;&gt;4 分割架构&lt;/h4&gt;

&lt;p&gt;我们将ILSVRC分类应用到FCNs增大它们用于dense prediction结合网内上采样和像素级损失。我们通过微调为分割进行训练。接下来我们增加了跨层来融合粗的、语义的和局部的表征信息。这种跨层式架构能学习端到端来改善输出的语义和空间预测。&lt;/p&gt;

&lt;p&gt;为此，我们训练和在PASCAL VOC 2011分割挑战赛&lt;sup id=&quot;fnref:8&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot;&gt;29&lt;/a&gt;&lt;/sup&gt;中验证。我们训练逐像素的多项式逻辑损失和验证标准度量的在集合中平均像素交集还有基于所有分类上的平均接收，包括背景。这个训练忽略了那些在groud truth中被遮盖的像素（模糊不清或者很难辨认）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/FCN/4.1.png&quot; alt=&quot;4.1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;注：不是每个可能的patch被包含在这种方法中，因为最后一层单位的的接收域依赖一个固定的、步长大的网格。然而，对该图像进行向左或向下随机平移接近该步长个单位，从所有可能的patches 中随机选取或许可以修复这个问题。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/FCN/4.2.png&quot; alt=&quot;4.2&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;41-从分类到dense-fcn&quot;&gt;4.1 从分类到dense FCN&lt;/h5&gt;

&lt;p&gt;我们在第3节中以卷积证明分类架构的。我们认为拿下了ILSVRC12的AlexNet3架构 &lt;sup id=&quot;fnref:22:5&quot;&gt;&lt;a href=&quot;#fn:22&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; 和VGG nets &lt;sup id=&quot;fnref:34:5&quot;&gt;&lt;a href=&quot;#fn:34&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; 、GoogLeNet4 &lt;sup id=&quot;fnref:35:5&quot;&gt;&lt;a href=&quot;#fn:35&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; 一样在ILSVRC14上表现的格外好。我们选择VGG 16层的网络5，发现它和19层的网络在这个任务（分类）上相当。对于GoogLeNet,我们仅仅使用的最后的损失层，通过丢弃了最后的平均池化层提高了表现能力。我们通过丢弃最后的分类切去每层网络头，然后将全连接层转化成卷积层。我们附加了一个1*1的、通道维数为21的卷积来预测每个PASCAL分类（包括背景）的得分在每个粗糙的输出位置，后面紧跟一个去卷积层用来双线性上采样粗糙输出到像素密集输出如3.3.节中描述。表1将初步验证结果和每层的基础特性比较。我们发现最好的结果在以一个固定的学习速率得到（最少175个epochs)。&lt;/p&gt;

&lt;p&gt;从分类到分割的微调对每层网络有一个合理的预测。甚至最坏的模型也能达到大约75%的良好表现。内设分割的VGG网络（FCN-VGG16）已经在val上平均IU 达到了56.0取得了最好的成绩，相比于52.6 &lt;sup id=&quot;fnref:17:7&quot;&gt;&lt;a href=&quot;#fn:17&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt; 。在额外数据上的训练将FCN-VGG16提高到59.4，将FCN-AlexNet提高到48.0。尽管相同的分类准确率，我们的用GoogLeNet并不能和VGG16的分割结果相比较。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/FCN/4.1.1.png&quot; alt=&quot;4.1.1&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;42-结合是什么和在哪里&quot;&gt;4.2 结合“是什么”和“在哪里”&lt;/h5&gt;

&lt;p&gt;我们定义了一个新的全卷积网用于结合了特征层级的分割并提高了输出的空间精度，见图3。当全卷积分类能被微调用于分割如4.1节所示，甚至在标准度量上得分更高，它们的输出不是很粗糙（见图4）。最后预测层的32像素步长限制了上采样输入的细节的尺寸。&lt;/p&gt;

&lt;p&gt;我们提出增加结合了最后预测层和有更细小步长的更低层的跨层信息&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;30&lt;/a&gt;&lt;/sup&gt;，将一个线划拓扑结构转变成DAG(有向无环图)，并且边界将从更底层向前跳跃到更高（图3）。因为它们只能获取更少的像素点，更精细的尺寸预测应该需要更少的层，所以从更浅的网中将它们输出是有道理的。结合了精细层和粗糙层让模型能做出遵从全局结构的局部预测。与Koenderick 和an Doorn &lt;sup id=&quot;fnref:21&quot;&gt;&lt;a href=&quot;#fn:21&quot; class=&quot;footnote&quot;&gt;31&lt;/a&gt;&lt;/sup&gt;的jet类似，我们把这种非线性特征层称之为deep jet。&lt;/p&gt;

&lt;p&gt;我们首先将输出步长分为一半，通过一个16像素步长层预测。我们增加了一个1*1的卷积层在pool4的顶部来产生附加的类别预测。我们将输出和预测融合在conv7（fc7的卷积化）的顶部以步长32计算，通过增加一个2×的上采样层和预测求和（见图3）。我们初始化这个2×上采样到双线性插值，但是允许参数能被学习，如3.3节所描述、最后，步长为16的预测被上采样回图像，我们把这种网结构称为FCN-16s。FCN-16s用来学习端到端，能被最后的参数初始化。这种新的、在pool4上生效的参数是初始化为0 的，所以这种网结构是以未变性的预测开始的。这种学习速率是以100倍的下降的。&lt;/p&gt;

&lt;p&gt;学习这种跨层网络能在3.0平均IU的有效集合上提高到62.4。图4展示了在精细结构输出上的提高。我们将这种融合学习和仅仅从pool4层上学习进行比较，结果表现糟糕，而且仅仅降低了学习速率而没有增加跨层，导致了没有提高输出质量的没有显著提高表现。&lt;/p&gt;

&lt;p&gt;我们继续融合pool3和一个融合了pool4和conv7的2×上采样预测，建立了FCN-8s的网络结构。在平均IU上我们获得了一个较小的附加提升到62.7，然后发现了一个在平滑度和输出细节上的轻微提高。这时我们的融合提高已经得到了一个衰减回馈，既在强调了大规模正确的IU度量的层面上，也在提升显著度上得到反映，如图4所示，所以即使是更低层我们也不需要继续融合。&lt;/p&gt;

&lt;p&gt;其他方式精炼化减少池层的步长是最直接的一种得到精细预测的方法。然而这么做对我们的基于VGG16的网络带来问题。设置pool5的步长到1，要求我们的卷积fc6核大小为14*14来维持它的接收域大小。另外它们的计算代价，通过如此大的滤波器学习非常困难。我们尝试用更小的滤波器重建pool5之上的层，但是并没有得到有可比性的结果；一个可能的解释是ILSVRC在更上层的初始化时非常重要的。&lt;/p&gt;

&lt;p&gt;另一种获得精细预测的方法就是利用3.2节中描述的shift-and-stitch技巧。在有限的实验中，我们发现从这种方法的提升速率比融合层的方法花费的代价更高。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/FCN/4.2.1.png&quot; alt=&quot;4.2.1&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;43-实验框架&quot;&gt;4.3 实验框架&lt;/h5&gt;

&lt;p&gt;优化我们利用momentum训练了GSD。我们利用了一个minibatch大小的20张图片，然后固定学习速率为10-3,10-4，和5-5用于FCN-AlexNet, FCN-VGG16,和FCN-GoogLeNet，通过各自的线性搜索选择。我们利用了0.9的momentum,权值衰减在5-4或是2-4，而且对于偏差的学习速率加倍了，尽管我们发现训练对单独的学习速率敏感。我们零初始化类的得分层，随机初始化既不能产生更好的表现也没有更快的收敛。Dropout被包含在用于原始分类的网络中。&lt;/p&gt;

&lt;p&gt;微调我们通过反向传播微调整个网络的所有层。经过表2的比较，微调单独的输出分类表现只有全微调的70%。考虑到学习基础分类网络所需的时间，从scratch中训练不是可行的。（注意VGG网络的训练是阶段性的，当我们从全16层初始化后）。对于粗糙的FCN-32s，在单GPU上，微调要花费三天的时间，而且大约每隔一天就要更新到FCN-16s和FCN-8s版本。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/FCN/4.3.1.png&quot; alt=&quot;4.3.1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;更多的训练数据PASCAL VOC 2011分割训练设置1112张图片的标签。Hariharan等人 &lt;sup id=&quot;fnref:16&quot;&gt;&lt;a href=&quot;#fn:16&quot; class=&quot;footnote&quot;&gt;32&lt;/a&gt;&lt;/sup&gt; 为一个更大的8498的PASCAL训练图片集合收集标签，被用于训练先前的先进系统,SDS &lt;sup id=&quot;fnref:17:8&quot;&gt;&lt;a href=&quot;#fn:17&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt; 。训练数据将FCV-VGG16得分提高了3.4个百分点到59.4。&lt;/p&gt;

&lt;p&gt;patch取样正如3.4节中解释的，我们的全图有效地训练每张图片batches到常规的、大的、重叠的patches网格。相反的，先前工作随机样本patches在一整个数据集 &lt;sup id=&quot;fnref:30:6&quot;&gt;&lt;a href=&quot;#fn:30&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:3:7&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:9:10&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:31:9&quot;&gt;&lt;a href=&quot;#fn:31&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:11:8&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt; ，可能导致更高的方差batches，可能加速收敛 &lt;sup id=&quot;fnref:24&quot;&gt;&lt;a href=&quot;#fn:24&quot; class=&quot;footnote&quot;&gt;33&lt;/a&gt;&lt;/sup&gt; 。我们通过空间采样之前方式描述的损失研究这种折中，以1-p的概率做出独立选择来忽略每个最后层单元。为了避免改变有效的批次尺寸，我们同时以因子1/p增加每批次图像的数量。注意的是因为卷积的效率，在足够大的p值下，这种拒绝采样的形式依旧比patchwose训练要快（比如，根据3.1节的数量，最起码p&amp;gt;0.2）图5展示了这种收敛的采样的效果。我们发现采样在收敛速率上没有很显著的效果相对于全图式训练，但是由于每个每个批次都需要大量的图像，很明显的需要花费更多的时间。&lt;/p&gt;

&lt;p&gt;分类平衡全卷积训练能通过按权重或对损失采样平衡类别。尽管我们的标签有轻微的不平衡（大约3/4是背景），我们发现类别平衡不是必要的。dense prediction分数是通过网内的去卷积层上采样到输出维度。最后层去卷积滤波被固定为双线性插值，当中间采样层是被初始化为双线性上采样，然后学习。扩大我们尝试通过随机反射扩大训练数据，”jettering”图像通过将它们在每个方向上转化成32像素（最粗糙预测的尺寸）。这并没有明显的改善。实现所有的模型都是在单NVIDIA Tesla K40c上用Caffe&lt;sup id=&quot;fnref:20&quot;&gt;&lt;a href=&quot;#fn:20&quot; class=&quot;footnote&quot;&gt;34&lt;/a&gt;&lt;/sup&gt;训练和学习。&lt;/p&gt;

&lt;h4 id=&quot;5-结果&quot;&gt;5 结果&lt;/h4&gt;

&lt;p&gt;我们训练FCN在语义分割和场景解析，研究了PASCAL VOC, NYUDv2和 SIFT Flow。尽管这些任务在以前主要是用在物体和区域上，我们都一律将它们视为像素预测。我们在这些数据集中都进行测试用来评估我们的FCN跨层式架构，然后对于NYUDv2将它扩展成一个多模型的输出，对于SIFT Flow则扩展成多任务的语义和集合标签。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;度量&lt;/strong&gt; 我们从常见的语义分割和场景解析评估中提出四种度量，它们在像素准确率和在联合的区域交叉上是不同的。令n_ij为类别i的被预测为类别j的像素数量，有n_ij个不同的类别，令
&lt;img src=&quot;/images/posts/FCN/5.1.png&quot; alt=&quot;5.1&quot; /&gt;
为类别i的像素总的数量。我们将计算：
&lt;img src=&quot;/images/posts/FCN/5.2.png&quot; alt=&quot;5.2&quot; /&gt;
&lt;strong&gt;PASCAL VOC&lt;/strong&gt; 表3给出了我们的FCN-8s的在PASCAL VOC2011和2012测试集上的表现，然后将它和之前的先进方法SDS[17]和著名的R-CNN[12]进行比较。我们在平均IU上取得了最好的结果相对提升了20%。推理时间被降低了114×（只有卷积网，没有proposals和微调)或者286×（全部都有）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/FCN/5.3.png&quot; alt=&quot;5.3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;NVUDv2&lt;/strong&gt; &lt;sup id=&quot;fnref:33&quot;&gt;&lt;a href=&quot;#fn:33&quot; class=&quot;footnote&quot;&gt;35&lt;/a&gt;&lt;/sup&gt;是一种通过利用Microsoft Kinect收集到的RGB-D数据集，含有已经被合并进Gupt等人[14]的40类别的语义分割任务的pixelwise标签。我们报告结果基于标准分离的795张图片和654张测试图片。（注意：所有的模型选择将展示在PASCAL 2011 val上)。表4给出了我们模型在一些变化上的表现。首先我们在RGB图片上训练我们的未经修改的粗糙模型（FCN-32s）。为了添加深度信息，我们训练模型升级到能采用4通道RGB-Ds的输入（早期融合）。这提供了一点便利，也许是由于模型一直要传播有意义的梯度的困难。紧随Gupta等人&lt;sup id=&quot;fnref:15:4&quot;&gt;&lt;a href=&quot;#fn:15&quot; class=&quot;footnote&quot;&gt;16&lt;/a&gt;&lt;/sup&gt;的成功，我们尝试3维的HHA编码深度，只在这个信息上（即深度）训练网络，和RGB与HHA的“后期融合”一样来自这两个网络中的预测将在最后一层进行总结，结果的双流网络将进行端到端的学习。最后我们将这种后期融合网络升级到16步长的版本。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/FCN/5.4.png&quot; alt=&quot;5.4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SIFT Flow&lt;/strong&gt;是一个带有33语义范畴（“桥”、“山”、“太阳”）的像素标签的2688张图片的数据集和3个几何分类（“水平”、“垂直”和“sky”)一样。一个FCN能自然学习共同代表权，即能同时预测标签的两种类别。我们学习FCN-16s的一种双向版本结合语义和几何预测层和损失。这种学习模型在这两种任务上作为独立的训练模型表现很好，同时它的学习和推理基本上和每个独立的模型一样快。表5的结果显示，计算在标准分离的2488张训练图片和200张测试图片上计算，在这两个任务上都表现的极好。&lt;/p&gt;

&lt;h4 id=&quot;6-结论&quot;&gt;6 结论&lt;/h4&gt;

&lt;p&gt;全卷积网络是模型非常重要的部分，是现代化分类网络中一个特殊的例子。认识到这个，将这些分类网络扩展到分割并通过多分辨率的层结合显著提高先进的技术，同时简化和加速学习和推理。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/FCN/6.1.png&quot; alt=&quot;6.1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;鸣谢&lt;/strong&gt; 这项工作有以下部分支持DARPA’s MSEE和SMISC项目，NSF awards IIS-1427425, IIS-1212798, IIS-1116411, 还有NSF GRFP,Toyota, 还有 Berkeley Vision和Learning Center。我们非常感谢NVIDIA捐赠的GPU。我们感谢Bharath Hariharan 和Saurabh Gupta的建议和数据集工具;我们感谢Sergio Guadarrama 重构了Caffe里的GoogLeNet;我们感谢Jitendra Malik的有帮助性评论;感谢Wei Liu指出了我们SIFT Flow平均IU计算上的一个问题和频率权重平均IU公式的错误。&lt;/p&gt;

&lt;h4 id=&quot;附录a-iu上界&quot;&gt;附录A IU上界&lt;/h4&gt;
&lt;p&gt;在这篇论文中，我们已经在平均IU分割度量上取到了很好的效果，即使是粗糙的语义预测。为了更好的理解这种度量还有关于这种方法的限制，我们在计算不同的规模上预测的表现的大致上界。我们通过下采样ground truth图像，然后再次对它们进行上采样，来模拟可以获得最好的结果，其伴随着特定的下采样因子。下表给出了不同下采样因子在PASCAL2011 val的一个子集上的平均IU。pixel-perfect预测很显然在取得最最好效果上不是必须的，而且，相反的，平均IU不是一个好的精细准确度的测量标准。&lt;/p&gt;

&lt;h4 id=&quot;附录b-更多的结果&quot;&gt;附录B 更多的结果&lt;/h4&gt;
&lt;p&gt;我们将我们的FCN用于语义分割进行了更进一步的评估。PASCAL-Context [29] 提供了PASCAL VOC 2011的全部场景注释。有超过400中不同的类别，我们遵循了 [29] 定义的被引用最频繁的59种类任务。我们分别训练和评估了训练集和val集。在表6中，我们将联合对象和Convolutional Feature Masking [4] 的stuff variation进行比较，后者是之前这项任务中最好的方法。FCN-8s在平均IU上得分为37.8，相对提高了20%。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/FCN/7.1.png&quot; alt=&quot;7.1&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;变更记录&quot;&gt;变更记录&lt;/h4&gt;
&lt;p&gt;论文的arXiv版本保持着最新的修正和其他的相关材料，接下来给出一份简短的变更历史。v2 添加了附录A和附录B。修正了PASCAL的有效数量（之前一些val图像被包含在训练中），SIFT Flow平均IU（用的不是很规范的度量），还有频率权重平均IU公式的一个错误。添加了模型和更新时间数字来反映改进的实现的链接（公开可用的）。&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;参考文献&quot;&gt;参考文献&lt;/h4&gt;

&lt;p&gt;arXiv:1408.5093, 2014. 7&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:22&quot;&gt;
      &lt;p&gt;A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012. 1, 2, 3, 5 &lt;a href=&quot;#fnref:22&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:22:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:22:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:22:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:22:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:22:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:34&quot;&gt;
      &lt;p&gt;K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR,abs/1409.1556, 2014. 1, 2, 3, 5 &lt;a href=&quot;#fnref:34&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:34:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:34:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:34:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:34:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:34:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:35&quot;&gt;
      &lt;p&gt;C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A.Rabinovich. Going deeper with convolutions. CoRR, abs/1409.4842,2014. 1, 2, 3, 5 &lt;a href=&quot;#fnref:35&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:35:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:35:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:35:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:35:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:35:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,E. Tzeng, and T. Darrell. DeCAF: A deep convolutional activation feature for generic visual recognition. In ICML, 2014.1, 2 &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:5:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:32&quot;&gt;
      &lt;p&gt;P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2014.1, 2, 4 &lt;a href=&quot;#fnref:32&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:32:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:32:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:32:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:12&quot;&gt;
      &lt;p&gt;R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Computer Vision and Pattern Recognition,2014. 1, 2, 7 &lt;a href=&quot;#fnref:12&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:12:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:12:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:19&quot;&gt;
      &lt;p&gt;K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV, 2014. 1, 2 &lt;a href=&quot;#fnref:19&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:19:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:42&quot;&gt;
      &lt;p&gt;N. Zhang, J. Donahue, R. Girshick, and T. Darrell. Partbased r-cnns for fine-grained category detection. In Computer Vision–ECCV 2014, pages 834–849. Springer, 2014.1 &lt;a href=&quot;#fnref:42&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:26&quot;&gt;
      &lt;p&gt;J. Long, N. Zhang, and T. Darrell. Do convnets learn correspondence?In NIPS, 2014. 1 &lt;a href=&quot;#fnref:26&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:26:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:10&quot;&gt;
      &lt;p&gt;P. Fischer, A. Dosovitskiy, and T. Brox. Descriptor matching with convolutional neural networks: a comparison to SIFT.CoRR, abs/1405.5769, 2014. 1 &lt;a href=&quot;#fnref:10&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:30&quot;&gt;
      &lt;p&gt;F. Ning, D. Delhomme, Y. LeCun, F. Piano, L. Bottou, and P. E. Barbano. Toward automatic phenotyping of developing embryos from videos. Image Processing, IEEE Transactions on, 14(9):1360–1371, 2005. 1, 2, 4, 7 &lt;a href=&quot;#fnref:30&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:30:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:30:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:30:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:30:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:30:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:30:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;D. C. Ciresan, A. Giusti, L. M. Gambardella, and J. Schmidhuber.Deep neural networks segment neuronal membranes in electron microscopy images. In NIPS, pages 2852–2860,2012. 1, 2, 4, 7 &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:3:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:3:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:3:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:3:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:3:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:3:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:3:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:9&quot;&gt;
      &lt;p&gt;C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Learning hierarchical features for scene labeling. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 2013. 1, 2, 4,7, 8 &lt;a href=&quot;#fnref:9&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:9:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:9:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:9:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:9:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:9:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:9:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:9:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:9:8&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:9:9&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:9:10&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;11&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:31&quot;&gt;
      &lt;p&gt;P. H. Pinheiro and R. Collobert. Recurrent convolutional neural networks for scene labeling. In ICML, 2014. 1, 2,4, 7, 8 &lt;a href=&quot;#fnref:31&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:31:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:31:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:31:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:31:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:31:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:31:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:31:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:31:8&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:31:9&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:17&quot;&gt;
      &lt;p&gt;B. Hariharan, P. Arbel´aez, R. Girshick, and J. Malik. Simultaneous detection and segmentation. In European Conference on Computer Vision (ECCV), 2014. 1, 2, 4, 5, 7, 8 &lt;a href=&quot;#fnref:17&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:17:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:17:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:17:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:17:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:17:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:17:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:17:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:17:8&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:15&quot;&gt;
      &lt;p&gt;S. Gupta, R. Girshick, P. Arbelaez, and J. Malik. Learning rich features from RGB-D images for object detection and segmentation. In ECCV. Springer, 2014. 1, 2, 8 &lt;a href=&quot;#fnref:15&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:15:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:15:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:15:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:15:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:11&quot;&gt;
      &lt;p&gt;Y. Ganin and V. Lempitsky. N4-fields: Neural network nearest neighbor fields for image transforms. In ACCV, 2014. 1,2, 7 &lt;a href=&quot;#fnref:11&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:11:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:11:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:11:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:11:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:11:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:11:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:11:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:11:8&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:41&quot;&gt;
      &lt;p&gt;M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In Computer Vision–ECCV 2014,pages 818–833. Springer, 2014. 2 &lt;a href=&quot;#fnref:41&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:28&quot;&gt;
      &lt;p&gt;O. Matan, C. J. Burges, Y. LeCun, and J. S. Denker. Multidigit recognition using a space displacement neural network.In NIPS, pages 488–495. Citeseer, 1991. 2 &lt;a href=&quot;#fnref:28&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:23&quot;&gt;
      &lt;p&gt;Y. LeCun, B. Boser, J. Denker, D. Henderson, R. E. Howard,W. Hubbard, and L. D. Jackel. Backpropagation applied to hand-written zip code recognition. In Neural Computation,1989. 2, 3 &lt;a href=&quot;#fnref:23&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:23:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:40&quot;&gt;
      &lt;p&gt;R. Wolf and J. C. Platt. Postal address block location using a convolutional locator network. Advances in Neural Information Processing Systems, pages 745–745, 1994. 2 &lt;a href=&quot;#fnref:40&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;D. Eigen, D. Krishnan, and R. Fergus. Restoring an image taken through a window covered with dirt or rain. In Computer Vision (ICCV), 2013 IEEE International Conference on, pages 633–640. IEEE, 2013. 2 &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:6:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:6:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:38&quot;&gt;
      &lt;p&gt;J. Tompson, A. Jain, Y. LeCun, and C. Bregler. Joint training of a convolutional network and a graphical model for human pose estimation. CoRR, abs/1406.2984, 2014. 2 &lt;a href=&quot;#fnref:38&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot;&gt;
      &lt;p&gt;D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction from a single image using a multi-scale deep network. arXiv preprint arXiv:1406.2283, 2014. 2 &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:7:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:18&quot;&gt;
      &lt;p&gt;B. Hariharan, P. Arbel´aez, R. Girshick, and J. Malik. Hypercolumns for object segmentation and fine-grained localization.In Computer Vision and Pattern Recognition, 2015.2 &lt;a href=&quot;#fnref:18&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:13&quot;&gt;
      &lt;p&gt;A. Giusti, D. C. Cires¸an, J. Masci, L. M. Gambardella, and J. Schmidhuber. Fast image scanning with deep max-pooling convolutional neural networks. In ICIP, 2013. 3, 4 &lt;a href=&quot;#fnref:13&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:13:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:27&quot;&gt;
      &lt;p&gt;S. Mallat. A wavelet tour of signal processing. Academic press, 2nd edition, 1999. 4 &lt;a href=&quot;#fnref:27&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:39&quot;&gt;
      &lt;p&gt;L. Wan, M. Zeiler, S. Zhang, Y. L. Cun, and R. Fergus. Regularization of neural networks using dropconnect. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 1058–1066, 2013. 4 &lt;a href=&quot;#fnref:39&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot;&gt;
      &lt;p&gt;M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2011 (VOC2011) Results. &lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;C. M. Bishop. Pattern recognition and machine learning,page 229. Springer-Verlag New York, 2006. 6 &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:21&quot;&gt;
      &lt;p&gt;J. J. Koenderink and A. J. van Doorn. Representation of local geometry in the visual system. Biological cybernetics,55(6):367–375, 1987. 6 &lt;a href=&quot;#fnref:21&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:16&quot;&gt;
      &lt;p&gt;B. Hariharan, P. Arbelaez, L. Bourdev, S. Maji, and J. Malik.Semantic contours from inverse detectors. In International Conference on Computer Vision (ICCV), 2011. 7 &lt;a href=&quot;#fnref:16&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:24&quot;&gt;
      &lt;p&gt;Y. A. LeCun, L. Bottou, G. B. Orr, and K.-R. M¨uller. Efficient backprop. In Neural networks: Tricks of the trade,pages 9–48. Springer, 1998. 7 &lt;a href=&quot;#fnref:24&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:20&quot;&gt;
      &lt;p&gt;Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint &lt;a href=&quot;#fnref:20&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:33&quot;&gt;
      &lt;p&gt;N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, 2012. 8 &lt;a href=&quot;#fnref:33&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Mon, 07 Jan 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/01/FCN/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/01/FCN/</guid>
        
        <category>深度学习-视觉</category>
        
        
        <category>深度学习-视觉</category>
        
      </item>
    
      <item>
        <title>github pages添加阅读量</title>
        <description>&lt;!--more--&gt;

&lt;p&gt;标签： github&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;前提是已经搭建好github-pages&quot;&gt;前提是已经搭建好github pages&lt;/h3&gt;

&lt;h3 id=&quot;1注册leancloud&quot;&gt;1.注册LeanCloud&lt;/h3&gt;

&lt;h3 id=&quot;2创建应用申请appid和appkey&quot;&gt;2.创建应用，申请appid和appkey&lt;/h3&gt;

&lt;h3 id=&quot;3创建一个class随便起名例如counter&quot;&gt;3.创建一个class，随便起名，例如Counter&lt;/h3&gt;

&lt;h3 id=&quot;4修改代码&quot;&gt;4.修改代码:&lt;/h3&gt;

&lt;h5 id=&quot;1-_configyml文件&quot;&gt;1. _config.yml文件&lt;/h5&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;leancloud:
enable: true
app_id: your_id
app_key: your_key
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;2添加includeleancloud-analyticshtml&quot;&gt;2.添加include/leancloud-analytics.html&lt;/h5&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;

&amp;lt;script src=&quot;https://code.jquery.com/jquery-3.2.0.min.js&quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;script src=&quot;https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js&quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;script&amp;gt;AV.initialize(&quot;TKFClR9mFN7woW6NwHuQAxDb-gzGzoHsz&quot;, &quot;8NibCKPlTVw5om1DF1dMaQvN&quot;);&amp;lt;/script&amp;gt;
&amp;lt;script&amp;gt;
function showHitCount(Counter) {
var query = new AV.Query(Counter);
var entries = [];
var $visitors = $(&quot;.leancloud_visitors&quot;);
$visitors.each(function () {
entries.push( $(this).attr(&quot;id&quot;).trim() );
});
query.containedIn('url', entries);
query.find()
.done(function (results) {
console.log(&quot;results&quot;,results);
var COUNT_CONTAINER_REF = '.leancloud-visitors-count';
if (results.length === 0) {
$visitors.find(COUNT_CONTAINER_REF).text(0);
return;
}
for (var i = 0; i &amp;lt; results.length; i++) {
var item = results[i];
var url = item.get('url');
var hits = item.get('hits');
var element = document.getElementById(url);
$(element).find(COUNT_CONTAINER_REF).text(hits);
}
for(var i = 0; i &amp;lt; entries.length; i++) {
var url = entries[i];
var element = document.getElementById(url);
var countSpan = $(element).find(COUNT_CONTAINER_REF);
if( countSpan.text() == '') {
countSpan.text(0);
}
}
})
.fail(function (object, error) {
console.log(&quot;Error: &quot; + error.code + &quot; &quot; + error.message);
});
}
function addCount(Counter) {
var $visitors = $(&quot;.leancloud_visitors&quot;);
var url = $visitors.attr('id').trim();
var title = $visitors.attr('data-flag-title').trim();
var query = new AV.Query(Counter);
query.equalTo(&quot;url&quot;, url);
query.find({
success: function(results) {
if (results.length &amp;gt; 0) {
var counter = results[0];
counter.fetchWhenSave(true);
counter.increment(&quot;hits&quot;);
counter.save(null, {
success: function(counter) {
var $element = $(document.getElementById(url));
$element.find('.leancloud-visitors-count').text(counter.get('hits'));
},
error: function(counter, error) {
console.log('Failed to save Visitor num, with error message: ' + error.message);
}
});
} else {
var newcounter = new Counter();
/* Set ACL */
var acl = new AV.ACL();
acl.setPublicReadAccess(true);
acl.setPublicWriteAccess(true);
newcounter.setACL(acl);
/* End Set ACL */
newcounter.set(&quot;title&quot;, title);
newcounter.set(&quot;url&quot;, url);
newcounter.set(&quot;hits&quot;, 1);
newcounter.save(null, {
success: function(newcounter) {
var $element = $(document.getElementById(url));
$element.find('.leancloud-visitors-count').text(newcounter.get('hits'));
},
error: function(newcounter, error) {
console.log('Failed to create');
}
});
}
},
error: function(error) {
console.log('Error:' + error.code + &quot; &quot; + error.message);
}
});
}
$(function() {
var Counter = AV.Object.extend(&quot;Counter&quot;);
if ($('.leancloud_visitors').length == 1) {
// in post.html, so add 1 to hit counts
addCount(Counter);
} else if ($('.post-link').length &amp;gt; 1){
// in index.html, there are many 'leancloud_visitors' and 'post-link', so just show hit counts.
showHitCount(Counter);
}
});
&amp;lt;/script&amp;gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;3_layoutsdefaulthtml&quot;&gt;3._layouts/default.html&lt;/h5&gt;
&lt;p&gt;添加&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&amp;lt;script src=&quot;https://code.jquery.com/jquery-3.2.0.min.js&quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;script src=&quot;https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js&quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;script&amp;gt;AV.initialize(&quot;TKFClR9mFN7woW6NwHuQAxDb-gzGzoHsz&quot;, &quot;8NibCKPlTVw5om1DF1dMaQvN&quot;);&amp;lt;/script&amp;gt;
&amp;lt;!--&amp;lt;script&amp;gt;console.log(&quot;Error: &quot; + error.code + &quot; &quot; + error.message);&amp;lt;/script&amp;gt;--&amp;gt;
&amp;lt;script&amp;gt;
    function showHitCount(Counter) {
        console.log(&quot;i was called&quot;);
        var query = new AV.Query(Counter);
        var entries = [];
        var $visitors = $(&quot;.leancloud_visitors&quot;);
        $visitors.each(function () {
            entries.push( $(this).attr(&quot;id&quot;).trim() );
        });
        query.containedIn('url', entries);
        query.find()
                .done(function (results) {
                    console.log(&quot;results&quot;,results);
                    var COUNT_CONTAINER_REF = '.leancloud-visitors-count';
                    if (results.length === 0) {
                        $visitors.find(COUNT_CONTAINER_REF).text(0);
                        return;
                    }
                    for (var i = 0; i &amp;lt; results.length; i++) {
                        var item = results[i];
                        var url = item.get('url');
                        var hits = item.get('hits');
                        var element = document.getElementById(url);
                        $(element).find(COUNT_CONTAINER_REF).text(hits);
                    }
                    for(var i = 0; i &amp;lt; entries.length; i++) {
                        var url = entries[i];
                        var element = document.getElementById(url);
                        var countSpan = $(element).find(COUNT_CONTAINER_REF);
                        if( countSpan.text() == '') {
                            countSpan.text(0);
                        }
                    }
                })
                .fail(function (object, error) {
                    console.log(&quot;Error: &quot; + error.code + &quot; &quot; + error.message);
                });
    }
    function addCount(Counter) {
        var $visitors = $(&quot;.leancloud_visitors&quot;);
        var url = $visitors.attr('id').trim();
        var title = $visitors.attr('data-flag-title').trim();
        var query = new AV.Query(Counter);
        query.equalTo(&quot;url&quot;, url);
        query.find({
            success: function(results) {
                if (results.length &amp;gt; 0) {
                    var counter = results[0];
                    counter.fetchWhenSave(true);
                    counter.increment(&quot;hits&quot;);
                    counter.save(null, {
                        success: function(counter) {
                            var $element = $(document.getElementById(url));
                            $element.find('.leancloud-visitors-count').text(counter.get('hits'));
                        },
                        error: function(counter, error) {
                            console.log('Failed to save Visitor num, with error message: ' + error.message);
                        }
                    });
                } else {
                    var newcounter = new Counter();
                    /* Set ACL */
                    var acl = new AV.ACL();
                    acl.setPublicReadAccess(true);
                    acl.setPublicWriteAccess(true);
                    newcounter.setACL(acl);
                    /* End Set ACL */
                    newcounter.set(&quot;title&quot;, title);
                    newcounter.set(&quot;url&quot;, url);
                    newcounter.set(&quot;hits&quot;, 1);
                    newcounter.save(null, {
                        success: function(newcounter) {
                            var $element = $(document.getElementById(url));
                            $element.find('.leancloud-visitors-count').text(newcounter.get('hits'));
                        },
                        error: function(newcounter, error) {
                            console.log('Failed to create');
                        }
                    });
                }
            },
            error: function(error) {
                console.log('Error:' + error.code + &quot; &quot; + error.message);
            }
        });
    }
    $(function() {
        var Counter = AV.Object.extend(&quot;Counter&quot;);
        console.log('this is a test');
        console.log('this is a test-add',$('.leancloud_visitors'));
        console.log('this is a test-show',$('.post-link'));
        if ($('.leancloud_visitors').length == 1) {
            // in post.html, so add 1 to hit counts
            addCount(Counter);
        } else if ($('.post-link').length &amp;gt; 1){
            // in index.html, there are many 'leancloud_visitors' and 'post-link', so just show hit counts.
            showHitCount(Counter);
        }
    });
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;4_layoutsposthtml&quot;&gt;4._layouts/post.html&lt;/h5&gt;
&lt;p&gt;添加&lt;/p&gt;
&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;span&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;id=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/2019/01/addreadvalue/&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;leancloud_visitors&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;data-flag-title=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;github pages添加阅读量&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;span&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;post-meta-divider&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;|&lt;span class=&quot;nt&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;span&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;post-meta-item-text&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt; Hits:  &lt;span class=&quot;nt&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;span&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;leancloud-visitors-count&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/span&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;reference:&lt;/p&gt;

&lt;p&gt;[1]&lt;a href=&quot;https://blog.csdn.net/u013553529/article/details/63357382&quot;&gt;在个人博客中添加文章点击次数&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2]&lt;a href=&quot;https://github.com/galian123/galian123.github.io&quot;&gt;github&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 06 Jan 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/01/addreadvalue/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/01/addreadvalue/</guid>
        
        <category>github</category>
        
        
        <category>github</category>
        
      </item>
    
      <item>
        <title>NVIDIA tx2刷机教程</title>
        <description>&lt;!--more--&gt;

&lt;p&gt;上个月买了个tx2开发板，跑一些深度模型，记录一下刷机教程。&lt;/p&gt;

</description>
        <pubDate>Sat, 05 Jan 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/01/tx2shuaji/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/01/tx2shuaji/</guid>
        
        <category>硬件</category>
        
        
        <category>硬件</category>
        
      </item>
    
      <item>
        <title>MarkDown支持的格式样例（from作业部落）</title>
        <description>&lt;!--more--&gt;

&lt;p&gt;标签： 杂记&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;我们理解您需要更便捷更高效的工具记录思想，整理笔记、知识，并将其中承载的价值传播给他人，&lt;strong&gt;Cmd Markdown&lt;/strong&gt; 是我们给出的答案 —— 我们为记录思想和分享知识提供更专业的工具。 您可以使用 Cmd Markdown：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;整理知识，学习笔记&lt;/li&gt;
    &lt;li&gt;发布日记，杂文，所见所想&lt;/li&gt;
    &lt;li&gt;撰写发布技术文稿（代码支持）&lt;/li&gt;
    &lt;li&gt;撰写发布学术论文（LaTeX 公式支持）&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://www.zybuluo.com/static/img/logo.png&quot; alt=&quot;cmd-markdown-logo&quot; /&gt;&lt;/p&gt;

&lt;p&gt;除了您现在看到的这个 Cmd Markdown 在线版本，您还可以前往以下网址下载：&lt;/p&gt;

&lt;h3 id=&quot;windowsmaclinux-全平台客户端&quot;&gt;&lt;a href=&quot;https://www.zybuluo.com/cmd/&quot;&gt;Windows/Mac/Linux 全平台客户端&lt;/a&gt;&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;请保留此份 Cmd Markdown 的欢迎稿兼使用说明，如需撰写新稿件，点击顶部工具栏右侧的 &lt;i class=&quot;icon-file&quot;&gt;&lt;/i&gt; &lt;strong&gt;新文稿&lt;/strong&gt; 或者使用快捷键 &lt;code class=&quot;highlighter-rouge&quot;&gt;Ctrl+Alt+N&lt;/code&gt;。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;什么是-markdown&quot;&gt;什么是 Markdown&lt;/h2&gt;

&lt;p&gt;Markdown 是一种方便记忆、书写的纯文本标记语言，用户可以使用这些标记符号以最小的输入代价生成极富表现力的文档：譬如您正在阅读的这份文档。它使用简单的符号标记不同的标题，分割不同的段落，&lt;strong&gt;粗体&lt;/strong&gt; 或者 &lt;em&gt;斜体&lt;/em&gt; 某些文字，更棒的是，它还可以&lt;/p&gt;

&lt;h3 id=&quot;1-制作一份待办事宜-todo-列表&quot;&gt;1. 制作一份待办事宜 &lt;a href=&quot;https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#13-待办事宜-todo-列表&quot;&gt;Todo 列表&lt;/a&gt;&lt;/h3&gt;

&lt;ul class=&quot;task-list&quot;&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;支持以 PDF 格式导出文稿&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;改进 Cmd 渲染算法，使用局部渲染技术提高渲染效率&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;新增 Todo 列表功能&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;修复 LaTex 公式渲染问题&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;新增 LaTex 公式编号功能&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-书写一个质能守恒公式&quot;&gt;2. 书写一个质能守恒公式&lt;sup id=&quot;fnref:LaTeX&quot;&gt;&lt;a href=&quot;#fn:LaTeX&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E=mc^2&lt;/script&gt;

&lt;h3 id=&quot;3-高亮一段代码&quot;&gt;3. 高亮一段代码&lt;sup id=&quot;fnref:code&quot;&gt;&lt;a href=&quot;#fn:code&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nd&quot;&gt;@requires_authorization&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;SomeClass&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'__main__'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# A comment&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'hello world'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;4-高效绘制-流程图暂不支持&quot;&gt;4. 高效绘制 &lt;a href=&quot;https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#7-流程图&quot;&gt;流程图&lt;/a&gt;（暂不支持）&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-flow&quot;&gt;st=&amp;gt;start: Start
op=&amp;gt;operation: Your Operation
cond=&amp;gt;condition: Yes or No?
e=&amp;gt;end

st-&amp;gt;op-&amp;gt;cond
cond(yes)-&amp;gt;e
cond(no)-&amp;gt;op
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;5-高效绘制-序列图暂不支持&quot;&gt;5. 高效绘制 &lt;a href=&quot;https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#8-序列图&quot;&gt;序列图&lt;/a&gt;（暂不支持）&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-seq&quot;&gt;Alice-&amp;gt;Bob: Hello Bob, how are you?
Note right of Bob: Bob thinks
Bob--&amp;gt;Alice: I am good thanks!
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;6-高效绘制-甘特图-暂不支持&quot;&gt;6. 高效绘制 &lt;a href=&quot;https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#9-甘特图&quot;&gt;甘特图&lt;/a&gt; (暂不支持)&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-gantt&quot;&gt;    title 项目开发流程
    section 项目确定
        需求分析       :a1, 2016-06-22, 3d
        可行性报告     :after a1, 5d
        概念验证       : 5d
    section 项目实施
        概要设计      :2016-07-05  , 5d
        详细设计      :2016-07-08, 10d
        编码          :2016-07-15, 10d
        测试          :2016-07-22, 5d
    section 发布验收
        发布: 2d
        验收: 3d
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;7-绘制表格&quot;&gt;7. 绘制表格&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;项目&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;价格&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;数量&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;计算机&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;$1600&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;手机&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;$12&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;管线&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;$1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;234&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;8-更详细语法说明&quot;&gt;8. 更详细语法说明&lt;/h3&gt;

&lt;p&gt;想要查看更详细的语法说明，可以参考我们准备的 &lt;a href=&quot;https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown&quot;&gt;Cmd Markdown 简明语法手册&lt;/a&gt;，进阶用户可以参考 &lt;a href=&quot;https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#cmd-markdown-高阶语法手册&quot;&gt;Cmd Markdown 高阶语法手册&lt;/a&gt; 了解更多高级功能。&lt;/p&gt;

&lt;p&gt;总而言之，不同于其它 &lt;em&gt;所见即所得&lt;/em&gt; 的编辑器：你只需使用键盘专注于书写文本内容，就可以生成印刷级的排版格式，省却在键盘和工具栏之间来回切换，调整内容和格式的麻烦。&lt;strong&gt;Markdown 在流畅的书写和印刷级的阅读体验之间找到了平衡。&lt;/strong&gt; 目前它已经成为世界上最大的技术分享网站 GitHub 和 技术问答网站 StackOverFlow 的御用书写格式。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;什么是-cmd-markdown&quot;&gt;什么是 Cmd Markdown&lt;/h2&gt;

&lt;p&gt;您可以使用很多工具书写 Markdown，但是 Cmd Markdown 是这个星球上我们已知的、最好的 Markdown 工具——没有之一 ：）因为深信文字的力量，所以我们和你一样，对流畅书写，分享思想和知识，以及阅读体验有极致的追求，我们把对于这些诉求的回应整合在 Cmd Markdown，并且一次，两次，三次，乃至无数次地提升这个工具的体验，最终将它演化成一个 &lt;strong&gt;编辑/发布/阅读&lt;/strong&gt; Markdown 的在线平台——您可以在任何地方，任何系统/设备上管理这里的文字。&lt;/p&gt;

&lt;h3 id=&quot;1-实时同步预览&quot;&gt;1. 实时同步预览&lt;/h3&gt;

&lt;p&gt;我们将 Cmd Markdown 的主界面一分为二，左边为&lt;strong&gt;编辑区&lt;/strong&gt;，右边为&lt;strong&gt;预览区&lt;/strong&gt;，在编辑区的操作会实时地渲染到预览区方便查看最终的版面效果，并且如果你在其中一个区拖动滚动条，我们有一个巧妙的算法把另一个区的滚动条同步到等价的位置，超酷！&lt;/p&gt;

&lt;h3 id=&quot;2-编辑工具栏&quot;&gt;2. 编辑工具栏&lt;/h3&gt;

&lt;p&gt;也许您还是一个 Markdown 语法的新手，在您完全熟悉它之前，我们在 &lt;strong&gt;编辑区&lt;/strong&gt; 的顶部放置了一个如下图所示的工具栏，您可以使用鼠标在工具栏上调整格式，不过我们仍旧鼓励你使用键盘标记格式，提高书写的流畅度。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.zybuluo.com/static/img/toolbar-editor.png&quot; alt=&quot;tool-editor&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3-编辑模式&quot;&gt;3. 编辑模式&lt;/h3&gt;

&lt;p&gt;完全心无旁骛的方式编辑文字：点击 &lt;strong&gt;编辑工具栏&lt;/strong&gt; 最右侧的拉伸按钮或者按下 &lt;code class=&quot;highlighter-rouge&quot;&gt;Ctrl + M&lt;/code&gt;，将 Cmd Markdown 切换到独立的编辑模式，这是一个极度简洁的写作环境，所有可能会引起分心的元素都已经被挪除，超清爽！&lt;/p&gt;

&lt;h3 id=&quot;4-实时的云端文稿&quot;&gt;4. 实时的云端文稿&lt;/h3&gt;

&lt;p&gt;为了保障数据安全，Cmd Markdown 会将您每一次击键的内容保存至云端，同时在 &lt;strong&gt;编辑工具栏&lt;/strong&gt; 的最右侧提示 &lt;code class=&quot;highlighter-rouge&quot;&gt;已保存&lt;/code&gt; 的字样。无需担心浏览器崩溃，机器掉电或者地震，海啸——在编辑的过程中随时关闭浏览器或者机器，下一次回到 Cmd Markdown 的时候继续写作。&lt;/p&gt;

&lt;h3 id=&quot;5-离线模式&quot;&gt;5. 离线模式&lt;/h3&gt;

&lt;p&gt;在网络环境不稳定的情况下记录文字一样很安全！在您写作的时候，如果电脑突然失去网络连接，Cmd Markdown 会智能切换至离线模式，将您后续键入的文字保存在本地，直到网络恢复再将他们传送至云端，即使在网络恢复前关闭浏览器或者电脑，一样没有问题，等到下次开启 Cmd Markdown 的时候，她会提醒您将离线保存的文字传送至云端。简而言之，我们尽最大的努力保障您文字的安全。&lt;/p&gt;

&lt;h3 id=&quot;6-管理工具栏&quot;&gt;6. 管理工具栏&lt;/h3&gt;

&lt;p&gt;为了便于管理您的文稿，在 &lt;strong&gt;预览区&lt;/strong&gt; 的顶部放置了如下所示的 &lt;strong&gt;管理工具栏&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.zybuluo.com/static/img/toolbar-manager.jpg&quot; alt=&quot;tool-manager&quot; /&gt;&lt;/p&gt;

&lt;p&gt;通过管理工具栏可以：&lt;/p&gt;

&lt;p&gt;&lt;i class=&quot;icon-share&quot;&gt;&lt;/i&gt; 发布：将当前的文稿生成固定链接，在网络上发布，分享
&lt;i class=&quot;icon-file&quot;&gt;&lt;/i&gt; 新建：开始撰写一篇新的文稿
&lt;i class=&quot;icon-trash&quot;&gt;&lt;/i&gt; 删除：删除当前的文稿
&lt;i class=&quot;icon-cloud&quot;&gt;&lt;/i&gt; 导出：将当前的文稿转化为 Markdown 文本或者 Html 格式，并导出到本地
&lt;i class=&quot;icon-reorder&quot;&gt;&lt;/i&gt; 列表：所有新增和过往的文稿都可以在这里查看、操作
&lt;i class=&quot;icon-pencil&quot;&gt;&lt;/i&gt; 模式：切换 普通/Vim/Emacs 编辑模式&lt;/p&gt;

&lt;h3 id=&quot;7-阅读工具栏&quot;&gt;7. 阅读工具栏&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://www.zybuluo.com/static/img/toolbar-reader.jpg&quot; alt=&quot;tool-manager&quot; /&gt;&lt;/p&gt;

&lt;p&gt;通过 &lt;strong&gt;预览区&lt;/strong&gt; 右上角的 &lt;strong&gt;阅读工具栏&lt;/strong&gt;，可以查看当前文稿的目录并增强阅读体验。&lt;/p&gt;

&lt;p&gt;工具栏上的五个图标依次为：&lt;/p&gt;

&lt;p&gt;&lt;i class=&quot;icon-list&quot;&gt;&lt;/i&gt; 目录：快速导航当前文稿的目录结构以跳转到感兴趣的段落
&lt;i class=&quot;icon-chevron-sign-left&quot;&gt;&lt;/i&gt; 视图：互换左边编辑区和右边预览区的位置
&lt;i class=&quot;icon-adjust&quot;&gt;&lt;/i&gt; 主题：内置了黑白两种模式的主题，试试 &lt;strong&gt;黑色主题&lt;/strong&gt;，超炫！
&lt;i class=&quot;icon-desktop&quot;&gt;&lt;/i&gt; 阅读：心无旁骛的阅读模式提供超一流的阅读体验
&lt;i class=&quot;icon-fullscreen&quot;&gt;&lt;/i&gt; 全屏：简洁，简洁，再简洁，一个完全沉浸式的写作和阅读环境&lt;/p&gt;

&lt;h3 id=&quot;8-阅读模式&quot;&gt;8. 阅读模式&lt;/h3&gt;

&lt;p&gt;在 &lt;strong&gt;阅读工具栏&lt;/strong&gt; 点击 &lt;i class=&quot;icon-desktop&quot;&gt;&lt;/i&gt; 或者按下 &lt;code class=&quot;highlighter-rouge&quot;&gt;Ctrl+Alt+M&lt;/code&gt; 随即进入独立的阅读模式界面，我们在版面渲染上的每一个细节：字体，字号，行间距，前背景色都倾注了大量的时间，努力提升阅读的体验和品质。&lt;/p&gt;

&lt;h3 id=&quot;9-标签分类和搜索&quot;&gt;9. 标签、分类和搜索&lt;/h3&gt;

&lt;p&gt;在编辑区任意行首位置输入以下格式的文字可以标签当前文档：&lt;/p&gt;

&lt;p&gt;标签： 未分类&lt;/p&gt;

&lt;p&gt;标签以后的文稿在【文件列表】（Ctrl+Alt+F）里会按照标签分类，用户可以同时使用键盘或者鼠标浏览查看，或者在【文件列表】的搜索文本框内搜索标题关键字过滤文稿，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.zybuluo.com/static/img/file-list.png&quot; alt=&quot;file-list&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;10-文稿发布和分享&quot;&gt;10. 文稿发布和分享&lt;/h3&gt;

&lt;p&gt;在您使用 Cmd Markdown 记录，创作，整理，阅读文稿的同时，我们不仅希望它是一个有力的工具，更希望您的思想和知识通过这个平台，连同优质的阅读体验，将他们分享给有相同志趣的人，进而鼓励更多的人来到这里记录分享他们的思想和知识，尝试点击 &lt;i class=&quot;icon-share&quot;&gt;&lt;/i&gt; (Ctrl+Alt+P) 发布这份文档给好友吧！&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;再一次感谢您花费时间阅读这份欢迎稿，点击 &lt;i class=&quot;icon-file&quot;&gt;&lt;/i&gt; (Ctrl+Alt+N) 开始撰写新的文稿吧！祝您在这里记录、阅读、分享愉快！&lt;/p&gt;

&lt;p&gt;作者 &lt;a href=&quot;http://weibo.com/ghosert&quot;&gt;@ghosert&lt;/a&gt;   &lt;br /&gt;
2016 年 07月 07日&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:LaTeX&quot;&gt;
      &lt;p&gt;支持 &lt;strong&gt;LaTeX&lt;/strong&gt; 编辑显示支持，例如：$\sum_{i=1}^n a_i=0$， 访问 &lt;a href=&quot;http://meta.math.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference&quot;&gt;MathJax&lt;/a&gt; 参考更多使用方法。 &lt;a href=&quot;#fnref:LaTeX&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:code&quot;&gt;
      &lt;p&gt;代码高亮功能支持包括 Java, Python, JavaScript 在内的，&lt;strong&gt;四十一&lt;/strong&gt;种主流编程语言。 &lt;a href=&quot;#fnref:code&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Fri, 04 Jan 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/01/markdown/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/01/markdown/</guid>
        
        <category>拾遗</category>
        
        
        <category>拾遗</category>
        
      </item>
    
      <item>
        <title>blog push到远程仓库（github pages）出错</title>
        <description>&lt;!--more--&gt;

&lt;p&gt;本地仓库更新blog后，需要远程推送到github仓库。用命令&lt;code class=&quot;highlighter-rouge&quot;&gt;git push xxx(远程库名字，有的默认origin) master&lt;/code&gt;。但是出现错误。
我怀疑是因为远程仓库被访问后，有些修改，所以需要pull到本地，再push，还没验证。
但是也可以&lt;code class=&quot;highlighter-rouge&quot;&gt;git push -f xxx master&lt;/code&gt;强制覆盖，反正也是你自己一个人在维护blog，–force也不会有人想要砍死你😎&lt;/p&gt;
</description>
        <pubDate>Thu, 03 Jan 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/01/pushgithubpage/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/01/pushgithubpage/</guid>
        
        <category>github</category>
        
        
        <category>github</category>
        
      </item>
    
      <item>
        <title>制作github pages</title>
        <description>&lt;!--more--&gt;

&lt;p&gt;标签： github&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;之前在阿里云租借服务器搭建网站，需要昂贵的费用很麻烦。所以改在用github上搭建自己的个人博客，既方便又美观（免费）。&lt;/p&gt;
&lt;h3 id=&quot;1-环境&quot;&gt;1. 环境&lt;/h3&gt;

&lt;ul class=&quot;task-list&quot;&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Mac&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;linux、windows没试&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;github账号&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;git&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;ruby&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;jekyll&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-配置&quot;&gt;2. 配置&lt;/h3&gt;
&lt;p&gt;为了方便，直接上&lt;a href=&quot;http://baixin.io/2016/10/jekyll_tutorials1/&quot;&gt;大神教程&lt;/a&gt;。内容很详细，直接按着来就行了。我这里就只写一些遇到的问题。&lt;/p&gt;
&lt;h5 id=&quot;1-jekyll-server报错&quot;&gt;1) jekyll server报错：&lt;/h5&gt;
&lt;p&gt;我的原因是ruby版本太老，于是更新。安装rvm（ruby版本管理工具Ruby Version Manager）&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl -L get.rvm.io | bash -s stable
source ~/.bashrc  
source ~/.bash_profile   (如果你的终端个性化配置过，可能会出差错，不要怕，退出重开就好了)
rvm -v  查看版本
rvm list known 查看可用版本
rvm install 2.4.1（可以换你需要的版本）
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;如果缺少一些包，比如xxx，直接直接执行&lt;code class=&quot;highlighter-rouge&quot;&gt;gem install xxx&lt;/code&gt;就行了（我缺少 minima）&lt;/p&gt;
&lt;h5 id=&quot;2-gem-源的问题&quot;&gt;2) gem 源的问题：&lt;/h5&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;gem sources -a http://gems.ruby-china.com/&lt;/code&gt;
(淘宝源没了，http://gems.ruby-china.org也没了，这是最新的，2019.1.3实验可用)&lt;/p&gt;

&lt;h3 id=&quot;3-本地运行效果&quot;&gt;3. 本地运行效果&lt;/h3&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$ jekyll server &lt;/code&gt;就行了
浏览器输入&lt;a href=&quot;http://127.0.0.1:4000&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;http://127.0.0.1:4000&lt;/code&gt;&lt;/a&gt;查看效果。&lt;/p&gt;

&lt;h3 id=&quot;4-push到github仓库&quot;&gt;4. push到github仓库&lt;/h3&gt;
&lt;p&gt;建一个username.github.io的仓库，把本地的项目push上去。浏览器访问&lt;code class=&quot;highlighter-rouge&quot;&gt;www.username.github.io&lt;/code&gt;就可以了。注意username一定要是你的github账号名字！&lt;/p&gt;

&lt;h3 id=&quot;5-更新文章&quot;&gt;5. 更新文章&lt;/h3&gt;
&lt;p&gt;直接更新_post内的.md文件就行,然后push到远程库上。&lt;/p&gt;

&lt;h3 id=&quot;6-推荐一个编辑器&quot;&gt;6. 推荐一个编辑器&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://www.zybuluo.com/mdeditor#&quot;&gt;MarkDown&lt;/a&gt;非常好用,墙裂推荐！！！童叟无欺！！！&lt;/p&gt;

&lt;h3 id=&quot;7-用自己的域名解析到github-page上&quot;&gt;7. 用自己的域名解析到github page上&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;注册到阿里云注册一个域名&lt;/li&gt;
  &lt;li&gt;解析域名，解析方式可以选择A，ip的话在终端ping username.github.io,得到IP地址。&lt;/li&gt;
  &lt;li&gt;github仓库新建一个文件名为CNAME内容为注册的域名（不用www前缀）的文件&lt;/li&gt;
  &lt;li&gt;over
 如果想看图文并茂的的，推荐此&lt;a href=&quot;https://www.cnblogs.com/olddoublemoon/p/6629398.html&quot;&gt;博客&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Thu, 03 Jan 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/01/page/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/01/page/</guid>
        
        <category>github</category>
        
        
        <category>github</category>
        
      </item>
    
      <item>
        <title>yolo</title>
        <description>&lt;!--more--&gt;

&lt;p&gt;标签： CNN，深度学习，检测&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1506.02640&quot;&gt;You Only Look Once: Unified, Real-Time Object Detection&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;abstract&quot;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.&lt;/p&gt;

&lt;h4 id=&quot;摘要&quot;&gt;摘要&lt;/h4&gt;
&lt;p&gt;我们提出了YOLO，一种新的目标检测方法。以前的目标检测工作重新利用分类器来执行检测。相反，我们将目标检测框架看作回归问题从空间上分割边界框和相关的类别概率。单个神经网络在一次评估中直接从完整图像上预测边界框和类别概率。由于整个检测流水线是单一网络，因此可以直接对检测性能进行端到端的优化。&lt;/p&gt;

&lt;p&gt;Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.&lt;/p&gt;

&lt;p&gt;我们的统一架构非常快。我们的基础YOLO模型以45帧/秒的速度实时处理图像。网络的一个较小版本，快速YOLO，每秒能处理惊人的155帧，同时实现其它实时检测器两倍的mAP。与最先进的检测系统相比，YOLO产生了更多的定位误差，但不太可能在背景上的预测假阳性。最后，YOLO学习目标非常通用的表示。当从自然图像到艺术品等其它领域泛化时，它都优于其它检测方法，包括DPM和R-CNN。&lt;/p&gt;

&lt;h3 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h3&gt;
&lt;p&gt;Humans glance at an image and instantly know what objects are in the image, where they are, and how they interact. The human visual system is fast and accurate, allowing us to perform complex tasks like driving with little conscious thought. Fast, accurate algorithms for object detection would allow computers to drive cars without specialized sensors, enable assistive devices to convey real-time scene information to human users, and unlock the potential for general purpose, responsive robotic systems.&lt;/p&gt;

&lt;h4 id=&quot;1-引言&quot;&gt;1. 引言&lt;/h4&gt;
&lt;p&gt;人们瞥一眼图像，立即知道图像中的物体是什么，它们在哪里以及它们如何相互作用。人类的视觉系统是快速和准确的，使我们能够执行复杂的任务，如驾驶时没有多少有意识的想法。快速，准确的目标检测算法可以让计算机在没有专门传感器的情况下驾驶汽车，使辅助设备能够向人类用户传达实时的场景信息，并表现出对一般用途和响应机器人系统的潜力。&lt;/p&gt;

&lt;p&gt;Current detection systems repurpose classifiers to perform detection. To detect an object, these systems take a classifier for that object and evaluate it at various locations and scales in a test image. Systems like deformable parts models (DPM) use a sliding window approach where the classifier is run at evenly spaced locations over the entire image [10].&lt;/p&gt;

&lt;p&gt;目前的检测系统重用分类器来执行检测。为了检测目标，这些系统为该目标提供一个分类器，并在不同的位置对其进行评估，并在测试图像中进行缩放。像可变形部件模型（DPM）这样的系统使用滑动窗口方法，其分类器在整个图像的均匀间隔的位置上运行[10]。&lt;/p&gt;

&lt;p&gt;More recent approaches like R-CNN use region proposal methods to first generate potential bounding boxes in an image and then run a classifier on these proposed boxes. After classification, post-processing is used to refine the bounding boxes, eliminate duplicate detections, and rescore the boxes based on other objects in the scene [13]. These complex pipelines are slow and hard to optimize because each individual component must be trained separately.&lt;/p&gt;

&lt;p&gt;最近的方法，如R-CNN使用区域提出方法首先在图像中生成潜在的边界框，然后在这些提出的框上运行分类器。在分类之后，后处理用于细化边界框，消除重复的检测，并根据场景中的其它目标重新定位边界框[13]。这些复杂的流程很慢，很难优化，因为每个单独的组件都必须单独进行训练。&lt;/p&gt;

&lt;p&gt;We reframe object detection as a single regression problem, straight from image pixels to bounding box coordinates and class probabilities. Using our system, you only look once (YOLO) at an image to predict what objects are present and where they are.&lt;/p&gt;

&lt;p&gt;我们将目标检测重新看作单一的回归问题，直接从图像像素到边界框坐标和类概率。使用我们的系统，您只需要在图像上看一次（YOLO），以预测出现的目标和位置。&lt;/p&gt;

&lt;p&gt;YOLO is refreshingly simple: see Figure 1. A single convolutional network simultaneously predicts multiple bounding boxes and class probabilities for those boxes. YOLO trains on full images and directly optimizes detection performance. This unified model has several benefits over traditional methods of object detection.&lt;/p&gt;

&lt;p&gt;YOLO很简单：参见图1。单个卷积网络同时预测这些盒子的多个边界框和类概率。YOLO在全图像上训练并直接优化检测性能。这种统一的模型比传统的目标检测方法有一些好处。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/yolo/1.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 1: The YOLO Detection System. Processing images with YOLO is simple and straightforward. Our system (1) resizes the input image to 448 × 448, (2) runs a single convolutional network on the image, and (3) thresholds the resulting detections by the model’s confidence.&lt;/p&gt;

&lt;p&gt;图1：YOLO检测系统。用YOLO处理图像简单直接。我们的系统（1）将输入图像调整为448×448，（2）在图像上运行单个卷积网络，以及（3）由模型的置信度对所得到的检测进行阈值处理。&lt;/p&gt;

&lt;p&gt;First, YOLO is extremely fast. Since we frame detection as a regression problem we don’t need a complex pipeline. We simply run our neural network on a new image at test time to predict detections. Our base network runs at 45 frames per second with no batch processing on a Titan X GPU and a fast version runs at more than 150 fps. This means we can process streaming video in real-time with less than 25 milliseconds of latency. Furthermore, YOLO achieves more than twice the mean average precision of other real-time systems. For a demo of our system running in real-time on a webcam please see our project webpage: &lt;a href=&quot;http://pjreddie.com/yolo/&quot;&gt;http://pjreddie.com/yolo/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;首先，YOLO速度非常快。由于我们将检测视为回归问题，所以我们不需要复杂的流程。测试时我们在一张新图像上简单的运行我们的神经网络来预测检测。我们的基础网络以每秒45帧的速度运行，在Titan X GPU上没有批处理，快速版本运行速度超过150fps。这意味着我们可以在不到25毫秒的延迟内实时处理流媒体视频。此外，YOLO实现了其它实时系统两倍以上的平均精度。关于我们的系统在网络摄像头上实时运行的演示，请参阅我们的项目网页：&lt;a href=&quot;http://pjreddie.com/yolo/&quot;&gt;http://pjreddie.com/yolo/&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;Second, YOLO reasons globally about the image when making predictions. Unlike sliding window and region proposal-based techniques, YOLO sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance. Fast R-CNN, a top detection method [14], mistakes background patches in an image for objects because it can’t see the larger context. YOLO makes less than half the number of background errors compared to Fast R-CNN.&lt;/p&gt;

&lt;p&gt;其次，YOLO在进行预测时，会对图像进行全面地推理。与基于滑动窗口和区域提出的技术不同，YOLO在训练期间和测试时会看到整个图像，所以它隐式地编码了关于类的上下文信息以及它们的外观。fast R-CNN是一种顶级的检测方法[14]，因为它看不到更大的上下文，所以在图像中会将背景块误检为目标。与快速R-CNN相比，YOLO的背景误检数量少了一半。&lt;/p&gt;

&lt;p&gt;Third, YOLO learns generalizable representations of objects. When trained on natural images and tested on artwork, YOLO outperforms top detection methods like DPM and R-CNN by a wide margin. Since YOLO is highly generalizable it is less likely to break down when applied to new domains or unexpected inputs.&lt;/p&gt;

&lt;p&gt;第三，YOLO学习目标的泛化表示。当在自然图像上进行训练并对艺术作品进行测试时，YOLO大幅优于DPM和R-CNN等顶级检测方法。由于YOLO具有高度泛化能力，因此在应用于新领域或碰到意外的输入时不太可能出故障。&lt;/p&gt;

&lt;p&gt;YOLO still lags behind state-of-the-art detection systems in accuracy. While it can quickly identify objects in images it struggles to precisely localize some objects, especially small ones. We examine these tradeoffs further in our experiments.&lt;/p&gt;

&lt;p&gt;YOLO在精度上仍然落后于最先进的检测系统。虽然它可以快速识别图像中的目标，但它仍在努力精确定位一些目标，尤其是小的目标。我们在实验中会进一步检查这些权衡。&lt;/p&gt;

&lt;p&gt;All of our training and testing code is open source. A variety of pretrained models are also available to download.&lt;/p&gt;

&lt;p&gt;我们所有的训练和测试代码都是开源的。各种预训练模型也都可以下载。&lt;/p&gt;

&lt;h3 id=&quot;2-unified-detection&quot;&gt;2. Unified Detection&lt;/h3&gt;
&lt;p&gt;We unify the separate components of object detection into a single neural network. Our network uses features from the entire image to predict each bounding box. It also predicts all bounding boxes across all classes for an image simultaneously. This means our network reasons globally about the full image and all the objects in the image. The YOLO design enables end-to-end training and real-time speeds while maintaining high average precision.&lt;/p&gt;

&lt;h4 id=&quot;2-统一检测&quot;&gt;2. 统一检测&lt;/h4&gt;
&lt;p&gt;我们将目标检测的单独组件集成到单个神经网络中。我们的网络使用整个图像的特征来预测每个边界框。它还可以同时预测一张图像中的所有类别的所有边界框。这意味着我们的网络全面地推理整张图像和图像中的所有目标。YOLO设计可实现端到端训练和实时的速度，同时保持较高的平均精度。&lt;/p&gt;

&lt;p&gt;Our system divides the input image into an S×S grid. If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object.&lt;/p&gt;

&lt;p&gt;我们的系统将输入图像分成S×S的网格。如果一个目标的中心落入一个网格单元中，该网格单元负责检测该目标。&lt;/p&gt;

&lt;p&gt;Each grid cell predicts B bounding boxes and confidence scores for those boxes. These confidence scores reflect how confident the model is that the box contains an object and also how accurate it thinks the box is that it predicts. Formally we define confidence as $\Pr(\textrm{Object}) * \textrm{IOU}_{\textrm{pred}}^{\textrm{truth}}$ . If no object exists in that cell, the confidence scores should be zero. Otherwise we want the confidence score to equal the intersection over union (IOU) between the predicted box and the ground truth.&lt;/p&gt;

&lt;p&gt;每个网格单元预测这些盒子的B个边界框和置信度分数。这些置信度分数反映了该模型对盒子是否包含目标的信心，以及它预测盒子的准确程度。在形式上，我们将置信度定义为$\Pr(\textrm{Object}) * \textrm{IOU}_{\textrm{pred}}^{\textrm{truth}}$。如果该单元格中不存在目标，则置信度分数应为零。否则，我们希望置信度分数等于预测框与真实值之间联合部分的交集（IOU）。&lt;/p&gt;

&lt;p&gt;Each bounding box consists of 5 predictions: $x$, $y$, $w$, $h$, and confidence. The $(x,y)$ coordinates represent the center of the box relative to the bounds of the grid cell. The width and height are predicted relative to the whole image. Finally the confidence prediction represents the IOU between the predicted box and any ground truth box.&lt;/p&gt;

&lt;p&gt;每个边界框包含5个预测：$x$，$y$，$w$，$h$和置信度。$(x，y)$坐标表示边界框相对于网格单元边界框的中心。宽度和高度是相对于整张图像预测的。最后，置信度预测表示预测框与实际边界框之间的IOU。&lt;/p&gt;

&lt;p&gt;Each grid cell also predicts $C$ conditional class probabilities,
$Pr(Class_i|Object)$. These probabilities are conditioned on the grid cell containing an object. We only predict one set of class probabilities per grid cell, regardless of the number of boxes $B$.&lt;/p&gt;

&lt;p&gt;每个网格单元还预测$C$个条件类别概率
$Pr(Class_i|Object)$。这些概率以包含目标的网格单元为条件。每个网格单元我们只预测的一组类别概率，而不管边界框的的数量$B$是多少。&lt;/p&gt;

&lt;p&gt;At test time we multiply the conditional class probabilities and the individual box confidence predictions,
&lt;script type=&quot;math/tex&quot;&gt;\Pr(\textrm{Class}_i | \textrm{Object}) * \Pr(\textrm{Object}) * \textrm{IOU}_{\textrm{pred}}^{\textrm{truth}} = \Pr(\textrm{Class}_i)*\textrm{IOU}_{\textrm{pred}}^{\textrm{truth}}&lt;/script&gt;which gives us class-specific confidence scores for each box. These scores encode both the probability of that class appearing in the box and how well the predicted box fits the object.&lt;/p&gt;

&lt;p&gt;在测试时，我们乘以条件类概率和单个盒子的置信度预测，
&lt;script type=&quot;math/tex&quot;&gt;\Pr(\textrm{Class}_i | \textrm{Object}) * \Pr(\textrm{Object}) * \textrm{IOU}_{\textrm{pred}}^{\textrm{truth}} = \Pr(\textrm{Class}_i)*\textrm{IOU}_{\textrm{pred}}^{\textrm{truth}}&lt;/script&gt;它为我们提供了每个框特定类别的置信度分数。这些分数编码了该类出现在框中的概率以及预测框拟合目标的程度。&lt;/p&gt;

&lt;p&gt;For evaluating YOLO on Pascal VOC, we use $S=7$, $B=2$. Pascal VOC has 20 labelled classes so $C=20$. Our final prediction is a $7×7×30$ tensor.&lt;/p&gt;

&lt;p&gt;为了在Pascal VOC上评估YOLO，我们使用S=7，B=2。Pascal VOC有20个标注类，所以C=20。我们最终的预测是7×7×30的张量。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/yolo/2.png&quot; alt=&quot;2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Model&lt;/strong&gt;. Our system models detection as a regression problem. It divides the image into an $S×S$ grid and for each grid cell predicts $B$ bounding boxes, confidence for those boxes, and $C$ class probabilities. These predictions are encoded as an $S×S×(B∗5+C)$ tensor.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;模型。&lt;/strong&gt; 我们的系统将检测建模为回归问题。它将图像分成$S×S$的网格，并且每个网格单元预测$B$个边界框，这些边界框的置信度以及$C$个类别概率。这些预测被编码为$S×S×(B∗5+C)$的张量。&lt;/p&gt;

&lt;h4 id=&quot;21-network-design&quot;&gt;2.1. Network Design&lt;/h4&gt;

&lt;p&gt;We implement this model as a convolutional neural network and evaluate it on the Pascal VOC detection dataset [9]. The initial convolutional layers of the network extract features from the image while the fully connected layers predict the output probabilities and coordinates.&lt;/p&gt;

&lt;h4 id=&quot;21-网络设计&quot;&gt;2.1. 网络设计&lt;/h4&gt;
&lt;p&gt;我们将此模型作为卷积神经网络来实现，并在Pascal VOC检测数据集[9]上进行评估。网络的初始卷积层从图像中提取特征，而全连接层预测输出概率和坐标。&lt;/p&gt;

&lt;p&gt;Our network architecture is inspired by the GoogLeNet model for image classification [34]. Our network has 24 convolutional layers followed by 2 fully connected layers. Instead of the inception modules used by GoogLeNet, we simply use 1×1 reduction layers followed by 3×3 convolutional layers, similar to Lin et al [22]. The full network is shown in Figure 3.&lt;/p&gt;

&lt;p&gt;我们的网络架构受到GoogLeNet图像分类模型的启发[34]。我们的网络有24个卷积层，后面是2个全连接层。我们只使用1×1降维层，后面是3×3卷积层，这与Lin等人[22]类似，而不是GoogLeNet使用的Inception模块。完整的网络如图3所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/yolo/3.png&quot; alt=&quot;3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 3: The Architecture.&lt;/strong&gt; Our detection network has 24 convolutional layers followed by 2 fully connected layers. Alternating 1×1 convolutional layers reduce the features space from preceding layers. We pretrain the convolutional layers on the ImageNet classification task at half the resolution (224×224 input image) and then double the resolution for detection.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;图3：架构。&lt;/strong&gt;我们的检测网络有24个卷积层，其次是2个全连接层。交替1×1卷积层减少了前面层的特征空间。我们在ImageNet分类任务上以一半的分辨率（224×224的输入图像）预训练卷积层，然后将分辨率加倍来进行检测。&lt;/p&gt;

&lt;p&gt;We also train a fast version of YOLO designed to push the boundaries of fast object detection. Fast YOLO uses a neural network with fewer convolutional layers (9 instead of 24) and fewer filters in those layers. Other than the size of the network, all training and testing parameters are the same between YOLO and Fast YOLO.&lt;/p&gt;

&lt;p&gt;我们还训练了快速版本的YOLO，旨在推动快速目标检测的界限。快速YOLO使用具有较少卷积层（9层而不是24层）的神经网络，在这些层中使用较少的滤波器。除了网络规模之外，YOLO和快速YOLO的所有训练和测试参数都是相同的。&lt;/p&gt;

&lt;p&gt;The final output of our network is the 7×7×30 tensor of predictions.&lt;/p&gt;

&lt;p&gt;我们网络的最终输出是7×7×30的预测张量。&lt;/p&gt;

&lt;h4 id=&quot;22-training&quot;&gt;2.2. Training&lt;/h4&gt;
&lt;p&gt;We pretrain our convolutional layers on the ImageNet 1000-class competition dataset [30]. For pretraining we use the first 20 convolutional layers from Figure 3 followed by a average-pooling layer and a fully connected layer. We train this network for approximately a week and achieve a single crop &lt;code class=&quot;highlighter-rouge&quot;&gt;top-5&lt;/code&gt; accuracy of 88% on the ImageNet 2012 validation set, comparable to the GoogLeNet models in Caffe’s Model Zoo [24]. We use the Darknet framework for all training and inference [26].&lt;/p&gt;

&lt;h4 id=&quot;22-训练&quot;&gt;2.2. 训练&lt;/h4&gt;
&lt;p&gt;我们在ImageNet 1000类竞赛数据集[30]上预训练我们的卷积图层。对于预训练，我们使用图3中的前20个卷积层，接着是平均池化层和全连接层。我们对这个网络进行了大约一周的训练，并且在ImageNet 2012验证集上获得了单一裁剪图像88%的&lt;code class=&quot;highlighter-rouge&quot;&gt;top-5&lt;/code&gt;准确率，与Caffe模型池中的GoogLeNet模型相当。我们使用Darknet框架进行所有的训练和推断[26]。&lt;/p&gt;

&lt;p&gt;We then convert the model to perform detection. Ren et al. show that adding both convolutional and connected layers to pretrained networks can improve performance [29]. Following their example, we add four convolutional layers and two fully connected layers with randomly initialized weights. Detection often requires fine-grained visual information so we increase the input resolution of the network from 224×224 to 448×448.&lt;/p&gt;

&lt;p&gt;然后我们转换模型来执行检测。Ren等人表明，预训练网络中增加卷积层和连接层可以提高性能[29]。按照他们的例子，我们添加了四个卷积层和两个全连接层，并且具有随机初始化的权重。检测通常需要细粒度的视觉信息，因此我们将网络的输入分辨率从224×224变为448×448。&lt;/p&gt;

&lt;p&gt;Our final layer predicts both class probabilities and bounding box coordinates. We normalize the bounding box width and height by the image width and height so that they fall between 0 and 1. We parametrize the bounding box x and y coordinates to be offsets of a particular grid cell location so they are also bounded between 0 and 1.&lt;/p&gt;

&lt;p&gt;我们的最后一层预测类概率和边界框坐标。我们通过图像宽度和高度来规范边界框的宽度和高度，使它们落在0和1之间。我们将边界框x和y坐标参数化为特定网格单元位置的偏移量，所以它们边界也在0和1之间。&lt;/p&gt;

&lt;p&gt;We use a linear activation function for the final layer and all other layers use the following leaky rectified linear activation:&lt;script type=&quot;math/tex&quot;&gt;\phi(x) = \begin{cases}     x, if x &gt; 0 \\     0.1x, otherwise \end{cases}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;我们对最后一层使用线性激活函数，所有其它层使用下面的漏泄修正线性激活：&lt;script type=&quot;math/tex&quot;&gt;\phi(x) = \begin{cases}     x, if x &gt; 0 \\     0.1x, otherwise \end{cases}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We optimize for sum-squared error in the output of our model. We use sum-squared error because it is easy to optimize, however it does not perfectly align with our goal of maximizing average precision. It weights localization error equally with classification error which may not be ideal. Also, in every image many grid cells do not contain any object. This pushes the “confidence” scores of those cells towards zero, often overpowering the gradient from cells that do contain objects. This can lead to model instability, causing training to diverge early on.&lt;/p&gt;

&lt;p&gt;我们优化了模型输出中的平方和误差。我们使用平方和误差，因为它很容易进行优化，但是它并不完全符合我们最大化平均精度的目标。分类误差与定位误差的权重是一样的，这可能并不理想。另外，在每张图像中，许多网格单元不包含任何对象。这将这些单元格的“置信度”分数推向零，通常压倒了包含目标的单元格的梯度。这可能导致模型不稳定，从而导致训练早期发散。&lt;/p&gt;

&lt;p&gt;To remedy this, we increase the loss from bounding box coordinate predictions and decrease the loss from confidence predictions for boxes that don’t contain objects. We use two parameters, $\lambda_\textrm{coord}$ and $\lambda_\textrm{noobj}$ to accomplish this. We set $\lambda_\textrm{coord}=5$ and $\lambda_\textrm{noobj}=.5$.&lt;/p&gt;

&lt;p&gt;为了改善这一点，我们增加了边界框坐标预测损失，并减少了不包含目标边界框的置信度预测损失。我们使用两个参数$\lambda_\textrm{coord}$和$\lambda_\textrm{noobj}$来完成这个工作。我们设置$\lambda_\textrm{coord}=5$和$\lambda_\textrm{noobj}=.5$。&lt;/p&gt;

&lt;p&gt;Sum-squared error also equally weights errors in large boxes and small boxes. Our error metric should reflect that small deviations in large boxes matter less than in small boxes. To partially address this we predict the square root of the bounding box width and height instead of the width and height directly.&lt;/p&gt;

&lt;p&gt;平方和误差也可以在大盒子和小盒子中同样加权误差。我们的错误指标应该反映出，大盒子小偏差的重要性不如小盒子小偏差的重要性。为了部分解决这个问题，我们直接预测边界框宽度和高度的平方根，而不是宽度和高度。&lt;/p&gt;

&lt;p&gt;YOLO predicts multiple bounding boxes per grid cell. At training time we only want one bounding box predictor to be responsible for each object. We assign one predictor to be “responsible” for predicting an object based on which prediction has the highest current IOU with the ground truth. This leads to specialization between the bounding box predictors. Each predictor gets better at predicting certain sizes, aspect ratios, or classes of object, improving overall recall.&lt;/p&gt;

&lt;p&gt;YOLO每个网格单元预测多个边界框。在训练时，每个目标我们只需要一个边界框预测器来负责。我们指定一个预测器“负责”根据哪个预测与真实值之间具有当前最高的IOU来预测目标。这导致边界框预测器之间的专业化。每个预测器可以更好地预测特定大小，方向角，或目标的类别，从而改善整体召回率。&lt;/p&gt;

&lt;p&gt;During training we optimize the following, multi-part loss function:
&lt;script type=&quot;math/tex&quot;&gt;\begin{multline} \lambda_\textbf{coord} \sum_{i = 0}^{S^2}     \sum_{j = 0}^{B}     \mathbb{𝟙}_{ij}^{\text{obj}}             \left[             \left(                 x_i - \hat{x}_i             \right)^2 +             \left(                 y_i - \hat{y}_i             \right)^2             \right] \\ + \lambda_\textbf{coord} \sum_{i = 0}^{S^2}     \sum_{j = 0}^{B}         \mathbb{𝟙}_{ij}^{\text{obj}}          \left[         \left(             \sqrt{w_i} - \sqrt{\hat{w}_i}         \right)^2 +         \left(             \sqrt{h_i} - \sqrt{\hat{h}_i}         \right)^2         \right] \\ + \sum_{i = 0}^{S^2}     \sum_{j = 0}^{B}         \mathbb{𝟙}_{ij}^{\text{obj}}         \left(             C_i - \hat{C}_i         \right)^2 \\ + \lambda_\textrm{noobj} \sum_{i = 0}^{S^2}     \sum_{j = 0}^{B}     \mathbb{𝟙}_{ij}^{\text{noobj}}         \left(             C_i - \hat{C}_i         \right)^2 \\ + \sum_{i = 0}^{S^2} \mathbb{𝟙}_i^{\text{obj}}     \sum_{c \in \textrm{classes}}         \left(             p_i(c) - \hat{p}_i(c)         \right)^2 \end{multline}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;where $\mathbb{𝟙}_i^{\text{obj}}$ denotes if object appears in cell $i$ and &lt;script type=&quot;math/tex&quot;&gt;\mathbb{𝟙}_{ij}^{\text{obj}}&lt;/script&gt; denotes that the $j$th bounding box predictor in cell $i$ is “responsible” for that prediction.&lt;/p&gt;

&lt;p&gt;在训练期间，我们优化以下多部分损失函数：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{multline} \lambda_\textbf{coord} \sum_{i = 0}^{S^2}     \sum_{j = 0}^{B}     \mathbb{𝟙}_{ij}^{\text{obj}}             \left[             \left(                 x_i - \hat{x}_i             \right)^2 +             \left(                 y_i - \hat{y}_i             \right)^2             \right] \\ + \lambda_\textbf{coord} \sum_{i = 0}^{S^2}     \sum_{j = 0}^{B}         \mathbb{𝟙}_{ij}^{\text{obj}}          \left[         \left(             \sqrt{w_i} - \sqrt{\hat{w}_i}         \right)^2 +         \left(             \sqrt{h_i} - \sqrt{\hat{h}_i}         \right)^2         \right] \\ + \sum_{i = 0}^{S^2}     \sum_{j = 0}^{B}         \mathbb{𝟙}_{ij}^{\text{obj}}         \left(             C_i - \hat{C}_i         \right)^2 \\ + \lambda_\textrm{noobj} \sum_{i = 0}^{S^2}     \sum_{j = 0}^{B}     \mathbb{𝟙}_{ij}^{\text{noobj}}         \left(             C_i - \hat{C}_i         \right)^2 \\ + \sum_{i = 0}^{S^2} \mathbb{𝟙}_i^{\text{obj}}     \sum_{c \in \textrm{classes}}         \left(             p_i(c) - \hat{p}_i(c)         \right)^2 \end{multline}&lt;/script&gt;

&lt;p&gt;其中$\mathbb{𝟙}_i^{\text{obj}}$表示目标是否出现在网格单元$i$中，&lt;script type=&quot;math/tex&quot;&gt;\mathbb{𝟙}_{ij}^{\text{obj}}&lt;/script&gt;表示网格单元$i$中的第$j$个边界框预测器“负责”该预测。&lt;/p&gt;

&lt;p&gt;Note that the loss function only penalizes classification error if an object is present in that grid cell (hence the conditional class probability discussed earlier). It also only penalizes bounding box coordinate error if that predictor is “responsible” for the ground truth box (i.e. has the highest IOU of any predictor in that grid cell).&lt;/p&gt;

&lt;p&gt;注意，如果目标存在于该网格单元中（前面讨论的条件类别概率），则损失函数仅惩罚分类错误。如果预测器“负责”实际边界框（即该网格单元中具有最高IOU的预测器），则它也仅惩罚边界框坐标错误。&lt;/p&gt;

&lt;p&gt;We train the network for about 135 epochs on the training and validation data sets from Pascal VOC 2007 and 2012. When testing on 2012 we also include the VOC 2007 test data for training. Throughout training we use a batch size of 64, a momentum of 0.9 and a decay of 0.0005.&lt;/p&gt;

&lt;p&gt;我们对Pascal VOC2007和2012的训练和验证数据集进行了大约135个迭代周期的网络训练。在Pascal VOC 2012上进行测试时，我们的训练包含了Pascal VOC 2007的测试数据。在整个训练过程中，我们使用了64的批大小，0.9的动量和0.0005的衰减。&lt;/p&gt;

&lt;p&gt;Our learning rate schedule is as follows: For the first epochs we slowly raise the learning rate from $10^{−3} $to $10^{−2}$. If we start at a high learning rate our model often diverges due to unstable gradients. We continue training with $10^{−2} $ for 75 epochs, then $10^{−3} $ for 30 epochs, and finally $10^{−4} $ for 30 epochs.&lt;/p&gt;

&lt;p&gt;我们的学习率方案如下：对于第一个迭代周期，我们慢慢地将学习率从$10^{−3} $提高到$10^{−2}$。如果我们从高学习率开始，我们的模型往往会由于不稳定的梯度而发散。我们继续以$10^{−2}$的学习率训练75个迭代周期，然后用$10^{−3}$的学习率训练30个迭代周期，最后用$10^{−4}$的学习率训练30个迭代周期。&lt;/p&gt;

&lt;p&gt;To avoid overfitting we use dropout and extensive data augmentation. A dropout layer with rate =.5 after the first connected layer prevents co-adaptation between layers [18]. For data augmentation we introduce random scaling and translations of up to 20% of the original image size. We also randomly adjust the exposure and saturation of the image by up to a factor of 1.5 in the HSV color space.&lt;/p&gt;

&lt;p&gt;为了避免过度拟合，我们使用丢弃和大量的数据增强。在第一个连接层之后，丢弃层使用=.5的比例，防止层之间的互相适应[18]。对于数据增强，我们引入高达原始图像20%大小的随机缩放和转换。我们还在HSV色彩空间中使用高达1.5的因子来随机调整图像的曝光和饱和度。&lt;/p&gt;

&lt;h4 id=&quot;23-inference&quot;&gt;2.3. Inference&lt;/h4&gt;
&lt;p&gt;Just like in training, predicting detections for a test image only requires one network evaluation. On Pascal VOC the network predicts 98 bounding boxes per image and class probabilities for each box. YOLO is extremely fast at test time since it only requires a single network evaluation, unlike classifier-based methods.&lt;/p&gt;

&lt;h4 id=&quot;23-推断&quot;&gt;2.3. 推断&lt;/h4&gt;
&lt;p&gt;就像在训练中一样，预测测试图像的检测只需要一次网络评估。在Pascal VOC上，每张图像上网络预测98个边界框和每个框的类别概率。YOLO在测试时非常快，因为它只需要一次网络评估，不像基于分类器的方法。&lt;/p&gt;

&lt;p&gt;The grid design enforces spatial diversity in the bounding box predictions. Often it is clear which grid cell an object falls in to and the network only predicts one box for each object. However, some large objects or objects near the border of multiple cells can be well localized by multiple cells. Non-maximal suppression can be used to fix these multiple detections. While not critical to performance as it is for R-CNN or DPM, non-maximal suppression adds 2−3% in mAP.&lt;/p&gt;

&lt;p&gt;网格设计强化了边界框预测中的空间多样性。通常很明显一个目标落在哪一个网格单元中，而网络只能为每个目标预测一个边界框。然而，一些大的目标或靠近多个网格单元边界的目标可以被多个网格单元很好地定位。非极大值抑制可以用来修正这些多重检测。对于R-CNN或DPM而言，性能不是关键的，非最大抑制会增加2−3%的mAP。&lt;/p&gt;

&lt;h4 id=&quot;24-limitations-of-yolo&quot;&gt;2.4. Limitations of YOLO&lt;/h4&gt;

&lt;p&gt;YOLO imposes strong spatial constraints on bounding box predictions since each grid cell only predicts two boxes and can only have one class. This spatial constraint limits the number of nearby objects that our model can predict. Our model struggles with small objects that appear in groups, such as flocks of birds.&lt;/p&gt;

&lt;h4 id=&quot;24-yolo的限制&quot;&gt;2.4. YOLO的限制&lt;/h4&gt;
&lt;p&gt;YOLO对边界框预测强加空间约束，因为每个网格单元只预测两个盒子，只能有一个类别。这个空间约束限制了我们的模型可以预测的邻近目标的数量。我们的模型与群组中出现的小物体（比如鸟群）进行斗争。&lt;/p&gt;

&lt;p&gt;Since our model learns to predict bounding boxes from data, it struggles to generalize to objects in new or unusual aspect ratios or configurations. Our model also uses relatively coarse features for predicting bounding boxes since our architecture has multiple downsampling layers from the input image.&lt;/p&gt;

&lt;p&gt;由于我们的模型学习从数据中预测边界框，因此它很难泛化到新的、不常见的方向比或配置的目标。我们的模型也使用相对较粗糙的特征来预测边界框，因为我们的架构具有来自输入图像的多个下采样层。&lt;/p&gt;

&lt;p&gt;Finally, while we train on a loss function that approximates detection performance, our loss function treats errors the same in small bounding boxes versus large bounding boxes. A small error in a large box is generally benign but a small error in a small box has a much greater effect on IOU. Our main source of error is incorrect localizations.&lt;/p&gt;

&lt;p&gt;最后，当我们训练一个近似检测性能的损失函数时，我们的损失函数会同样的对待小边界框与大边界框的误差。大边界框的小误差通常是良性的，但小边界框的小误差对IOU的影响要大得多。我们的主要错误来源是不正确的定位。&lt;/p&gt;

&lt;h3 id=&quot;3-comparison-to-other-detection-systems&quot;&gt;3. Comparison to Other Detection Systems&lt;/h3&gt;
&lt;p&gt;Object detection is a core problem in computer vision. Detection pipelines generally start by extracting a set of robust features from input images (Haar [25], SIFT [23], HOG [4], convolutional features [6]). Then, classifiers [36, 21, 13, 10] or localizers [1, 32] are used to identify objects in the feature space. These classifiers or localizers are run either in sliding window fashion over the whole image or on some subset of regions in the image [35, 15, 39]. We compare the YOLO detection system to several top detection frameworks, highlighting key similarities and differences.&lt;/p&gt;

&lt;h4 id=&quot;3-与其它检测系统的比较&quot;&gt;3. 与其它检测系统的比较&lt;/h4&gt;
&lt;p&gt;目标检测是计算机视觉中的核心问题。检测流程通常从输入图像上（Haar [25]，SIFT [23]，HOG [4]，卷积特征[6]）提取一组鲁棒特征开始。然后，分类器[36,21,13,10]或定位器[1,32]被用来识别特征空间中的目标。这些分类器或定位器在整个图像上或在图像中的一些子区域上以滑动窗口的方式运行[35,15,39]。我们将YOLO检测系统与几种顶级检测框架进行比较，突出了关键的相似性和差异性。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deformable parts models.&lt;/strong&gt; Deformable parts models (DPM) use a sliding window approach to object detection [10]. DPM uses a disjoint pipeline to extract static features, classify regions, predict bounding boxes for high scoring regions, etc. Our system replaces all of these disparate parts with a single convolutional neural network. The network performs feature extraction, bounding box prediction, non-maximal suppression, and contextual reasoning all concurrently. Instead of static features, the network trains the features in-line and optimizes them for the detection task. Our unified architecture leads to a faster, more accurate model than DPM.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;可变形部件模型。&lt;/strong&gt;可变形零件模型（DPM）使用滑动窗口方法进行目标检测[10]。DPM使用不相交的流程来提取静态特征，对区域进行分类，预测高评分区域的边界框等。我们的系统用单个卷积神经网络替换所有这些不同的部分。网络同时进行特征提取，边界框预测，非极大值抑制和上下文推理。网络内嵌训练特征而不是静态特征，并为检测任务优化它们。我们的统一架构导致了比DPM更快，更准确的模型。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;R-CNN.&lt;/strong&gt; R-CNN and its variants use region proposals instead of sliding windows to find objects in images. Selective Search [35] generates potential bounding boxes, a convolutional network extracts features, an SVM scores the boxes, a linear model adjusts the bounding boxes, and non-max suppression eliminates duplicate detections. Each stage of this complex pipeline must be precisely tuned independently and the resulting system is very slow, taking more than 40 seconds per image at test time [14].&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;R-CNN.&lt;/strong&gt; R-CNN及其变种使用区域提出而不是滑动窗口来查找图像中的目标。选择性搜索[35]产生潜在的边界框，卷积网络提取特征，SVM对边界框进行评分，线性模型调整边界框，非极大值抑制消除重复检测。这个复杂流程的每个阶段都必须独立地进行精确调整，所得到的系统非常慢，测试时每张图像需要超过40秒[14]。&lt;/p&gt;

&lt;p&gt;YOLO shares some similarities with R-CNN. Each grid cell proposes potential bounding boxes and scores those boxes using convolutional features. However, our system puts spatial constraints on the grid cell proposals which helps mitigate multiple detections of the same object. Our system also proposes far fewer bounding boxes, only 98 per image compared to about 2000 from Selective Search. Finally, our system combines these individual components into a single, jointly optimized model.&lt;/p&gt;

&lt;p&gt;YOLO与R-CNN有一些相似之处。每个网格单元提出潜在的边界框并使用卷积特征对这些框进行评分。但是，我们的系统对网格单元提出进行了空间限制，这有助于缓解对同一目标的多次检测。我们的系统还提出了更少的边界框，每张图像只有98个，而选择性搜索则只有2000个左右。最后，我们的系统将这些单独的组件组合成一个单一的，共同优化的模型。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Other Fast Detectors.&lt;/strong&gt; Fast and Faster R-CNN focus on speeding up the R-CNN framework by sharing computation and using neural networks to propose regions instead of Selective Search [14] [28]. While they offer speed and accuracy improvements over R-CNN, both still fall short of real-time performance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;其它快速检测器。&lt;/strong&gt;快速和更快的R-CNN通过共享计算和使用神经网络替代选择性搜索来提出区域加速R-CNN框架[14]，[28]。虽然它们提供了比R-CNN更快的速度和更高的准确度，但两者仍然不能达到实时性能。&lt;/p&gt;

&lt;p&gt;Many research efforts focus on speeding up the DPM pipeline [31] [38] [5]. They speed up HOG computation, use cascades, and push computation to GPUs. However, only 30Hz DPM [31] actually runs in real-time.&lt;/p&gt;

&lt;p&gt;许多研究工作集中在加快DPM流程上[31] [38] [5]。它们加速HOG计算，使用级联，并将计算推动到GPU上。但是，实际上只有30Hz的DPM [31]可以实时运行。&lt;/p&gt;

&lt;p&gt;Instead of trying to optimize individual components of a large detection pipeline, YOLO throws out the pipeline entirely and is fast by design.&lt;/p&gt;

&lt;p&gt;YOLO不是试图优化大型检测流程的单个组件，而是完全抛弃流程，被设计为快速检测。&lt;/p&gt;

&lt;p&gt;Detectors for single classes like faces or people can be highly optimized since they have to deal with much less variation [37]. YOLO is a general purpose detector that learns to detect a variety of objects simultaneously.&lt;/p&gt;

&lt;p&gt;像人脸或行人等单类别的检测器可以高度优化，因为他们必须处理更少的变化[37]。YOLO是一种通用的检测器，可以学习同时检测多个目标。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deep MultiBox.&lt;/strong&gt; Unlike R-CNN, Szegedy et al. train a convolutional neural network to predict regions of interest [8] instead of using Selective Search. MultiBox can also perform single object detection by replacing the confidence prediction with a single class prediction. However, MultiBox cannot perform general object detection and is still just a piece in a larger detection pipeline, requiring further image patch classification. Both YOLO and MultiBox use a convolutional network to predict bounding boxes in an image but YOLO is a complete detection system.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deep MultiBox。&lt;/strong&gt;与R-CNN不同，Szegedy等人训练了一个卷积神经网络来预测感兴趣区域[8]，而不是使用选择性搜索。MultiBox还可以通过用单类预测替换置信度预测来执行单目标检测。然而，MultiBox无法执行通用的目标检测，并且仍然只是一个较大的检测流程中的一部分，需要进一步的图像块分类。YOLO和MultiBox都使用卷积网络来预测图像中的边界框，但是YOLO是一个完整的检测系统。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;OverFeat.&lt;/strong&gt; Sermanet et al. train a convolutional neural network to perform localization and adapt that localizer to perform detection [32]. OverFeat efficiently performs sliding window detection but it is still a disjoint system. OverFeat optimizes for localization, not detection performance. Like DPM, the localizer only sees local information when making a prediction. OverFeat cannot reason about global context and thus requires significant post-processing to produce coherent detections.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;OverFeat。&lt;/strong&gt; Sermanet等人训练了一个卷积神经网络来执行定位，并使该定位器进行检测[32]。OverFeat高效地执行滑动窗口检测，但它仍然是一个不相交的系统。OverFeat优化了定位，而不是检测性能。像DPM一样，定位器在进行预测时只能看到局部信息。OverFeat不能推断全局上下文，因此需要大量的后处理来产生连贯的检测。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MultiGrasp.&lt;/strong&gt; Our work is similar in design to work on grasp detection by Redmon et al [27]. Our grid approach to bounding box prediction is based on the MultiGrasp system for regression to grasps. However, grasp detection is a much simpler task than object detection. MultiGrasp only needs to predict a single graspable region for an image containing one object. It doesn’t have to estimate the size, location, or boundaries of the object or predict it’s class, only find a region suitable for grasping. YOLO predicts both bounding boxes and class probabilities for multiple objects of multiple classes in an image.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MultiGrasp。&lt;/strong&gt;我们的工作在设计上类似于Redmon等[27]的抓取检测。我们对边界框预测的网格方法是基于MultiGrasp系统抓取的回归分析。然而，抓取检测比目标检测任务要简单得多。MultiGrasp只需要为包含一个目标的图像预测一个可以抓取的区域。不必估计目标的大小，位置或目标边界或预测目标的类别，只找到适合抓取的区域。YOLO预测图像中多个类别的多个目标的边界框和类别概率。&lt;/p&gt;

&lt;h3 id=&quot;4-experiments&quot;&gt;4. Experiments&lt;/h3&gt;
&lt;p&gt;First we compare YOLO with other real-time detection systems on PASCAL VOC 2007. To understand the differences between YOLO and R-CNN variants we explore the errors on VOC 2007 made by YOLO and Fast R-CNN, one of the highest performing versions of R-CNN [14]. Based on the different error profiles we show that YOLO can be used to rescore Fast R-CNN detections and reduce the errors from background false positives, giving a significant performance boost. We also present VOC 2012 results and compare mAP to current state-of-the-art methods. Finally, we show that YOLO generalizes to new domains better than other detectors on two artwork datasets.&lt;/p&gt;

&lt;h4 id=&quot;4-实验&quot;&gt;4. 实验&lt;/h4&gt;
&lt;p&gt;首先，我们在PASCAL VOC 2007上比较YOLO和其它的实时检测系统。为了理解YOLO和R-CNN变种之间的差异，我们探索了YOLO和R-CNN性能最高的版本之一Fast R-CNN[14]在VOC 2007上错误率。根据不同的误差曲线，我们显示YOLO可以用来重新评估Fast R-CNN检测，并减少背景假阳性带来的错误，从而显著提升性能。我们还展示了在VOC 2012上的结果，并与目前最先进的方法比较了mAP。最后，在两个艺术品数据集上我们显示了YOLO可以比其它检测器更好地泛化到新领域。&lt;/p&gt;

&lt;h4 id=&quot;41-comparison-to-other-real-time-systems&quot;&gt;4.1. Comparison to Other Real-Time Systems&lt;/h4&gt;
&lt;p&gt;Many research efforts in object detection focus on making standard detection pipelines fast [5] [38] [31] [14] [17] [28]. However, only Sadeghi et al. actually produce a detection system that runs in real-time (30 frames per second or better) [31]. We compare YOLO to their GPU implementation of DPM which runs either at 30Hz or 100Hz. While the other efforts don’t reach the real-time milestone we also compare their relative mAP and speed to examine the accuracy-performance tradeoffs available in object detection systems.&lt;/p&gt;

&lt;h4 id=&quot;41-与其它实时系统的比较&quot;&gt;4.1. 与其它实时系统的比较&lt;/h4&gt;
&lt;p&gt;目标检测方面的许多研究工作都集中在快速制定标准检测流程上[5]，[38]，[31]，[14]，[17]，[28]。然而，只有Sadeghi等实际上产生了一个实时运行的检测系统（每秒30帧或更好）[31]。我们将YOLO与DPM的GPU实现进行了比较，其在30Hz或100Hz下运行。虽然其它的努力没有达到实时性的里程碑，我们也比较了它们的相对mAP和速度来检查目标检测系统中精度——性能权衡。&lt;/p&gt;

&lt;p&gt;Fast YOLO is the fastest object detection method on PASCAL; as far as we know, it is the fastest extant object detector. With 52.7% mAP, it is more than twice as accurate as prior work on real-time detection. YOLO pushes mAP to 63.4% while still maintaining real-time performance.&lt;/p&gt;

&lt;p&gt;快速YOLO是PASCAL上最快的目标检测方法；据我们所知，它是现有的最快的目标检测器。具有52.7%的mAP，实时检测的精度是以前工作的两倍以上。YOLO将mAP推到63.4%的同时保持了实时性能。&lt;/p&gt;

&lt;p&gt;We also train YOLO using VGG-16. This model is more accurate but also significantly slower than YOLO. It is useful for comparison to other detection systems that rely on VGG-16 but since it is slower than real-time the rest of the paper focuses on our faster models.&lt;/p&gt;

&lt;p&gt;我们还使用VGG-16训练YOLO。这个模型比YOLO更准确，但也比它慢得多。对于依赖于VGG-16的其它检测系统来说，它是比较有用的，但由于它比实时的YOLO更慢，本文的其它部分将重点放在我们更快的模型上。&lt;/p&gt;

&lt;p&gt;Fastest DPM effectively speeds up DPM without sacrificing much mAP but it still misses real-time performance by a factor of 2 [38]. It also is limited by DPM’s relatively low accuracy on detection compared to neural network approaches.&lt;/p&gt;

&lt;p&gt;最快的DPM可以在不牺牲太多mAP的情况下有效地加速DPM，但仍然会将实时性能降低2倍[38]。与神经网络方法相比，DPM相对低的检测精度也受到限制。&lt;/p&gt;

&lt;p&gt;R-CNN minus R replaces Selective Search with static bounding box proposals [20]. While it is much faster than R-CNN, it still falls short of real-time and takes a significant accuracy hit from not having good proposals.&lt;/p&gt;

&lt;p&gt;减去R的R-CNN用静态边界框提出取代选择性搜索[20]。虽然速度比R-CNN更快，但仍然不能实时，并且由于没有好的边界框提出，准确性受到了严重影响。&lt;/p&gt;

&lt;p&gt;Fast R-CNN speeds up the classification stage of R-CNN but it still relies on selective search which can take around 2 seconds per image to generate bounding box proposals. Thus it has high mAP but at 0.5 fps it is still far from real-time.&lt;/p&gt;

&lt;p&gt;快速R-CNN加快了R-CNN的分类阶段，但是仍然依赖选择性搜索，每张图像需要花费大约2秒来生成边界框提出。因此，它具有很高的mAP，但是0.5的fps仍离实时性很远。&lt;/p&gt;

&lt;p&gt;The recent Faster R-CNN replaces selective search with a neural network to propose bounding boxes, similar to Szegedy et al. [8]. In our tests, their most accurate model achieves 7 fps while a smaller, less accurate one runs at 18 fps. The VGG-16 version of Faster R-CNN is 10 mAP higher but is also 6 times slower than YOLO. The Zeiler-Fergus Faster R-CNN is only 2.5 times slower than YOLO but is also less accurate.&lt;/p&gt;

&lt;p&gt;最近更快的R-CNN用神经网络替代了选择性搜索来提出边界框，类似于Szegedy等[8]。在我们的测试中，他们最精确的模型达到了7fps，而较小的，不太精确的模型以18fps运行。VGG-16版本的Faster R-CNN要高出10mAP，但比YOLO慢6倍。Zeiler-Fergus的Faster R-CNN只比YOLO慢了2.5倍，但也不太准确。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/yolo/4.png&quot; alt=&quot;4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;表1：Pascal VOC 2007上的实时系统。&lt;/strong&gt;比较快速检测器的性能和速度。快速YOLO是Pascal VOC检测记录中速度最快的检测器，其精度仍然是其它实时检测器的两倍。YOLO比快速版本更精确10mAP，同时在速度上仍保持实时性。&lt;/p&gt;

&lt;h4 id=&quot;42-voc-2007-error-analysis&quot;&gt;4.2. VOC 2007 Error Analysis&lt;/h4&gt;
&lt;p&gt;To further examine the differences between YOLO and state-of-the-art detectors, we look at a detailed breakdown of results on VOC 2007. We compare YOLO to Fast R-CNN since Fast R-CNN is one of the highest performing detectors on PASCAL and it’s detections are publicly available.&lt;/p&gt;

&lt;h4 id=&quot;42-voc-2007错误分析&quot;&gt;4.2. VOC 2007错误分析&lt;/h4&gt;
&lt;p&gt;为了进一步检查YOLO和最先进的检测器之间的差异，我们详细分析了VOC 2007的结果。我们将YOLO与Fast R-CNN进行比较，因为Fast R-CNN是PASCAL上性能最高的检测器之一并且它的检测代码是可公开得到的。&lt;/p&gt;

&lt;p&gt;We use the methodology and tools of Hoiem et al. [19] For each category at test time we look at the top N predictions for that category. Each prediction is either correct or it is classified based on the type of error:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Correct: correct class and IOU &amp;gt;.5&lt;/li&gt;
  &lt;li&gt;Localization: correct class, .1&amp;lt;IOU&amp;lt;.5&lt;/li&gt;
  &lt;li&gt;Similar: class is similar, IOU &amp;gt;.1&lt;/li&gt;
  &lt;li&gt;Other: class is wrong, IOU &amp;gt;.1&lt;/li&gt;
  &lt;li&gt;Background: IOU &amp;lt;.1 for any object&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Figure 4 shows the breakdown of each error type averaged across all 20 classes.&lt;/p&gt;

&lt;p&gt;我们使用Hoiem等人[19]的方法和工具。对于测试时的每个类别，我们看这个类别的前N个预测。每个预测或者是正确的，或者根据错误类型进行分类：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Correct：正确的类别且IOU&amp;gt;0.5。&lt;/li&gt;
  &lt;li&gt;Localization：正确的类别，0.1&amp;lt;IOU&amp;lt;0.5。&lt;/li&gt;
  &lt;li&gt;Similar：类别相似，IOU &amp;gt;0.1。&lt;/li&gt;
  &lt;li&gt;Other：类别错误，IOU &amp;gt;0.1。&lt;/li&gt;
  &lt;li&gt;Background：任何IOU &amp;lt;0.1的目标。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;图4显示了在所有的20个类别上每种错误类型平均值的分解图。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/yolo/5.png&quot; alt=&quot;5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 4: Error Analysis: Fast R-CNN vs. YOLO&lt;/strong&gt; These charts show the percentage of localization and background errors in the top N detections for various categories (N = # objects in that category).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;图4，误差分析：Fast R-CNN vs. YOLO。&lt;/strong&gt;这些图显示了各种类别的前N个预测中定位错误和背景错误的百分比（N = #表示目标在那个类别中）。&lt;/p&gt;

&lt;p&gt;YOLO struggles to localize objects correctly. Localization errors account for more of YOLO’s errors than all other sources combined. Fast R-CNN makes much fewer localization errors but far more background errors. 13.6% of it’s top detections are false positives that don’t contain any objects. Fast R-CNN is almost 3x more likely to predict background detections than YOLO.&lt;/p&gt;

&lt;p&gt;YOLO努力地正确定位目标。定位错误占YOLO错误的大多数，比其它错误源加起来都多。Fast R-CNN使定位错误少得多，但背景错误更多。它的检测的13.6%是不包含任何目标的误报。Fast R-CNN比YOLO预测背景检测的可能性高出近3倍。&lt;/p&gt;

&lt;h4 id=&quot;43-combining-fast-r-cnn-and-yolo&quot;&gt;4.3. Combining Fast R-CNN and YOLO&lt;/h4&gt;
&lt;p&gt;YOLO makes far fewer background mistakes than Fast R-CNN. By using YOLO to eliminate background detections from Fast R-CNN we get a significant boost in performance. For every bounding box that R-CNN predicts we check to see if YOLO predicts a similar box. If it does, we give that prediction a boost based on the probability predicted by YOLO and the overlap between the two boxes.&lt;/p&gt;

&lt;h4 id=&quot;43-结合fast-r-cnn和yolo&quot;&gt;4.3. 结合Fast R-CNN和YOLO&lt;/h4&gt;
&lt;p&gt;YOLO比Fast R-CNN的背景误检要少得多。通过使用YOLO消除Fast R-CNN的背景检测，我们获得了显著的性能提升。对于R-CNN预测的每个边界框，我们检查YOLO是否预测一个类似的框。如果是这样，我们根据YOLO预测的概率和两个盒子之间的重叠来对这个预测进行提升。&lt;/p&gt;

&lt;p&gt;The best Fast R-CNN model achieves a mAP of 71.8% on the VOC 2007 test set. When combined with YOLO, its mAP increases by 3.2% to 75.0%. We also tried combining the top Fast R-CNN model with several other versions of Fast R-CNN. Those ensembles produced small increases in mAP between .3 and .6%, see Table 2 for details.&lt;/p&gt;

&lt;p&gt;最好的Fast R-CNN模型在VOC 2007测试集上达到了71.8%的mAP。当与YOLO结合时，其mAP增加了3.2%达到了75.0%。我们也尝试将最好的Fast R-CNN模型与其它几个版本的Fast R-CNN结合起来。这些模型组合产生了0.3到0.6%之间的小幅增加，详见表2。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/yolo/6.png&quot; alt=&quot;6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;**Table 2: Model combination experiments on VOC 2007. We examine the effect of combining various models with the best version of Fast R-CNN. Other versions of Fast R-CNN provide only a small benefit while YOLO provides a significant performance boost.&lt;/p&gt;

&lt;p&gt;表2：VOC 2007模型组合实验。我们检验了各种模型与Fast R-CNN最佳版本结合的效果。Fast R-CNN的其它版本只提供很小的好处，而YOLO则提供了显著的性能提升。&lt;/p&gt;

&lt;p&gt;The boost from YOLO is not simply a byproduct of model ensembling since there is little benefit from combining different versions of Fast R-CNN. Rather, it is precisely because YOLO makes different kinds of mistakes at test time that it is so effective at boosting Fast R-CNN’s performance.&lt;/p&gt;

&lt;p&gt;来自YOLO的提升不仅仅是模型组合的副产品，因为组合不同版本的Fast R-CNN几乎没有什么好处。相反，正是因为YOLO在测试时出现了各种各样的错误，所以在提高Fast R-CNN的性能方面非常有效。&lt;/p&gt;

&lt;p&gt;Unfortunately, this combination doesn’t benefit from the speed of YOLO since we run each model seperately and then combine the results. However, since YOLO is so fast it doesn’t add any significant computational time compared to Fast R-CNN.&lt;/p&gt;

&lt;p&gt;遗憾的是，这个组合并没有从YOLO的速度中受益，因为我们分别运行每个模型，然后结合结果。但是，由于YOLO速度如此之快，与Fast R-CNN相比，不会增加任何显著的计算时间。&lt;/p&gt;

&lt;h4 id=&quot;44-voc-2012-results&quot;&gt;4.4. VOC 2012 Results&lt;/h4&gt;
&lt;p&gt;On the VOC 2012 test set, YOLO scores 57.9% mAP. This is lower than the current state of the art, closer to the original R-CNN using VGG-16, see Table 3. Our system struggles with small objects compared to its closest competitors. On categories like &lt;code class=&quot;highlighter-rouge&quot;&gt;bottle&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;sheep&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;tv/monitor&lt;/code&gt; YOLO scores 8−10% lower than R-CNN or Feature Edit. However, on other categories like &lt;code class=&quot;highlighter-rouge&quot;&gt;cat&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;train&lt;/code&gt; YOLO achieves higher performance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/yolo/7.png&quot; alt=&quot;7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table 3: PASCAL VOC 2012 Leaderboard.&lt;/strong&gt; YOLO compared with the full comp4 (outside data allowed) public leaderboard as of November 6th, 2015. Mean average precision and per-class average precision are shown for a variety of detection methods. YOLO is the only real-time detector. Fast R-CNN + YOLO is the forth highest scoring method, with a 2.3% boost over Fast R-CNN.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;表3：PASCAL VOC 2012排行榜。&lt;/strong&gt;截至2015年11月6日，YOLO与完整comp4（允许外部数据）公开排行榜进行了比较。显示了各种检测方法的平均精度均值和每类的平均精度。YOLO是唯一的实时检测器。Fast R-CNN + YOLO是评分第四高的方法，比Fast R-CNN提升了2.3％。&lt;/p&gt;

&lt;p&gt;Our combined Fast R-CNN + YOLO model is one of the highest performing detection methods. Fast R-CNN gets a 2.3% improvement from the combination with YOLO, boosting it 5 spots up on the public leaderboard.&lt;/p&gt;

&lt;p&gt;我们联合的Fast R-CNN + YOLO模型是性能最高的检测方法之一。Fast R-CNN从与YOLO的组合中获得了2.3%的提高，在公开排行榜上上移了5位。&lt;/p&gt;

&lt;h4 id=&quot;45-generalizability-person-detection-in-artwork&quot;&gt;4.5. Generalizability: Person Detection in Artwork&lt;/h4&gt;
&lt;p&gt;Academic datasets for object detection draw the training and testing data from the same distribution. In real-world applications it is hard to predict all possible use cases and the test data can diverge from what the system has seen before [3]. We compare YOLO to other detection systems on the Picasso Dataset [12] and the People-Art Dataset [3], two datasets for testing person detection on artwork.&lt;/p&gt;

&lt;h4 id=&quot;45-泛化能力艺术品中的行人检测&quot;&gt;4.5. 泛化能力：艺术品中的行人检测&lt;/h4&gt;
&lt;p&gt;用于目标检测的学术数据集以相同分布获取训练和测试数据。在现实世界的应用中，很难预测所有可能的用例，而且测试数据可能与系统之前看到的不同[3]。我们在Picasso数据集上[12]和People-Art数据集[3]上将YOLO与其它的检测系统进行比较，这两个数据集用于测试艺术品中的行人检测。&lt;/p&gt;

&lt;p&gt;Figure 5 shows comparative performance between YOLO and other detection methods. For reference, we give VOC 2007 detection AP on person where all models are trained only on VOC 2007 data. On Picasso models are trained on VOC 2012 while on People-Art they are trained on VOC 2010.&lt;/p&gt;

&lt;p&gt;图5显示了YOLO和其它检测方法之间的比较性能。作为参考，我们在person上提供VOC 2007的检测AP，其中所有模型仅在VOC 2007数据上训练。在Picasso数据集上的模型在VOC 2012上训练，而People-Art数据集上的模型则在VOC 2010上训练。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/yolo/8.png&quot; alt=&quot;8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 5: Generalization results on Picasso and People-Art datasets.&lt;/p&gt;

&lt;p&gt;图5：Picasso和People-Art数据集上的泛化结果。&lt;/p&gt;

&lt;p&gt;R-CNN has high AP on VOC 2007. However, R-CNN drops off considerably when applied to artwork. R-CNN uses Selective Search for bounding box proposals which is tuned for natural images. The classifier step in R-CNN only sees small regions and needs good proposals.&lt;/p&gt;

&lt;p&gt;R-CNN在VOC 2007上有高AP。然而，当应用于艺术品时，R-CNN明显下降。R-CNN使用选择性搜索来调整自然图像的边界框提出。R-CNN中的分类器步骤只能看到小区域，并且需要很好的边界框提出。&lt;/p&gt;

&lt;p&gt;DPM maintains its AP well when applied to artwork. Prior work theorizes that DPM performs well because it has strong spatial models of the shape and layout of objects. Though DPM doesn’t degrade as much as R-CNN, it starts from a lower AP.&lt;/p&gt;

&lt;p&gt;DPM在应用于艺术品时保持了其AP。之前的工作认为DPM表现良好，因为它具有目标形状和布局的强大空间模型。虽然DPM不会像R-CNN那样退化，但它开始时的AP较低。&lt;/p&gt;

&lt;p&gt;YOLO has good performance on VOC 2007 and its AP degrades less than other methods when applied to artwork. Like DPM, YOLO models the size and shape of objects, as well as relationships between objects and where objects commonly appear. Artwork and natural images are very different on a pixel level but they are similar in terms of the size and shape of objects, thus YOLO can still predict good bounding boxes and detections.&lt;/p&gt;

&lt;p&gt;YOLO在VOC 2007上有很好的性能，在应用于艺术品时其AP下降低于其它方法。像DPM一样，YOLO建模目标的大小和形状，以及目标和目标通常出现的位置之间的关系。艺术品和自然图像在像素级别上有很大不同，但是它们在目标的大小和形状方面是相似的，因此YOLO仍然可以预测好的边界框和检测结果。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/yolo/9.png&quot; alt=&quot;9&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 6: Qualitative Results.&lt;/strong&gt; YOLO running on sample artwork and natural images from the internet. It is mostly accurate although it does think one person is an airplane.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;图6：定性结果。&lt;/strong&gt; YOLO在网络采样的艺术品和自然图像上的运行结果。虽然它将人误检成了飞机，但它大部分上是准确的。&lt;/p&gt;

&lt;h3 id=&quot;5-real-time-detection-in-the-wild&quot;&gt;5. Real-Time Detection In The Wild&lt;/h3&gt;
&lt;p&gt;YOLO is a fast, accurate object detector, making it ideal for computer vision applications. We connect YOLO to a webcam and verify that it maintains real-time performance, including the time to fetch images from the camera and display the detections.&lt;/p&gt;

&lt;h4 id=&quot;5-现实环境下的实时检测&quot;&gt;5. 现实环境下的实时检测&lt;/h4&gt;
&lt;p&gt;YOLO是一种快速，精确的目标检测器，非常适合计算机视觉应用。我们将YOLO连接到网络摄像头，并验证它是否能保持实时性能，包括从摄像头获取图像并显示检测结果的时间。&lt;/p&gt;

&lt;p&gt;The resulting system is interactive and engaging. While YOLO processes images individually, when attached to a webcam it functions like a tracking system, detecting objects as they move around and change in appearance. A demo of the system and the source code can be found on our project website: &lt;a href=&quot;http://pjreddie.com/yolo/&quot;&gt;http://pjreddie.com/yolo/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;由此产生的系统是交互式和参与式的。虽然YOLO单独处理图像，但当连接到网络摄像头时，其功能类似于跟踪系统，可在目标移动和外观变化时检测目标。系统演示和源代码可以在我们的项目网站上找到：http://pjreddie.com/yolo/。&lt;/p&gt;

&lt;h3 id=&quot;6-conclusion&quot;&gt;6. Conclusion&lt;/h3&gt;
&lt;p&gt;We introduce YOLO, a unified model for object detection. Our model is simple to construct and can be trained directly on full images. Unlike classifier-based approaches, YOLO is trained on a loss function that directly corresponds to detection performance and the entire model is trained jointly.&lt;/p&gt;

&lt;h4 id=&quot;6-结论&quot;&gt;6. 结论&lt;/h4&gt;
&lt;p&gt;我们介绍了YOLO，一种统一的目标检测模型。我们的模型构建简单，可以直接在整张图像上进行训练。与基于分类器的方法不同，YOLO直接在对应检测性能的损失函数上训练，并且整个模型联合训练。&lt;/p&gt;

&lt;p&gt;Fast YOLO is the fastest general-purpose object detector in the literature and YOLO pushes the state-of-the-art in real-time object detection. YOLO also generalizes well to new domains making it ideal for applications that rely on fast, robust object detection.&lt;/p&gt;

&lt;p&gt;快速YOLO是文献中最快的通用目的的目标检测器，YOLO推动了实时目标检测的最新技术。YOLO还很好地泛化到新领域，使其成为依赖快速，强大的目标检测应用的理想选择。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Acknowledgements:&lt;/strong&gt; This work is partially supported by ONR N00014-13-1-0720, NSF IIS-1338054, and The Allen Distinguished Investigator Award.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;致谢：&lt;/strong&gt;这项工作得到了ONR N00014-13-1-0720，NSF IIS-1338054和艾伦杰出研究者奖的部分支持。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;References
[1] M. B. Blaschko and C. H. Lampert. Learning to localize objects with structured output regression. In Computer Vision–ECCV 2008, pages 2–15. Springer, 2008. 4&lt;/p&gt;

&lt;p&gt;[2] L. Bourdev and J. Malik. Poselets: Body part detectors trained using 3d human pose annotations. In International Conference on Computer Vision (ICCV), 2009. 8&lt;/p&gt;

&lt;p&gt;[3] H. Cai, Q. Wu, T. Corradi, and P. Hall. The cross-depiction problem: Computer vision algorithms for recognising objects in artwork and in photographs. arXiv preprint arXiv:1505.00110, 2015. 7&lt;/p&gt;

&lt;p&gt;[4] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 886–893. IEEE, 2005. 4, 8&lt;/p&gt;

&lt;p&gt;[5] T. Dean, M. Ruzon, M. Segal, J. Shlens, S. Vijaya-narasimhan, J. Yagnik, et al. Fast, accurate detection of 100,000 object classes on a single machine. In Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pages 1814–1821. IEEE, 2013. 5&lt;/p&gt;

&lt;p&gt;[6] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. arXiv preprint arXiv:1310.1531, 2013. 4&lt;/p&gt;

&lt;p&gt;[7] J. Dong, Q. Chen, S. Yan, and A. Yuille. Towards unified object detection and semantic segmentation. In Computer Vision–ECCV 2014, pages 299–314. Springer, 2014. 7&lt;/p&gt;

&lt;p&gt;[8] D.Erhan, C.Szegedy, A.Toshev, and D.Anguelov. Scalable object detection using deep neural networks. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 2155–2162. IEEE, 2014. 5, 6&lt;/p&gt;

&lt;p&gt;[9] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object classes challenge: A retrospective. International Journal of Computer Vision, 111(1):98–136, Jan. 2015. 2&lt;/p&gt;

&lt;p&gt;[10] P.F.Felzenszwalb, R.B.Girshick, D.McAllester, and D.Ramanan. Object detection with discriminatively trained part based models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(9):1627–1645, 2010. 1, 4&lt;/p&gt;

&lt;p&gt;[11] S. Gidaris and N. Komodakis. Object detection via a multi-region &amp;amp; semantic segmentation-aware CNN model. CoRR, abs/1505.01749, 2015. 7&lt;/p&gt;

&lt;p&gt;[12] S. Ginosar, D. Haas, T. Brown, and J. Malik. Detecting people in cubist art. In Computer Vision-ECCV 2014 Workshops, pages 101–116. Springer, 2014. 7&lt;/p&gt;

&lt;p&gt;[13] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 580–587. IEEE, 2014. 1, 4, 7&lt;/p&gt;

&lt;p&gt;[14] R. B. Girshick. Fast R-CNN. CoRR, abs/1504.08083, 2015. 2, 5, 6, 7&lt;/p&gt;

&lt;p&gt;[15] S. Gould, T. Gao, and D. Koller. Region-based segmentation and object detection. In Advances in neural information processing systems, pages 655–663, 2009. 4&lt;/p&gt;

&lt;p&gt;[16] B. Hariharan, P. Arbeláez, R. Girshick, and J. Malik. Simultaneous detection and segmentation. In Computer Vision–ECCV 2014, pages 297–312. Springer, 2014. 7&lt;/p&gt;

&lt;p&gt;[17] K.He, X.Zhang, S.Ren, and J.Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. arXiv preprint arXiv:1406.4729, 2014. 5&lt;/p&gt;

&lt;p&gt;[18] G.E.Hinton, N.Srivastava, A.Krizhevsky, I.Sutskever, and R. R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012. 4&lt;/p&gt;

&lt;p&gt;[19] D.Hoiem, Y.Chodpathumwan, and Q.Dai. Diagnosing error in object detectors. In Computer Vision–ECCV 2012, pages 340–353. Springer, 2012. 6&lt;/p&gt;

&lt;p&gt;[20] K. Lenc and A. Vedaldi. R-cnn minus r. arXiv preprint arXiv:1506.06981, 2015. 5, 6&lt;/p&gt;

&lt;p&gt;[21] R. Lienhart and J. Maydt. An extended set of haar-like features for rapid object detection. In Image Processing. 2002. Proceedings. 2002
International Conference on, volume 1, pages I–900. IEEE, 2002. 4&lt;/p&gt;

&lt;p&gt;[22] M. Lin, Q. Chen, and S. Yan. Network in network. CoRR, abs/1312.4400, 2013. 2&lt;/p&gt;

&lt;p&gt;[23] D. G. Lowe. Object recognition from local scale-invariant features. In Computer vision, 1999. The proceedings of the seventh IEEE international conference on, volume 2, pages 1150–1157. Ieee, 1999. 4&lt;/p&gt;

&lt;p&gt;[24] D. Mishkin. Models accuracy on imagenet 2012 val. https://github.com/BVLC/caffe/wiki/ Models-accuracy-on-ImageNet-2012-val. Accessed: 2015-10-2. 3&lt;/p&gt;

&lt;p&gt;[25] C. P. Papageorgiou, M. Oren, and T. Poggio. A general framework for object detection. In Computer vision, 1998. sixth international conference on, pages 555–562. IEEE, 1998. 4&lt;/p&gt;

&lt;p&gt;[26] J. Redmon. Darknet: Open source neural networks in c. http://pjreddie.com/darknet/, 2013–2016. 3&lt;/p&gt;

&lt;p&gt;[27] J.Redmon and A.Angelova. Real-time grasp detection using convolutional neural networks. CoRR, abs/1412.3128, 2014. 5&lt;/p&gt;

&lt;p&gt;[28] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. arXiv preprint arXiv:1506.01497, 2015. 5, 6, 7&lt;/p&gt;

&lt;p&gt;[29] S. Ren, K. He, R. B. Girshick, X. Zhang, and J. Sun. Object detection networks on convolutional feature maps. CoRR, abs/1504.06066, 2015. 3, 7&lt;/p&gt;

&lt;p&gt;[30] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 2015. 3&lt;/p&gt;

&lt;p&gt;[31] M. A. Sadeghi and D. Forsyth. 30hz object detection with dpm v5. In Computer Vision–ECCV 2014, pages 65–79. Springer, 2014. 5, 6&lt;/p&gt;

&lt;p&gt;[32] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. CoRR, abs/1312.6229, 2013. 4, 5&lt;/p&gt;

&lt;p&gt;[33] Z.Shen and X.Xue. Do more dropouts in pool5 feature maps for better object detection. arXiv preprint arXiv:1409.6911, 2014. 7&lt;/p&gt;

&lt;p&gt;[34] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. CoRR, abs/1409.4842, 2014. 2&lt;/p&gt;

&lt;p&gt;[35] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeulders. Selective search for object recognition. International journal of computer vision, 104(2):154–171, 2013. 4, 5&lt;/p&gt;

&lt;p&gt;[36] P. Viola and M. Jones. Robust real-time object detection. International Journal of Computer Vision, 4:34–47, 2001. 4&lt;/p&gt;

&lt;p&gt;[37] P. Viola and M. J. Jones. Robust real-time face detection. International journal of computer vision, 57(2):137–154, 2004. 5&lt;/p&gt;

&lt;p&gt;[38] J. Yan, Z. Lei, L. Wen, and S. Z. Li. The fastest deformable part model for object detection. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 2497–2504. IEEE, 2014. 5, 6&lt;/p&gt;

&lt;p&gt;[39] C.L.Zitnick and P.Dollár.Edgeboxes:Locating object proposals from edges. In Computer Vision–ECCV 2014, pages 391–405. Springer, 2014. 4&lt;/p&gt;

</description>
        <pubDate>Mon, 31 Dec 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/12/yolo/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/12/yolo/</guid>
        
        <category>深度学习-视觉</category>
        
        
        <category>深度学习-视觉</category>
        
      </item>
    
      <item>
        <title>AlexNet</title>
        <description>&lt;!--more--&gt;

&lt;p&gt;标签： 深度学习-视觉&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;imagenet-classification-with-deep-convolutional-neural-networks&quot;&gt;&lt;a href=&quot;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&quot;&gt;ImageNet Classification with Deep Convolutional Neural Networks&lt;/a&gt;&lt;/h3&gt;

&lt;h4 id=&quot;用更深的卷积神经网络分类imagenet&quot;&gt;用更深的卷积神经网络分类ImageNet&lt;/h4&gt;

&lt;h3 id=&quot;abstract&quot;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.&lt;/p&gt;

&lt;h4 id=&quot;摘要&quot;&gt;摘要&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;我们训练了一个大型深度卷积神经网络来将&lt;code class=&quot;highlighter-rouge&quot;&gt;ImageNet LSVRC-2010&lt;/code&gt;竞赛的120万高分辨率的图像分到1000不同的类别中。在测试数据上，我们得到了&lt;code class=&quot;highlighter-rouge&quot;&gt;top-1 37.5%, top-5 17.0%&lt;/code&gt;的错误率，这个结果比目前的最好结果好很多。这个神经网络有6000万参数和650000个神经元，包含5个卷积层（某些卷积层后面带有池化层）和3个全连接层，最后是一个1000维的softmax。为了训练的更快，我们使用了非饱和神经元并对卷积操作进行了非常有效的GPU实现。为了减少全连接层的过拟合，我们采用了一个最近开发的名为dropout的正则化方法，结果证明是非常有效的。我们也使用这个模型的一个变种参加了&lt;code class=&quot;highlighter-rouge&quot;&gt;ILSVRC-2012&lt;/code&gt;竞赛，赢得了&lt;code class=&quot;highlighter-rouge&quot;&gt;冠军&lt;/code&gt;并且与第二名 top-5 26.2%的错误率相比，我们取得了top-5 15.3%的错误率。&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;1-introduction&quot;&gt;1 Introduction&lt;/h3&gt;
&lt;p&gt;Current approaches to object recognition make essential use of machine learning methods. To improve their performance, we can collect larger datasets, learn more powerful models, and use better techniques for preventing overfitting. Until recently, datasets of labeled images were relatively small – on the order of tens of thousands of images (e.g., NORB [16], Caltech-101/256 [8, 9], and CIFAR-10/100 [12]). Simple recognition tasks can be solved quite well with datasets of this size, especially if they are augmented with label-preserving transformations. For example, the current best error rate on the MNIST digit-recognition task (&amp;lt;0.3%) approaches human performance [4]. But objects in realistic settings exhibit considerable variability, so to learn to recognize them it is necessary to use much larger training sets. And indeed, the shortcomings of small image datasets have been widely recognized (e.g., Pinto et al. [21]), but it has only recently become possible to collect labeled datasets with millions of images. The new larger datasets include LabelMe [23], which consists of hundreds of thousands of fully-segmented images, and ImageNet [6], which consists of over 15 million labeled high-resolution images in over 22,000 categories.&lt;/p&gt;

&lt;h4 id=&quot;1-引言&quot;&gt;1 引言&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;当前的目标识别方法基本上都使用了机器学习方法。为了提高目标识别的性能，我们可以收集更大的数据集，学习更强大的模型，使用更好的技术来防止过拟合。直到最近，标注图像的数据集都相对较小–在几万张图像的数量级上（例如，NORB[16]，Caltech-101/256 [8, 9]和CIFAR-10/100 [12]）。简单的识别任务在这样大小的数据集上可以被解决的相当好，尤其是如果通过标签保留变换进行数据增强的情况下。例如，目前在MNIST数字识别任务上（&amp;lt;0.3%）的最好准确率已经接近了人类水平[4]。但真实环境中的对象表现出了相当大的可变性，因此为了学习识别它们，有必要使用更大的训练数据集。实际上，小图像数据集的缺点已经被广泛认识到（例如，Pinto et al. [21]），但收集上百万图像的标注数据仅在最近才变得的可能。新的更大的数据集包括LabelMe [23]，它包含了数十万张完全分割的图像，ImageNet[6]，它包含了22000个类别上的超过1500万张标注的高分辨率的图像。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To learn about thousands of objects from millions of images, we need a model with a large learning capacity. However, the immense complexity of the object recognition task means that this problem cannot be specified even by a dataset as large as ImageNet, so our model should also have lots of prior knowledge to compensate for all the data we don’t have. Convolutional neural networks (CNNs) constitute one such class of models [16, 11, 13, 18, 15, 22, 26]. Their capacity can be controlled by varying their depth and breadth, and they also make strong and mostly correct assumptions about the nature of images (namely, stationarity of statistics and locality of pixel dependencies). Thus, compared to standard feedforward neural networks with similarly-sized layers, CNNs have much fewer connections and parameters and so they are easier to train, while their theoretically-best performance is likely to be only slightly worse.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;为了从数百万张图像中学习几千个对象，我们需要一个有很强学习能力的模型。然而对象识别任务的巨大复杂性意味着这个问题不能被指定，即使通过像ImageNet这样的大数据集，因此我们的模型应该也有许多先验知识来补偿我们所没有的数据。卷积神经网络(CNNs)构成了一个这样的模型[16, 11, 13, 18, 15, 22, 26]。它们的能力可以通过改变它们的广度和深度来控制，它们也可以对图像的本质进行强大且通常正确的假设（也就是说，统计的稳定性和像素依赖的局部性）。因此，与具有层次大小相似的标准前馈神经网络，CNNs有更少的连接和参数，因此它们更容易训练，而它们理论上的最佳性能可能仅比标准前馈神经网络差一点。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Despite the attractive qualities of CNNs, and despite the relative efficiency of their local architecture, they have still been prohibitively expensive to apply in large scale to high-resolution images. Luckily, current GPUs, paired with a highly-optimized implementation of 2D convolution, are powerful enough to facilitate the training of interestingly-large CNNs, and recent datasets such as ImageNet contain enough labeled examples to train such models without severe overfitting.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;尽管CNN具有引人注目的质量，尽管它们的局部架构相当有效，但将它们大规模的应用到到高分辨率图像中仍然是极其昂贵的。幸运的是，目前的GPU，搭配了高度优化的2D卷积实现，强大到足够促进有趣地大量CNN的训练，最近的数据集例如ImageNet包含足够的标注样本来训练这样的模型而没有严重的过拟合。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The specific contributions of this paper are as follows: we trained one of the largest convolutional neural networks to date on the subsets of ImageNet used in the ILSVRC-2010 and ILSVRC-2012 competitions [2] and achieved by far the best results ever reported on these datasets. We wrote a highly-optimized GPU implementation of 2D convolution and all the other operations inherent in training convolutional neural networks, which we make available publicly. Our network contains a number of new and unusual features which improve its performance and reduce its training time, which are detailed in Section 3. The size of our network made overfitting a significant problem, even with 1.2 million labeled training examples, so we used several effective techniques for preventing overfitting, which are described in Section 4. Our final network contains five convolutional and three fully-connected layers, and this depth seems to be important: we found that removing any convolutional layer (each of which contains no more than 1% of the model’s parameters) resulted in inferior performance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;本文具体的贡献如下：我们在ILSVRC-2010和ILSVRC-2012[2]的ImageNet子集上训练了到目前为止最大的神经网络之一，并取得了迄今为止在这些数据集上报道过的最好结果。我们编写了高度优化的2D卷积GPU实现以及训练卷积神经网络内部的所有其它操作，我们把它公开了。我们的网络包含许多新的不寻常的特性，这些特性提高了神经网络的性能并减少了训练时间，详见第三节。即使使用了120万标注的训练样本，我们的网络尺寸仍然使过拟合成为一个明显的问题，因此我们使用了一些有效的技术来防止过拟合，详见第四节。我们最终的网络包含5个卷积层和3个全连接层，深度似乎是非常重要的：我们发现移除任何卷积层（每个卷积层包含的参数不超过模型参数的1%）都会导致更差的性能。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In the end, the network’s size is limited mainly by the amount of memory available on current GPUs and by the amount of training time that we are willing to tolerate. Our network takes between five and six days to train on two GTX 580 3GB GPUs. All of our experiments suggest that our results can be improved simply by waiting for faster GPUs and bigger datasets to become available.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;最后，网络尺寸主要受限于目前GPU的内存容量和我们能忍受的训练时间。我们的网络在两个GTX 580 3GB GPU上训练五六天。我们的所有实验表明我们的结果可以简单地通过等待更快的GPU和更大的可用数据集来提高。&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;2the-dataset&quot;&gt;2.The Dataset&lt;/h3&gt;
&lt;p&gt;ImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories. The images were collected from the web and labeled by human labelers using Amazon’s Mechanical Turk crowd-sourcing tool. Starting in 2010, as part of the Pascal Visual Object Challenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held. ILSVRC uses a subset of ImageNet with roughly 1000 images in each of 1000 categories. In all, there are roughly 1.2 million training images, 50,000 validation images, and 150,000 testing images.&lt;/p&gt;

&lt;h4 id=&quot;2数据集&quot;&gt;2.数据集&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;ImageNet数据集有超过1500万的标注高分辨率图像，这些图像属于大约22000个类别。这些图像是从网上收集的，使用了Amazon’s Mechanical Turk的众包工具通过人工标注的。从2010年起，作为Pascal视觉对象挑战赛的一部分，每年都会举办ImageNet大规模视觉识别挑战赛（ILSVRC）。ILSVRC使用ImageNet的一个子集，1000个类别每个类别大约1000张图像。总计，大约120万训练图像，50000张验证图像和15万测试图像。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ILSVRC-2010 is the only version of ILSVRC for which the test set labels are available, so this is the version on which we performed most of our experiments. Since we also entered our model in the ILSVRC-2012 competition, in Section 6 we report our results on this version of the dataset as well, for which test set labels are unavailable. On ImageNet, it is customary to report two error rates: top-1 and top-5, where the top-5 error rate is the fraction of test images for which the correct label is not among the five labels considered most probable by the model.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ILSVRC-2010是ILSVRC竞赛中唯一可以获得测试集标签的版本，因此我们大多数实验都是在这个版本上运行的。由于我们也使用我们的模型参加了ILSVRC-2012竞赛，因此在第六节我们也报告了模型在这个版本的数据集上的结果，这个版本的测试标签是不可获得的。在ImageNet上，按照惯例报告两个错误率：&lt;code class=&quot;highlighter-rouge&quot;&gt;top-1&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;top-5&lt;/code&gt;，top-5错误率是指测试图像的正确标签不在模型认为的五个最可能的便签之中。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ImageNet consists of variable-resolution images, while our system requires a constant input dimensionality. Therefore, we down-sampled the images to a fixed resolution of 256 × 256. Given a rectangular image, we first rescaled the image such that the shorter side was of length 256, and then cropped out the central 256×256 patch from the resulting image. We did not pre-process the images in any other way, except for subtracting the mean activity over the training set from each pixel. So we trained our network on the (centered) raw RGB values of the pixels.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ImageNet包含各种分辨率的图像，&lt;code class=&quot;highlighter-rouge&quot;&gt;而我们的系统要求不变的输入维度&lt;/code&gt;。因此，我们将图像进行下采样到固定的&lt;code class=&quot;highlighter-rouge&quot;&gt;256×256&lt;/code&gt;分辨率。给定一个矩形图像，我们首先缩放图像短边长度为256，然后从结果图像中裁剪中心的256×256大小的图像块。除了在训练集上对像素减去平均活跃度外，我们不对图像做任何其它的预处理。因此我们在原始的RGB像素值（中心的）上训练我们的网络。&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;3the-architecture&quot;&gt;3.The Architecture&lt;/h3&gt;
&lt;p&gt;The architecture of our network is summarized in Figure 2. It contains eight learned layers — five convolutional and three fully-connected. Below, we describe some of the novel or unusual features of our network’s architecture. Sections 3.1-3.4 are sorted according to our estimation of their importance, with the most important first.&lt;/p&gt;

&lt;h4 id=&quot;3架构&quot;&gt;3.架构&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;我们的网络架构概括为图2。它包含八个学习层–5个卷积层和3个全连接层。下面，我们将描述我们网络结构中的一些新奇的不寻常的特性。3.1-3.4小节按照我们对它们评估的重要性进行排序，最重要的最有先。&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;31-relu-nonlinearity&quot;&gt;3.1 ReLU Nonlinearity&lt;/h4&gt;
&lt;p&gt;The standard way to model a neuron’s output f as a function of its input x is with f(x) = tanh(x) or f(x) = (1 + e−x)−1. In terms of training time with gradient descent, these saturating nonlinearities are much slower than the non-saturating nonlinearity f(x) = max(0,x). Following Nair and Hinton [20], we refer to neurons with this nonlinearity as Rectified Linear Units (ReLUs). Deep convolutional neural networks with ReLUs train several times faster than their equivalents with tanh units. This is demonstrated in Figure 1, which shows the number of iterations required to reach 25% training error on the CIFAR-10 dataset for a particular four-layer convolutional network. This plot shows that we would not have been able to experiment with such large neural networks for this work if we had used traditional saturating neuron models.&lt;/p&gt;

&lt;h4 id=&quot;31-relu非线性&quot;&gt;3.1 ReLU非线性&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;将神经元输出$f$建模为输入$x$的函数的标准方式是用$f(x) = tanh(x)$或$f(x) = (1 + e−x)−1$。考虑到梯度下降的训练时间，这些饱和的非线性比非饱和非线性$f(x) = max(0,x)$更慢。根据Nair和Hinton[20]的说法，&lt;code class=&quot;highlighter-rouge&quot;&gt;我们将这种非线性神经元称为修正线性单元(ReLU)&lt;/code&gt;。采用ReLU的深度卷积神经网络训练时间比等价的tanh单元要快几倍。在图1中，对于一个特定的四层卷积网络，在CIFAR-10数据集上达到25%的训练误差所需要的迭代次数可以证实这一点。这幅图表明，如果我们采用传统的饱和神经元模型，我们将不能在如此大的神经网络上实验该工作。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/AlexNet/1.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 1: A four-layer convolutional neural network with ReLUs (solid line) reaches a 25% training error rate on CIFAR-10 six times faster than an equivalent network with tanh neurons (dashed line). The learning rates for each network were chosen independently to make training as fast as possible. No regularization of any kind was employed. The magnitude of the effect demonstrated here varies with network architecture, but networks with ReLUs consistently learn several times faster than equivalents with saturating neurons.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;图1：使用ReLU的四层卷积神经网络在CIFAR-10数据集上达到25%的训练误差比使用tanh神经元的等价网络（虚线）快六倍。为了使训练尽可能快，每个网络的学习率是单独选择的。没有采用任何类型的正则化。影响的大小随着网络结构的变化而变化，这一点已得到证实，但使用ReLU的网络都比等价的饱和神经元快几倍。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We are not the first to consider alternatives to traditional neuron models in CNNs. For example, Jarrett et al. [11] claim that the nonlinearity f(x) = |tanh(x)| works particularly well with their type of contrast normalization followed by local average pooling on the Caltech-101 dataset. However, on this dataset the primary concern is preventing overfitting, so the effect they are observing is different from the accelerated ability to fit the training set which we report when using ReLUs. Faster learning has a great influence on the performance of large models trained on large datasets.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;我们不是第一个考虑替代CNN中传统神经元模型的人。例如，Jarrett等人[11]声称非线性函数f(x) = |tanh(x)|与其对比度归一化一起，然后是局部均值池化，在Caltech-101数据集上工作的非常好。然而，在这个数据集上主要的关注点是防止过拟合，因此他们观测到的影响不同于我们使用ReLU拟合数据集时的加速能力。更快的学习对大型数据集上大型模型的性能有很大的影响。&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;32-training-on-multiple-gpus&quot;&gt;3.2 Training on Multiple GPUs&lt;/h4&gt;
&lt;p&gt;A single GTX 580 GPU has only 3GB of memory, which limits the maximum size of the networks that can be trained on it. It turns out that 1.2 million training examples are enough to train networks which are too big to fit on one GPU. Therefore we spread the net across two GPUs. Current GPUs are particularly well-suited to cross-GPU parallelization, as they are able to read from and write to one another’s memory directly, without going through host machine memory. The parallelization scheme that we employ essentially puts half of the kernels (or neurons) on each GPU, with one additional trick: the GPUs communicate only in certain layers. This means that, for example, the kernels of layer 3 take input from all kernel maps in layer 2. However, kernels in layer 4 take input only from those kernel maps in layer 3 which reside on the same GPU. Choosing the pattern of connectivity is a problem for cross-validation, but this allows us to precisely tune the amount of communication until it is an acceptable fraction of the amount of computation.&lt;/p&gt;

&lt;h4 id=&quot;32-多gpu训练&quot;&gt;3.2 多GPU训练&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;单个GTX580 GPU只有3G内存，这限制了可以在GTX580上进行训练的网络最大尺寸。事实证明120万图像用来进行网络训练是足够的，但网络太大因此不能在单个GPU上进行训练。因此我们将网络分布在两个GPU上。目前的GPU非常适合跨GPU并行，因为它们可以直接互相读写内存，而不需要通过主机内存。我们采用的并行方案基本上每个GPU放置一半的核（或神经元），还有一个额外的技巧：只在某些特定的层上进行GPU通信。这意味着，例如，第3层的核会将第2层的所有核映射作为输入。然而，第4层的核只将位于相同GPU上的第3层的核映射作为输入。连接模式的选择是一个交叉验证问题，但这可以让我们准确地调整通信数量，直到它的计算量在可接受的范围内。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The resultant architecture is somewhat similar to that of the “columnar” CNN employed by Ciresan et al. [5], except that our columns are not independent (see Figure 2). This scheme reduces our top-1 and top-5 error rates by 1.7% and 1.2%, respectively, as compared with a net with half as
many kernels in each convolutional layer trained on one GPU. The two-GPU net takes slightly less time to train than the one-GPU net.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;除了我们的列不是独立的之外（看图2），最终的架构有点类似于Ciresan等人[5]采用的“columnar” CNN。与每个卷积层一半的核在单GPU上训练的网络相比，这个方案降分别低了我们的top-1 1.7%，top-5 1.2%的错误率。双GPU网络比单GPU网络稍微减少了训练时间。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/AlexNet/2.png&quot; alt=&quot;2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2: An illustration of the architecture of our CNN, explicitly showing the delineation of responsibilities between the two GPUs. One GPU runs the layer-parts at the top of the figure while the other runs the layer-parts at the bottom. The GPUs communicate only at certain layers. The network’s input is 150,528-dimensional, and the number of neurons in the network’s remaining layers is given by 253,440–186,624–64,896–64,896–43,264– 4096–4096–1000.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;图 2：我们CNN架构图解，明确描述了两个GPU之间的责任。在图的顶部，一个GPU运行在部分层上，而在图的底部，另一个GPU运行在部分层上。GPU只在特定的层进行通信。网络的输入是150,528维，网络剩下层的神经元数目分别是253,440–186,624–64,896–64,896–43,264–4096–4096–1000（8层）。&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;33-local-response-normalization&quot;&gt;3.3 Local Response Normalization&lt;/h4&gt;
&lt;p&gt;ReLUs have the desirable property that they do not require input normalization to prevent them from saturating. If at least some training examples produce a positive input to a ReLU, learning will happen in that neuron. However, we still find that the following local normalization scheme aids generalization. Denoting by $a_{x,y}^i$ the activity of a neuron computed by applying kernel $i$ at position $(x,y)$ and then applying the ReLU nonlinearity, the response-normalized activity $b^i_{x,y}$ is given by the expression
&lt;script type=&quot;math/tex&quot;&gt;b^i_{x,y} = a_{x,y}^i / ( k + \alpha \sum _{j = max(0, i-n / 2)} ^{min(N-1, i+n / 2)} (a_{x,y}^i)^2 )^\beta&lt;/script&gt;
where the sum runs over n “adjacent” kernel maps at the same spatial position, and N is the total number of kernels in the layer. The ordering of the kernel maps is of course arbitrary and determined before training begins. This sort of response normalization implements a form of lateral inhibition inspired by the type found in real neurons, creating competition for big activities amongst neuron outputs computed using different kernels. The constants k, n, α, and β are hyper-parameters whose values are determined using a validation set; we used k = 2, n = 5, α = 0.0001, and β = 0.75. We applied this normalization after applying the ReLU nonlinearity in certain layers (see Section 3.5).&lt;/p&gt;

&lt;h4 id=&quot;33-局部响应归一化&quot;&gt;3.3 局部响应归一化&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;ReLU具有让人满意的特性，它不需要通过输入归一化来防止饱和。如果至少一些训练样本对ReLU产生了正输入，那么那个神经元上将发生学习。然而，我们仍然发现接下来的局部响应归一化有助于泛化。$a_{x,y}^i$表示神经元激活，通过在$(x,y)$位置应用核$i$，然后应用ReLU非线性来计算，响应归一化激活$b_{x,y}^i$通过下式给定：&lt;/strong&gt;
&lt;script type=&quot;math/tex&quot;&gt;b^i_{x,y} = a_{x,y}^i / ( k + \alpha \sum _{j = max(0, i-n / 2)} ^{min(N-1, i+n / 2)} (a_{x,y}^i)^2 )^\beta&lt;/script&gt;
&lt;strong&gt;求和运算在n个“毗邻的”核映射的同一位置上执行，N是本层的卷积核数目。核映射的顺序当然是任意的，在训练开始前确定。响应归一化的顺序实现了一种侧抑制形式，灵感来自于真实神经元中发现的类型，为使用不同核进行神经元输出计算的较大活动创造了竞争。常量k，n，α，β是超参数，它们的值通过验证集确定；我们设k=2，n=5，α=0.0001，β=0.75。我们在特定的层使用的ReLU非线性之后应用了这种归一化（请看3.5小节）。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This scheme bears some resemblance to the local contrast normalization scheme of Jarrett et al. [11], but ours would be more correctly termed “brightness normalization”, since we do not subtract the mean activity. Response normalization reduces our top-1 and top-5 error rates by 1.4% and 1.2%, respectively. We also verified the effectiveness of this scheme on the CIFAR-10 dataset: a four-layer CNN achieved a 13% test error rate without normalization and 11% with normalization.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;这个方案与Jarrett等人[11]的局部对比度归一化方案有一定的相似性，但我们更恰当的称其为“亮度归一化”，因此我们没有减去均值。响应归一化分别减少了top-1 1.4%，top-5 1.2%的错误率。我们也在CIFAR-10数据集上验证了这个方案的有效性：一个不使用归一化的四层CNN取得了13%的错误率，而使用归一化取得了11%的错误率。&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;34-overlapping-pooling&quot;&gt;3.4 Overlapping Pooling&lt;/h4&gt;

&lt;p&gt;Pooling layers in CNNs summarize the outputs of neighboring groups of neurons in the same kernel map. Traditionally, the neighborhoods summarized by adjacent pooling units do not overlap (e.g., [17, 11, 4]). To be more precise, a pooling layer can be thought of as consisting of a grid of pooling units spaced s pixels apart, each summarizing a neighborhood of size z×z centered at the location of the pooling unit. If we set $s=z$, we obtain traditional local pooling as commonly employed in CNNs. If we set $s&amp;lt;z$, we obtain overlapping pooling. This is what we use throughout our network, with $s=2$ and $z=3$. This scheme reduces the top-1 and top-5 error rates by 0.4% and 0.3%, respectively, as compared with the non-overlapping scheme $s=2$,$z=2$, which produces output of equivalent dimensions. We generally observe during training that models with overlapping pooling find it slightly more difficult to overfit.&lt;/p&gt;

&lt;h4 id=&quot;34-重叠池化&quot;&gt;3.4 重叠池化&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;CNN中的池化层归纳了同一核映射上相邻组神经元的输出。习惯上，相邻池化单元归纳的区域是不重叠的（例如[17, 11, 4]）。更确切的说，池化层可看作由池化单元网格组成，网格间距为s个像素，每个网格归纳池化单元中心位置z×z大小的邻居。如果设置$s=z$，我们会得到通常在CNN中采用的传统局部池化。如果设置$s&amp;lt;z$，我们会得到重叠池化。这就是我们网络中使用的方法，设置$s=2$，$z=3$。这个方案分别降低了top-1 0.4%，top-5 0.3%的错误率，与非重叠方案$s=2$，$z=2$相比，输出的维度是相等的。我们在训练过程中通常观察采用重叠池化的模型，发现它更难过拟合。&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;35-overall-architecture&quot;&gt;3.5 Overall Architecture&lt;/h4&gt;
&lt;p&gt;Now we are ready to describe the overall architecture of our CNN. As depicted in Figure 2, the net contains eight layers with weights; the first five are convolutional and the remaining three are fully-connected. The output of the last fully-connected layer is fed to a 1000-way softmax which produces a distribution over the 1000 class labels. Our network maximizes the multinomial logistic regression objective, which is equivalent to maximizing the average across training cases of the log-probability of the correct label under the prediction distribution.&lt;/p&gt;

&lt;h4 id=&quot;35-整体架构&quot;&gt;3.5 整体架构&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;现在我们准备描述我们的CNN的整体架构。如图2所示，我们的网络包含8个带权重的层；前5层是卷积层，剩下的3层是全连接层。最后一层全连接层的输出是1000维softmax的输入，softmax会产生1000类标签的分布。我们的网络最大化多项逻辑回归的目标，这等价于最大化预测分布下训练样本正确标签的对数概率的均值。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The kernels of the second, fourth, and fifth convolutional layers are connected only to those kernel maps in the previous layer which reside on the same GPU (see Figure 2). The kernels of the third convolutional layer are connected to all kernel maps in the second layer. The neurons in the fully-connected layers are connected to all neurons in the previous layer. Response-normalization layers follow the first and second convolutional layers. Max-pooling layers, of the kind described in Section 3.4, follow both response-normalization layers as well as the fifth convolutional layer. The ReLU non-linearity is applied to the output of every convolutional and fully-connected layer.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第2，4，5卷积层的核只与位于同一GPU上的前一层的核映射相连接（看图2）。第3卷积层的核与第2层的所有核映射相连。全连接层的神经元与前一层的所有神经元相连。第1，2卷积层之后是响应归一化层。3.4节描述的这种最大池化层在响应归一化层和第5卷积层之后。ReLU非线性应用在每个卷积层和全连接层的输出上。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The first convolutional layer filters the 224 × 224 × 3 input image with 96 kernels of size 11 × 11 × 3 with a stride of 4 pixels (this is the distance between the receptive field centers of neighboring neurons in a kernel map). The second convolutional layer takes as input the (response-normalized and pooled) output of the first convolutional layer and filters it with 256 kernels of size 5 × 5 × 48. The third, fourth, and fifth convolutional layers are connected to one another without any intervening pooling or normalization layers. The third convolutional layer has 384 kernels of size 3 × 3 × 256 connected to the (normalized, pooled) outputs of the second convolutional layer. The fourth convolutional layer has 384 kernels of size 3 × 3 × 192 , and the fifth convolutional layer has 256 kernels of size 3 × 3 × 192. The fully-connected layers have 4096 neurons each.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第1卷积层使用96个核对224 × 224 × 3的输入图像进行滤波，核大小为11 × 11 × 3，步长是4个像素（核映射中相邻神经元感受野中心之间的距离）。第2卷积层使用用第1卷积层的输出（响应归一化和池化）作为输入，并使用256个核进行滤波，核大小为5 × 5 × 48。第3，4，5卷积层互相连接，中间没有接入池化层或归一化层。第3卷积层有384个核，核大小为3 × 3 × 256，与第2卷积层的输出（归一化的，池化的）相连。第4卷积层有384个核，核大小为3 × 3 × 192，第5卷积层有256个核，核大小为3 × 3 × 192。每个全连接层有4096个神经元。&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;4-reducing-overfitting&quot;&gt;4 Reducing Overfitting&lt;/h3&gt;
&lt;p&gt;Our neural network architecture has 60 million parameters. Although the 1000 classes of ILSVRC make each training example impose 10 bits of constraint on the mapping from image to label, this turns out to be insufficient to learn so many parameters without considerable overfitting. Below, we describe the two primary ways in which we combat overfitting.&lt;/p&gt;

&lt;h4 id=&quot;4-减少过拟合&quot;&gt;4 减少过拟合&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;我们的神经网络架构有6000万参数。尽管ILSVRC的1000类使每个训练样本从图像到标签的映射上强加了10比特的约束，但这不足以学习这么多的参数而没有相当大的过拟合。下面，我们会描述我们用来克服过拟合的两种主要方式。&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;41-data-augmentation&quot;&gt;4.1 Data Augmentation&lt;/h4&gt;
&lt;p&gt;The easiest and most common method to reduce overfitting on image data is to artificially enlarge the dataset using label-preserving transformations (e.g., [25, 4, 5]). We employ two distinct forms of data augmentation, both of which allow transformed images to be produced from the original images with very little computation, so the transformed images do not need to be stored on disk. In our implementation, the transformed images are generated in Python code on the CPU while the GPU is training on the previous batch of images. So these data augmentation schemes are, in effect, computationally free.&lt;/p&gt;

&lt;h4 id=&quot;41-数据增强&quot;&gt;4.1 数据增强&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;图像数据上最简单常用的用来减少过拟合的方法是使用标签保留变换（例如[25, 4, 5]）来人工增大数据集。我们使用了两种独特的数据增强方式，这两种方式都可以从原始图像通过非常少的计算量产生变换的图像，因此变换图像不需要存储在硬盘上。在我们的实现中，变换图像通过CPU的Python代码生成，而此时GPU正在训练前一批图像。因此，实际上这些数据增强方案是计算免费的。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The first form of data augmentation consists of generating image translations and horizontal reflections. We do this by extracting random 224 × 224 patches (and their horizontal reflections) from the 256×256 images and training our network on these extracted patches. This increases the size of our training set by a factor of 2048, though the resulting training examples are, of course, highly interdependent. Without this scheme, our network suffers from substantial overfitting, which would have forced us to use much smaller networks. At test time, the network makes a prediction by extracting five 224 × 224 patches (the four corner patches and the center patch) as well as their horizontal reflections (hence ten patches in all), and averaging the predictions made by the network’s softmax layer on the ten patches.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第一种数据增强方式包括产生图像变换和水平翻转。我们从256×256图像上通过随机提取224 × 224的图像块实现了这种方式，然后在这些提取的图像块上进行训练。这通过一个2048因子增大了我们的训练集，尽管最终的训练样本是高度相关的。没有这个方案，我们的网络会有大量的过拟合，这会迫使我们使用更小的网络。在测试时，网络会提取5个224 × 224的图像块（四个角上的图像块和中心的图像块）和它们的水平翻转（因此总共10个图像块）进行预测，然后对网络在10个图像块上的softmax层进行平均。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The second form of data augmentation consists of altering the intensities of the RGB channels in training images. Specifically, we perform PCA on the set of RGB pixel values throughout the ImageNet training set. To each training image, we add multiples of the found principal components, with magnitudes proportional to the corresponding eigenvalues times a random variable drawn from a Gaussian with mean zero and standard deviation 0.1. Therefore to each RGB image pixel $I_xy = [I^R_{xy} , I^G_{xy} , I^B_{xy} ]^T$ we add the following quantity:
&lt;script type=&quot;math/tex&quot;&gt;[p_1, p_2, p_3][\alpha_1\lambda_1, \alpha_2\lambda_2, \alpha_3\lambda_3]^T&lt;/script&gt;
where $p_i$ and $λ_i$ are $i$th eigenvector and eigenvalue of the 3 × 3 covariance matrix of RGB pixel values, respectively, and $α_i$ is the aforementioned random variable. Each $α_i$ is drawn only once for all the pixels of a particular training image until that image is used for training again, at which point it is re-drawn. This scheme approximately captures an important property of natural images, namely, that object identity is invariant to changes in the intensity and color of the illumination. This scheme reduces the top-1 error rate by over 1%.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第二种数据增强方式包括改变训练图像的RGB通道的强度。具体地，我们在整个ImageNet训练集上对RGB像素值集合执行PCA。对于每幅训练图像，我们加上多倍找到的主成分，大小成正比的对应特征值乘以一个随机变量，随机变量通过均值为0，标准差为0.1的高斯分布得到。因此对于每幅RGB图像像素$I_xy = [I^R_{xy} , I^G_{xy} , I^B_{xy} ]^T$，我们加上下面的数量：&lt;/strong&gt;
&lt;script type=&quot;math/tex&quot;&gt;[p_1, p_2, p_3][\alpha_1\lambda_1, \alpha_2\lambda_2, \alpha_3\lambda_3]^T&lt;/script&gt;
&lt;strong&gt;$p_i$，$λ_i$分别是RGB像素值3 × 3协方差矩阵的第i个特征向量和特征值，$α_i$是前面提到的随机变量。对于某个训练图像的所有像素，每个$α_i$只获取一次，直到图像进行下一次训练时才重新获取。这个方案近似抓住了自然图像的一个重要特性，即光照的颜色和强度发生变化时，目标身份是不变的。这个方案减少了&lt;code class=&quot;highlighter-rouge&quot;&gt;top 1&lt;/code&gt;错误率1%以上。&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;42-dropout&quot;&gt;4.2 Dropout&lt;/h4&gt;
&lt;p&gt;Combining the predictions of many different models is a very successful way to reduce test errors [1, 3], but it appears to be too expensive for big neural networks that already take several days to train. There is, however, a very efficient version of model combination that only costs about a factor of two during training. The recently-introduced technique, called “dropout” [10], consists of setting to zero the output of each hidden neuron with probability 0.5. The neurons which are “dropped out” in this way do not contribute to the forward pass and do not participate in back-propagation. So every time an input is presented, the neural network samples a different architecture, but all these architectures share weights. This technique reduces complex co-adaptations of neurons, since a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to learn more robust features that are useful in conjunction with many different random subsets of the other neurons. At test time, we use all the neurons but multiply their outputs by 0.5, which is a reasonable approximation to taking the geometric mean of the predictive distributions produced by the exponentially-many dropout networks.&lt;/p&gt;

&lt;h4 id=&quot;42-失活dropout&quot;&gt;4.2 失活(Dropout)&lt;/h4&gt;
&lt;p&gt;将许多不同模型的预测结合起来是降低测试误差[1, 3]的一个非常成功的方法，但对于需要花费几天来训练的大型神经网络来说，这似乎太昂贵了。然而，有一个非常有效的模型结合版本，它只花费两倍的训练成本。这种最近引入的技术，叫做“dropout”[10]，它会以0.5的概率对每个隐层神经元的输出设为0。&lt;code class=&quot;highlighter-rouge&quot;&gt;那些“失活的”的神经元不再进行前向传播并且不参与反向传播&lt;/code&gt;。因此每次输入时，神经网络会采样一个不同的架构，但所有架构共享权重。这个技术减少了复杂的神经元互适应，因为一个神经元不能依赖特定的其它神经元的存在。因此，神经元被强迫学习更鲁棒的特征，它在与许多不同的其它神经元的随机子集结合时是有用的。在测试时，我们使用所有的神经元但它们的&lt;code class=&quot;highlighter-rouge&quot;&gt;输出乘以0.5&lt;/code&gt;，对指数级的许多失活网络的预测分布进行几何平均，这是一种合理的近似。&lt;/p&gt;

&lt;p&gt;We use dropout in the first two fully-connected layers of Figure 2. Without dropout, our network exhibits substantial overfitting. Dropout roughly doubles the number of iterations required to converge.&lt;/p&gt;

&lt;p&gt;我们在图2中的前两个全连接层使用失活。如果没有失活，我们的网络表现出大量的过拟合。&lt;code class=&quot;highlighter-rouge&quot;&gt;失活大致上使要求收敛的迭代次数翻了一倍&lt;/code&gt;。&lt;/p&gt;

&lt;h3 id=&quot;5-details-of-learning&quot;&gt;5 Details of learning&lt;/h3&gt;
&lt;p&gt;We trained our models using stochastic gradient descent with a batch size of 128 examples, momentum of 0.9, and weight decay of 0.0005. We found that this small amount of weight decay was important for the model to learn. In other words, weight decay here is not merely a regularizer: it reduces the model’s training error. The update rule for weight $w$ was
&lt;script type=&quot;math/tex&quot;&gt;v_{i+1} := 0.9 \bullet v_i - 0.0005 \bullet \varepsilon \bullet w_i - \varepsilon \bullet \langle \frac{\partial L} {\partial w} |_{w_i}\rangle _{D_i}&lt;/script&gt;
where $i$ is the iteration index, $v$ is the momentum variable, $ε$ is the learning rate, and $\langle \frac{\partial L} {\partial w} |_{w_i}\rangle _{D_i}$ is the average over the ith batch $D_i$ of the derivative of the objective with respect to $w$, evaluated at $w_i$.&lt;/p&gt;

&lt;h4 id=&quot;5-学习细节&quot;&gt;5 学习细节&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;我们使用随机梯度下降来训练我们的模型，样本的batch size为128，动量为0.9，权重衰减为0.0005。我们发现少量的权重衰减对于模型的学习是重要的。换句话说，权重衰减不仅仅是一个正则项：它减少了模型的训练误差。权重$w$的更新规则是&lt;/strong&gt;
&lt;script type=&quot;math/tex&quot;&gt;v_{i+1} := 0.9 \bullet v_i - 0.0005 \bullet \varepsilon \bullet w_i - \varepsilon \bullet \langle \frac{\partial L} {\partial w} |_{w_i}\rangle _{D_i}&lt;/script&gt;
&lt;strong&gt;$i$是迭代索引，$v$是动量变量，$ε$是学习率，$\langle \frac{\partial L} {\partial w} |_{w_i}\rangle _{D_i}$是目标函数对$w$，在$w_i$上的第$i$批微分$D_i$的平均。&lt;/strong&gt;
We initialized the weights in each layer from a zero-mean Gaussian distribution with standard deviation 0.01. We initialized the neuron biases in the second, fourth, and fifth convolutional layers, as well as in the fully-connected hidden layers, with the constant 1. This initialization accelerates the early stages of learning by providing the ReLUs with positive inputs. We initialized the neuron biases in the remaining layers with the constant 0.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;我们使用均值为0，标准差为0.01的高斯分布对每一层的权重进行初始化。我们在第2，4，5卷积层和全连接隐层将神经元偏置初始化为常量1。这个初始化通过为ReLU提供正输入加速了学习的早期阶段。我们在剩下的层将神经元偏置初始化为0。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We used an equal learning rate for all layers, which we adjusted manually throughout training. The heuristic which we followed was to divide the learning rate by 10 when the validation error rate stopped improving with the current learning rate. The learning rate was initialized at 0.01 and reduced three times prior to termination. We trained the network for roughly 90 cycles through the training set of 1.2 million images, which took five to six days on two NVIDIA GTX 580 3GB GPUs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;我们对所有的层使用相等的学习率，这个是在整个训练过程中我们手动调整得到的。当验证误差在当前的学习率下停止提供时，我们遵循启发式的方法将学习率除以10。学习率初始化为0.01，在训练停止之前降低三次。我们在120万图像的训练数据集上训练神经网络大约90个循环，在两个NVIDIA GTX 580 3GB GPU上花费了五到六天。&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;6-results&quot;&gt;6 Results&lt;/h3&gt;
&lt;p&gt;Our results on ILSVRC-2010 are summarized in Table 1. Our network achieves top-1 and top-5 test set error rates of 37.5% and 17.0%. The best performance achieved during the ILSVRC-2010 competition was 47.1% and 28.2% with an approach that averages the predictions produced from six sparse-coding models trained on different features [2], and since then the best published results are 45.7% and 25.7% with an approach that averages the predictions of two classifiers trained on Fisher Vectors (FVs) computed from two types of densely-sampled features [24].&lt;/p&gt;

&lt;h4 id=&quot;6-结果&quot;&gt;6 结果&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;我们在&lt;code class=&quot;highlighter-rouge&quot;&gt;ILSVRC-2010上&lt;/code&gt;的结果概括为表1。我们的神经网络取得了&lt;code class=&quot;highlighter-rouge&quot;&gt;top-1 37.5%，top-5 17.0%的错误率&lt;/code&gt;。在&lt;code class=&quot;highlighter-rouge&quot;&gt;ILSVRC-2010竞赛中最佳结果是top-1 47.1%，top-5 28.2%&lt;/code&gt;，使用的方法是对6个在不同特征上训练的稀疏编码模型生成的预测进行平均，从那时起已公布的最好结果是top-1 45.7%，top-5 25.7%，使用的方法是平均在Fisher向量（FV）上训练的两个分类器的预测结果，Fisher向量是通过两种密集采样特征计算得到的[24]。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/AlexNet/3.png&quot; alt=&quot;3&quot; /&gt;
Table 1: Comparison of results on ILSVRC-2010 test set.In italics are best results achieved by others.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;表1：ILSVRC-2010测试集上的结果对比。斜体是其它人取得的最好结果。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We also entered our model in the ILSVRC-2012 competition and report our results in Table 2. Since the ILSVRC-2012 test set labels are not publicly available, we cannot report test error rates for all the models that we tried. In the remainder of this paragraph, we use validation and test error rates interchangeably because in our experience they do not differ by more than 0.1% (see Table 2). The CNN described in this paper achieves a top-5 error rate of 18.2%. Averaging the predictions of five similar CNNs gives an error rate of 16.4%. Training one CNN, with an extra sixth convolutional layer over the last pooling layer, to classify the entire ImageNet Fall 2011 release (15M images, 22K categories), and then “fine-tuning” it on ILSVRC-2012 gives an error rate of 16.6%. Averaging the predictions of two CNNs that were pre-trained on the entire Fall 2011 release with the aforementioned five CNNs gives an error rate of 15.3%. The second-best contest entry achieved an error rate of 26.2% with an approach that averages the predictions of several classifiers trained on FVs computed from different types of densely-sampled features [7].&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;我们也用我们的模型参加了ILSVRC-2012竞赛并在表2中报告了我们的结果。由于ILSVRC-2012的测试集标签不可以公开得到，我们不能报告我们尝试的所有模型的测试错误率。在这段的其余部分，我们会使用验证误差率和测试误差率互换，因为在我们的实验中它们的差别不会超过0.1%（看图2）。本文中描述的CNN取得了top-5 18.2%的错误率。五个类似的CNN预测的平均误差率为16.4%。为了对ImageNet 2011秋季发布的整个数据集（1500万图像，22000个类别）进行分类，我们在最后的池化层之后有一个额外的第6卷积层，训练了一个CNN，然后在它上面进行“fine-tuning”，在ILSVRC-2012取得了16.6%的错误率。对在ImageNet 2011秋季发布的整个数据集上预训练的两个CNN和前面提到的五个CNN的预测进行平均得到了15.3%的错误率。第二名的最好竞赛输入取得了26.2%的错误率，他的方法是对FV上训练的一些分类器的预测结果进行平均，FV在不同类型密集采样特征计算得到的。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/AlexNet/4.png&quot; alt=&quot;4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Table 2: Comparison of error rates on ILSVRC-2012 validation and test sets. In italics are best results achieved by others. Models with an asterisk were “pre-trained” to classify the entire ImageNet 2011 Fall release. See Section 6 for details.
&lt;strong&gt;表2：ILSVRC-2012验证集和测试集的误差对比。斜线部分是其它人取得的最好的结果。带星号的是“预训练的”对ImageNet 2011秋季数据集进行分类的模型。更多细节请看第六节。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Finally, we also report our error rates on the Fall 2009 version of ImageNet with 10,184 categories and 8.9 million images. On this dataset we follow the convention in the literature of using half of the images for training and half for testing. Since there is no established test set, our split necessarily differs from the splits used by previous authors, but this does not affect the results appreciably. Our top-1 and top-5 error rates on this dataset are 67.4% and 40.9%, attained by the net described above but with an additional, sixth convolutional layer over the last pooling layer. The best published results on this dataset are 78.1% and 60.9% [19].&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;最后，我们也报告了我们在ImageNet 2009秋季数据集上的误差率，ImageNet 2009秋季数据集有10,184个类，890万图像。在这个数据集上我们按照惯例用一半的图像来训练，一半的图像来测试。由于没有建立测试集，我们的数据集分割有必要不同于以前作者的数据集分割，但这对结果没有明显的影响。我们在这个数据集上的的top-1和top-5错误率是67.4%和40.9%，使用的是上面描述的在最后的池化层之后有一个额外的第6卷积层网络。这个数据集上公开可获得的最好结果是78.1%和60.9%[19]。&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;61-qualitative-evaluations&quot;&gt;6.1 Qualitative Evaluations&lt;/h4&gt;
&lt;p&gt;Figure 3 shows the convolutional kernels learned by the network’s two data-connected layers. The network has learned a variety of frequency and orientation-selective kernels, as well as various colored blobs. Notice the specialization exhibited by the two GPUs, a result of the restricted connectivity described in Section 3.5. The kernels on GPU 1 are largely color-agnostic, while the kernels on on GPU 2 are largely color-specific. This kind of specialization occurs during every run and is independent of any particular random weight initialization (modulo a renumbering of the GPUs).&lt;/p&gt;

&lt;h4 id=&quot;61-定性评估&quot;&gt;6.1 定性评估&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;图3显示了网络的两个数据连接层学习到的卷积核。网络学习到了大量的频率核、方向选择核，也学到了各种颜色点。注意两个GPU表现出的专业化，3.5小节中描述的受限连接的结果。GPU 1上的核主要是没有颜色的，而GPU 2上的核主要是针对颜色的。这种专业化在每次运行时都会发生，并且是与任何特别的随机权重初始化（以GPU的重新编号为模）无关的。&lt;/strong&gt;
&lt;img src=&quot;/images/posts/AlexNet/5.png&quot; alt=&quot;5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 3: 96 convolutional kernels of size 11×11×3 learned by the first convolutional layer on the 224×224×3 input images. The top 48 kernels were learned on GPU 1 while the bottom 48 kernels were learned on GPU 2. See Section 6.1 for details.
&lt;strong&gt;图3：第一卷积层在224×224×3的输入图像上学习到的大小为11×11×3的96个卷积核。上面的48个核是在GPU 1上学习到的而下面的48个卷积核是在GPU 2上学习到的。更多细节请看6.1小节。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In the left panel of Figure 4 we qualitatively assess what the network has learned by computing its top-5 predictions on eight test images. Notice that even off-center objects, such as the mite in the top-left, can be recognized by the net. Most of the top-5 labels appear reasonable. For example, only other types of cat are considered plausible labels for the leopard. In some cases (grille, cherry) there is genuine ambiguity about the intended focus of the photograph.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;在图4的左边部分，我们通过在8张测试图像上计算它的top-5预测定性地评估了网络学习到的东西。注意即使是不在图像中心的目标也能被网络识别，例如左上角的小虫。大多数的top-5标签似乎是合理的。例如，对于美洲豹来说，只有其它类型的猫被认为是看似合理的标签。在某些案例（格栅，樱桃）中，网络在意的图片焦点真的很含糊。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/AlexNet/6.png&quot; alt=&quot;6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 4: (Left) Eight ILSVRC-2010 test images and the five labels considered most probable by our model. The correct label is written under each image, and the probability assigned to the correct label is also shown with a red bar (if it happens to be in the top 5). (Right) Five ILSVRC-2010 test images in the first column. The remaining columns show the six training images that produce feature vectors in the last hidden layer with the smallest Euclidean distance from the feature vector for the test image.
&lt;strong&gt;图4：（左）8张ILSVRC-2010测试图像和我们的模型认为最可能的5个标签。每张图像的下面是它的正确标签，正确标签的概率用红条表示（如果正确标签在top 5中）。（右）第一列是5张ILSVRC-2010测试图像。剩下的列展示了6张训练图像，这些图像在最后的隐藏层的特征向量与测试图像的特征向量有最小的欧氏距离。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Another way to probe the network’s visual knowledge is to consider the feature activations induced by an image at the last, 4096-dimensional hidden layer. If two images produce feature activation vectors with a small Euclidean separation, we can say that the higher levels of the neural network consider them to be similar. Figure 4 shows five images from the test set and the six images from the training set that are most similar to each of them according to this measure. Notice that at the pixel level, the retrieved training images are generally not close in L2 to the query images in the first column. For example, the retrieved dogs and elephants appear in a variety of poses. We present the results for many more test images in the supplementary material.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;探索网络可视化知识的另一种方式是思考最后的4096维隐藏层在图像上得到的特征激活。如果两幅图像生成的特征激活向量之间有较小的欧式距离，我们可以认为神经网络的更高层特征认为它们是相似的。图4表明根据这个度量标准，测试集的5张图像和训练集的6张图像中的每一张都是最相似的。注意在像素级别，检索到的训练图像与第一列的查询图像在L2上通常是不接近的。例如，检索的狗和大象似乎有很多姿态。我们在补充材料中对更多的测试图像呈现了这种结果。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Computing similarity by using Euclidean distance between two 4096-dimensional, real-valued vectors is inefficient, but it could be made efficient by training an auto-encoder to compress these vectors to short binary codes. This should produce a much better image retrieval method than applying auto-encoders to the raw pixels [14], which does not make use of image labels and hence has a tendency to retrieve images with similar patterns of edges, whether or not they are semantically similar.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;通过两个4096维实值向量间的欧氏距离来计算相似性是效率低下的，但通过训练一个自动编码器将这些向量压缩为短二值编码可以使其变得高效。这应该会产生一种比将自动编码器应用到原始像素上[14]更好的图像检索方法，自动编码器应用到原始像素上的方法没有使用图像标签，因此会趋向于检索与要检索的图像具有相似边缘模式的图像，无论它们是否是语义上相似。&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;7-discussion&quot;&gt;7 Discussion&lt;/h3&gt;
&lt;p&gt;Our results show that a large, deep convolutional neural network is capable of achieving record-breaking results on a highly challenging dataset using purely supervised learning. It is notable that our network’s performance degrades if a single convolutional layer is removed. For example, removing any of the middle layers results in a loss of about 2% for the top-1 performance of the network. So the depth really is important for achieving our results.&lt;/p&gt;

&lt;h4 id=&quot;7-探讨&quot;&gt;7 探讨&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;我们的结果表明一个大型深度卷积神经网络在一个具有高度挑战性的数据集上使用纯有监督学习可以取得破纪录的结果。值得注意的是，如果移除一个卷积层，我们的网络性能会降低。例如，移除任何中间层都会引起网络损失大约2%的top-1性能。因此深度对于实现我们的结果非常重要。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To simplify our experiments, we did not use any unsupervised pre-training even though we expect that it will help, especially if we obtain enough computational power to significantly increase the size of the network without obtaining a corresponding increase in the amount of labeled data. Thus far, our results have improved as we have made our network larger and trained it longer but we still have many orders of magnitude to go in order to match the infero-temporal pathway of the human visual system. Ultimately we would like to use very large and deep convolutional nets on video sequences where the temporal structure provides very helpful information that is missing or far less obvious in static images.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;为了简化我们的实验，我们没有使用任何无监督的预训练，尽管我们希望它会有所帮助，特别是在如果我们能获得足够的计算能力来显著增加网络的大小而标注的数据量没有对应增加的情况下。到目前为止，我们的结果已经提高了，因为我们的网络更大、训练时间更长，但为了匹配人类视觉系统的下颞线（视觉专业术语）我们仍然有许多数量级要达到。最后我们想在视频序列上使用非常大的深度卷积网络，视频序列的时序结构会提供非常有帮助的信息，这些信息在静态图像上是缺失的或远不那么明显。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;References
[1] R.M.BellandY.Koren.Lessonsfromthenetflixprizechallenge.ACMSIGKDDExplorationsNewsletter, 9(2):75–79, 2007.&lt;/p&gt;

&lt;p&gt;[2] A. Berg, J. Deng, and L. Fei-Fei. Large scale visual recognition challenge 2010. www.imagenet.org/challenges. 2010.&lt;/p&gt;

&lt;p&gt;[3] L. Breiman. Random forests. Machine learning, 45(1):5–32, 2001.&lt;/p&gt;

&lt;p&gt;[4] D. Cires ̧an, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classification. Arxiv preprint arXiv:1202.2745, 2012.&lt;/p&gt;

&lt;p&gt;[5] D.C. Cires ̧an, U. Meier, J. Masci, L.M. Gambardella, and J. Schmidhuber. High-performance neural networks for visual object classification. Arxiv preprint arXiv:1102.0183, 2011.&lt;/p&gt;

&lt;p&gt;[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009.&lt;/p&gt;

&lt;p&gt;[7] J. Deng, A. Berg, S. Satheesh, H. Su, A. Khosla, and L. Fei-Fei. ILSVRC-2012, 2012. URL http://www.image-net.org/challenges/LSVRC/2012/.&lt;/p&gt;

&lt;p&gt;[8] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. Computer Vision and Image Understanding, 106(1):59–70, 2007.&lt;/p&gt;

&lt;p&gt;[9] G. Griffin, A. Holub, and P. Perona. Caltech-256 object category dataset. Technical Report 7694, California Institute of Technology, 2007. URL http://authors.library.caltech.edu/7694.&lt;/p&gt;

&lt;p&gt;[10] G.E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R.R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.&lt;/p&gt;

&lt;p&gt;[11] K. Jarrett, K. Kavukcuoglu, M. A. Ranzato, and Y. LeCun. What is the best multi-stage architecture for object recognition? In International Conference on Computer Vision, pages 2146–2153. IEEE, 2009.&lt;/p&gt;

&lt;p&gt;[12] A. Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, Department of Computer Science, University of Toronto, 2009.&lt;/p&gt;

&lt;p&gt;[13] A. Krizhevsky. Convolutional deep belief networks on cifar-10. Unpublished manuscript, 2010.&lt;/p&gt;

&lt;p&gt;[14] A. Krizhevsky and G.E. Hinton. Using very deep autoencoders for content-based image retrieval. In ESANN, 2011.&lt;/p&gt;

&lt;p&gt;[15] Y. Le Cun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, L.D. Jackel, et al. Handwritten digit recognition with a back-propagation network. In Advances in neural information processing systems, 1990.&lt;/p&gt;

&lt;p&gt;[16] Y. LeCun, F.J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance to pose and lighting. In Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on, volume 2, pages II–97. IEEE, 2004.&lt;/p&gt;

&lt;p&gt;[17] Y. LeCun, K. Kavukcuoglu, and C. Farabet. Convolutional networks and applications in vision. In Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on, pages 253–256. IEEE, 2010.&lt;/p&gt;

&lt;p&gt;[18] H. Lee, R. Grosse, R. Ranganath, and A.Y. Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 609–616. ACM, 2009.&lt;/p&gt;

&lt;p&gt;[19] T. Mensink, J. Verbeek, F. Perronnin, and G. Csurka. Metric Learning for Large Scale Image Classification: Generalizing to New Classes at Near-Zero Cost. In ECCV - European Conference on Computer Vision, Florence, Italy, October 2012.&lt;/p&gt;

&lt;p&gt;[20] V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proc. 27th International Conference on Machine Learning, 2010.&lt;/p&gt;

&lt;p&gt;[21] N. Pinto, D.D. Cox, and J.J. DiCarlo. Why is real-world visual object recognition hard? PLoS computational biology, 4(1):e27, 2008.&lt;/p&gt;

&lt;p&gt;[22] N. Pinto, D. Doukhan, J.J. DiCarlo, and D.D. Cox. A high-throughput screening approach to discovering good forms of biologically inspired visual representation. PLoS computational biology, 5(11):e1000579,2009.&lt;/p&gt;

&lt;p&gt;[23] B.C. Russell, A. Torralba, K.P. Murphy, and W.T. Freeman. Labelme: a database and web-based tool for image annotation. International journal of computer vision, 77(1):157–173, 2008.&lt;/p&gt;

&lt;p&gt;[24] J.SánchezandF.Perronnin.High-dimensionalsignaturecompressionforlarge-scaleimageclassification. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1665–1672. IEEE,2011.&lt;/p&gt;

&lt;p&gt;[25] P.Y. Simard, D. Steinkraus, and J.C. Platt. Best practices for convolutional neural networks applied to visual document analysis. In Proceedings of the Seventh International Conference on Document Analysis and Recognition, volume 2, pages 958–962, 2003.&lt;/p&gt;

&lt;p&gt;[26] S.C.Turaga,J.F.Murray,V.Jain,F.Roth,M.Helmstaedter,K.Briggman,W.Denk,andH.S.Seung.Convolutional networks can learn to generate affinity graphs for image segmentation. Neural Computation, 22(2):511–538, 2010.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;声明：译文部分摘自&lt;a href=&quot;http://noahsnail.com/2017/07/04/2017-07-04-AlexNet%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/&quot;&gt;这里&lt;/a&gt;，如有侵权请通知作者删除。&lt;/strong&gt;&lt;/p&gt;

</description>
        <pubDate>Tue, 25 Dec 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/12/AlexNet/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/12/AlexNet/</guid>
        
        <category>深度学习-视觉</category>
        
        
        <category>深度学习-视觉</category>
        
      </item>
    
  </channel>
</rss>
