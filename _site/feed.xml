<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>半甜不要腻</title>
    <description>welcome to my page</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 12 Jan 2019 22:43:24 +0800</pubDate>
    <lastBuildDate>Sat, 12 Jan 2019 22:43:24 +0800</lastBuildDate>
    <generator>Jekyll v3.8.3</generator>
    
      <item>
        <title>jekyll&amp;valine实现评论功能</title>
        <description>&lt;!--more--&gt;

&lt;p&gt;标签： 拾遗&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;要借助leancloud平台&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;注册leancloud&lt;/li&gt;
  &lt;li&gt;新建应用，名字任取&lt;/li&gt;
  &lt;li&gt;创建class，可取名comment，默认配置即可&lt;/li&gt;
  &lt;li&gt;include/comments.html添加代码&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;script &lt;/span&gt;&lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;//cdn1.lncld.net/static/js/3.0.4/av-min.js&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;script &lt;/span&gt;&lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'//unpkg.com/valine/dist/Valine.min.js'&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;id=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;comment&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;script&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;valine&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;Valine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;valine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;el&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'#comment'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;appId&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'App ID'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;//leancloud里找&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;appKey&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'App Key'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;//leancloud里找&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;notify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'/2019/01/supportcomment/'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'默认评论'&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
        <pubDate>Tue, 08 Jan 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/01/supportcomment/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/01/supportcomment/</guid>
        
        <category>拾遗</category>
        
        
        <category>拾遗</category>
        
      </item>
    
      <item>
        <title>jekyll支持latex公式</title>
        <description>&lt;!--more--&gt;

&lt;p&gt;标签： 拾遗&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;在incloud/head.html文件里加入以下代码：&lt;/p&gt;
&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;script &lt;/span&gt;&lt;span class=&quot;na&quot;&gt;type=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text/x-mathjax-config&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;MathJax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;Hub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;Config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;tex2jax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;skipTags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'script'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'noscript'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'style'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'textarea'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'pre'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;inlineMath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'$'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'$'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;script &lt;/span&gt;&lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML'&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;async&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;来个秀：
&lt;script type=&quot;math/tex&quot;&gt;\begin{multline} \lambda_\textbf{coord} \sum_{i = 0}^{S^2}     \sum_{j = 0}^{B}     \mathbb{𝟙}_{ij}^{\text{obj}}             \left[             \left(                 x_i - \hat{x}_i             \right)^2 +             \left(                 y_i - \hat{y}_i             \right)^2             \right] \\ + \lambda_\textbf{coord} \sum_{i = 0}^{S^2}     \sum_{j = 0}^{B}         \mathbb{𝟙}_{ij}^{\text{obj}}          \left[         \left(             \sqrt{w_i} - \sqrt{\hat{w}_i}         \right)^2 +         \left(             \sqrt{h_i} - \sqrt{\hat{h}_i}         \right)^2         \right] \\ + \sum_{i = 0}^{S^2}     \sum_{j = 0}^{B}         \mathbb{𝟙}_{ij}^{\text{obj}}         \left(             C_i - \hat{C}_i         \right)^2 \\ + \lambda_\textrm{noobj} \sum_{i = 0}^{S^2}     \sum_{j = 0}^{B}     \mathbb{𝟙}_{ij}^{\text{noobj}}         \left(             C_i - \hat{C}_i         \right)^2 \\ + \sum_{i = 0}^{S^2} \mathbb{𝟙}_i^{\text{obj}}     \sum_{c \in \textrm{classes}}         \left(             p_i(c) - \hat{p}_i(c)         \right)^2 \end{multline}&lt;/script&gt;&lt;/p&gt;

</description>
        <pubDate>Tue, 08 Jan 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/01/supportLatex/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/01/supportLatex/</guid>
        
        <category>拾遗</category>
        
        
        <category>拾遗</category>
        
      </item>
    
      <item>
        <title>全卷积神经网络(FCN)</title>
        <description>&lt;!--more--&gt;

&lt;p&gt;标签： CNN，深度学习，检测&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;FCN又称全卷积神经网络&lt;a href=&quot;https://link.jianshu.com/?t=https://arxiv.org/abs/1411.4038&quot;&gt;《Fully Convolutional Networks for Semantic Segmentation》&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;fcn的精髓&quot;&gt;FCN的精髓&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;1、把卷积层 -&amp;gt; 全连接层，看成是对一整张图片的卷积层运算。&lt;/li&gt;
    &lt;li&gt;2、把全连接层 -&amp;gt; 全连接层，看成是采用1*1大小的卷积核，进行卷积层运算。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;论文翻译&quot;&gt;论文翻译&lt;/h3&gt;
&lt;p&gt;先来原文的翻译（大部分来最于&lt;a href=&quot;https://www.cnblogs.com/xuanxufeng/p/6249834.html&quot;&gt;这里&lt;/a&gt;，进行些许微整理）
&lt;img src=&quot;/images/posts/FCN/title.jpg&quot; alt=&quot;title&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;摘要&quot;&gt;摘要&lt;/h4&gt;
&lt;p&gt;卷积网络在特征分层领域是非常强大的视觉模型。我们证明了经过端到端、像素到像素训练的卷积网络超过语义分割中最先进的技术。我们的核心观点是建立“全卷积”网络，输入任意尺寸，经过有效的推理和学习产生相应尺寸的输出。我们定义并指定全卷积网络的空间，解释它们在空间范围内dense prediction任务(预测每个像素所属的类别)和获取与先验模型联系的应用。我们改编当前的分类网络(AlexNet &lt;sup id=&quot;fnref:22&quot;&gt;&lt;a href=&quot;#fn:22&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; ,the VGG net &lt;sup id=&quot;fnref:34&quot;&gt;&lt;a href=&quot;#fn:34&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; , and GoogLeNet &lt;sup id=&quot;fnref:35&quot;&gt;&lt;a href=&quot;#fn:35&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; )到完全卷积网络和通过微调 &lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; 传递它们的学习表现到分割任务中。然后我们定义了一个跳跃式的架构，结合来自深、粗层的语义信息和来自浅、细层的表征信息来产生准确和精细的分割。我们的完全卷积网络成为了在PASCAL VOC最出色的分割方式（在2012年相对62.2%的平均IU提高了20%），NYUDv2，和SIFT Flow,对一个典型图像推理只需要花费不到0.2秒的时间。&lt;/p&gt;

&lt;h4 id=&quot;1-引言&quot;&gt;1. 引言&lt;/h4&gt;
&lt;p&gt;卷积网络在识别领域前进势头很猛。卷积网不仅全图式的分类上有所提高 &lt;sup id=&quot;fnref:22:1&quot;&gt;&lt;a href=&quot;#fn:22&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:34:1&quot;&gt;&lt;a href=&quot;#fn:34&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:35:1&quot;&gt;&lt;a href=&quot;#fn:35&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; ,也在结构化输出的局部任务上取得了进步。包括在目标检测边界框 &lt;sup id=&quot;fnref:32&quot;&gt;&lt;a href=&quot;#fn:32&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:12&quot;&gt;&lt;a href=&quot;#fn:12&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:19&quot;&gt;&lt;a href=&quot;#fn:19&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; 、部分和关键点预测 &lt;sup id=&quot;fnref:42&quot;&gt;&lt;a href=&quot;#fn:42&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:26&quot;&gt;&lt;a href=&quot;#fn:26&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt; 和局部通信 &lt;sup id=&quot;fnref:26:1&quot;&gt;&lt;a href=&quot;#fn:26&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:10&quot;&gt;&lt;a href=&quot;#fn:10&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt; 的进步。&lt;/p&gt;

&lt;p&gt;在从粗糙到精细推理的进展中下一步自然是对每一个像素进行预测。早前的方法已经将卷积网络用于语义分割 &lt;sup id=&quot;fnref:30&quot;&gt;&lt;a href=&quot;#fn:30&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:9&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:31&quot;&gt;&lt;a href=&quot;#fn:31&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:17&quot;&gt;&lt;a href=&quot;#fn:17&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:15&quot;&gt;&lt;a href=&quot;#fn:15&quot; class=&quot;footnote&quot;&gt;16&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:11&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt; ,其中每个像素被标记为其封闭对象或区域的类别，但是这些工作还是有缺点。
&lt;img src=&quot;/images/posts/FCN/1.1.png&quot; alt=&quot;1.1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我们证明了经过&lt;strong&gt;端到端&lt;/strong&gt;、像素到像素训练的的卷积网络超过语义分割中没有further machinery的技术。我们认为，这是第一次训练端到端(1)的FCN在像素级别的预测，而且来自监督式预处理(2)。全卷积在现有的网络基础上从任意尺寸的输入预测密集输出。学习和推理能在全图通过密集的前馈计算和反向传播一次执行。网内上采样层能在像素级别预测和通过下采样池化学习。&lt;/p&gt;

&lt;p&gt;这种方法非常有效，无论是渐进地还是完全地，消除了在其他方法中的并发问题。Patchwise训练是常见的 &lt;sup id=&quot;fnref:30:1&quot;&gt;&lt;a href=&quot;#fn:30&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:3:1&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:9:1&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:31:1&quot;&gt;&lt;a href=&quot;#fn:31&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:11:1&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt;，但是缺少了全卷积训练的有效性。我们的方法不是利用预处理或者后期处理解决并发问题，包括超像素 &lt;sup id=&quot;fnref:9:2&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:17:1&quot;&gt;&lt;a href=&quot;#fn:17&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt; ，proposals &lt;sup id=&quot;fnref:17:2&quot;&gt;&lt;a href=&quot;#fn:17&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:15:1&quot;&gt;&lt;a href=&quot;#fn:15&quot; class=&quot;footnote&quot;&gt;16&lt;/a&gt;&lt;/sup&gt;，或者对通过随机域事后细化或者局部分类 &lt;sup id=&quot;fnref:9:3&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:17:3&quot;&gt;&lt;a href=&quot;#fn:17&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt; 。我们的模型通过重新解释分类网到全卷积网络和微调它们的学习表现将最近在分类上的成功 &lt;sup id=&quot;fnref:22:2&quot;&gt;&lt;a href=&quot;#fn:22&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:34:2&quot;&gt;&lt;a href=&quot;#fn:34&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:35:2&quot;&gt;&lt;a href=&quot;#fn:35&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; 移植到dense prediction。与此相反，先前的工作应用的是小规模、没有超像素预处理的卷积网。&lt;/p&gt;

&lt;p&gt;语义分割面临在语义和位置的内在张力问题：全局信息解决的“是什么”，而局部信息解决的是“在哪里”。深层特征通过非线性的局部到全局金字塔编码了位置和语义信息。我们在4.2节(见图3）定义了一种利用集合了深、粗层的语义信息和浅、细层的表征信息的特征谱的跨层架构。&lt;/p&gt;

&lt;p&gt;在下一节，我们回顾深层分类网、FCNs和最近一些利用卷积网解决语义分割的相关工作。接下来的章节将解释FCN设计和密集预测权衡，介绍我们的网内上采样和多层结合架构，描述我们的实验框架。最后，我们展示了最先进技术在PASCAL VOC 2011-2, NYUDv2, 和SIFT Flow上的实验结果。&lt;/p&gt;

&lt;h4 id=&quot;2-相关工作&quot;&gt;2. 相关工作&lt;/h4&gt;

&lt;p&gt;我们的方法是基于最近深层网络在图像分类上的成功 &lt;sup id=&quot;fnref:22:3&quot;&gt;&lt;a href=&quot;#fn:22&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:34:3&quot;&gt;&lt;a href=&quot;#fn:34&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:35:3&quot;&gt;&lt;a href=&quot;#fn:35&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; 和转移学习。转移第一次被证明在各种视觉识别任务 &lt;sup id=&quot;fnref:5:1&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:41&quot;&gt;&lt;a href=&quot;#fn:41&quot; class=&quot;footnote&quot;&gt;18&lt;/a&gt;&lt;/sup&gt; ，然后是检测，不仅在实例还有融合proposal-classification模型的语义分割 &lt;sup id=&quot;fnref:12:1&quot;&gt;&lt;a href=&quot;#fn:12&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:17:4&quot;&gt;&lt;a href=&quot;#fn:17&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:15:2&quot;&gt;&lt;a href=&quot;#fn:15&quot; class=&quot;footnote&quot;&gt;16&lt;/a&gt;&lt;/sup&gt; 。我们现在重新构建和微调直接的、dense prediction语义分割的分类网。在这个框架里我们绘制FCNs的空间并将过去的或是最近的先验模型置于其中。&lt;/p&gt;

&lt;p&gt;全卷积网络据我们所知，第一次将卷积网扩展到任意尺寸的输入的是Matan等人 &lt;sup id=&quot;fnref:28&quot;&gt;&lt;a href=&quot;#fn:28&quot; class=&quot;footnote&quot;&gt;19&lt;/a&gt;&lt;/sup&gt; ,它将经典的LeNet &lt;sup id=&quot;fnref:23&quot;&gt;&lt;a href=&quot;#fn:23&quot; class=&quot;footnote&quot;&gt;20&lt;/a&gt;&lt;/sup&gt; 扩展到识别字符串的位数。因为他们的网络结构限制在一维的输入串，Matan等人利用译码器译码获得输出。Wolf和Platt &lt;sup id=&quot;fnref:40&quot;&gt;&lt;a href=&quot;#fn:40&quot; class=&quot;footnote&quot;&gt;21&lt;/a&gt;&lt;/sup&gt; 将卷积网输出扩展到来检测邮政地址块的四角得分的二维图。这些先前工作做的是推理和用于检测的全卷积式学习。Ning等人 &lt;sup id=&quot;fnref:30:2&quot;&gt;&lt;a href=&quot;#fn:30&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt; 定义了一种卷积网络用于秀丽线虫组织的粗糙的、多分类分割，基于全卷积推理。&lt;/p&gt;

&lt;p&gt;全卷积计算也被用在现在的一些多层次的网络结构中。Sermanet等人的滑动窗口检测 &lt;sup id=&quot;fnref:32:1&quot;&gt;&lt;a href=&quot;#fn:32&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; ，Pinherio 和Collobert的语义分割 &lt;sup id=&quot;fnref:31:2&quot;&gt;&lt;a href=&quot;#fn:31&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt; ，Eigen等人的图像修复 &lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;22&lt;/a&gt;&lt;/sup&gt; 都做了全卷积式推理。全卷积训练很少，但是被Tompson等人 &lt;sup id=&quot;fnref:38&quot;&gt;&lt;a href=&quot;#fn:38&quot; class=&quot;footnote&quot;&gt;23&lt;/a&gt;&lt;/sup&gt; 用来学习一种端到端的局部检测和姿态估计的空间模型非常有效，尽管他们没有解释或者分析这种方法。&lt;/p&gt;

&lt;p&gt;此外，He等人&lt;sup id=&quot;fnref:19:1&quot;&gt;&lt;a href=&quot;#fn:19&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;在特征提取时丢弃了分类网的无卷积部分。他们结合proposals和空间金字塔池来产生一个局部的、固定长度的特征用于分类。尽管快速且有效，但是这种混合模型不能进行端到端的学习。&lt;/p&gt;

&lt;p&gt;基于卷积网的dense prediction近期的一些工作已经将卷积网应用于dense prediction问题，包括Ning等人的语义分割 &lt;sup id=&quot;fnref:30:3&quot;&gt;&lt;a href=&quot;#fn:30&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt; ,Farabet等人 &lt;sup id=&quot;fnref:9:4&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt; 以及Pinheiro和Collobert &lt;sup id=&quot;fnref:31:3&quot;&gt;&lt;a href=&quot;#fn:31&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt; ；Ciresan等人的电子显微镜边界预测 &lt;sup id=&quot;fnref:3:2&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt; 以及Ganin和Lempitsky &lt;sup id=&quot;fnref:11:2&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt; 的通过混合卷积网和最邻近模型的处理自然场景图像;还有Eigen等人 &lt;sup id=&quot;fnref:6:1&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;22&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:7&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;24&lt;/a&gt;&lt;/sup&gt; 的图像修复和深度估计。这些方法的相同点包括如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;限制容量和接收域的小模型&lt;/li&gt;
  &lt;li&gt;patchwise训练 &lt;sup id=&quot;fnref:30:4&quot;&gt;&lt;a href=&quot;#fn:30&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:3:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:9:5&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:31:4&quot;&gt;&lt;a href=&quot;#fn:31&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:11:3&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;超像素投影的预处理，随机场正则化、滤波或局部分类 &lt;sup id=&quot;fnref:9:6&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:3:4&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:11:4&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;输入移位和dense输出的隔行交错输出 &lt;sup id=&quot;fnref:32:2&quot;&gt;&lt;a href=&quot;#fn:32&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:31:5&quot;&gt;&lt;a href=&quot;#fn:31&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:11:5&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;多尺度金字塔处理 &lt;sup id=&quot;fnref:9:7&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:31:6&quot;&gt;&lt;a href=&quot;#fn:31&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:11:6&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;饱和双曲线正切非线性 &lt;sup id=&quot;fnref:9:8&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:6:2&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;22&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:31:7&quot;&gt;&lt;a href=&quot;#fn:31&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;集成 &lt;sup id=&quot;fnref:3:5&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:11:7&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然而我们的方法确实没有这种机制。但是我们研究了patchwise训练 （3.4节）和从FCNs的角度出发的“shift-and-stitch”dense输出（3.2节）。我们也讨论了网内上采样（3.3节），其中Eigen等人&lt;sup id=&quot;fnref:7:1&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;24&lt;/a&gt;&lt;/sup&gt;的全连接预测是一个特例。&lt;/p&gt;

&lt;p&gt;和这些现有的方法不同的是，我们改编和扩展了深度分类架构，使用图像分类作为监督预处理，和从全部图像的输入和ground truths(用于有监督训练的训练集的分类准确性)通过全卷积微调进行简单且高效的学习。&lt;/p&gt;

&lt;p&gt;Hariharan等人 &lt;sup id=&quot;fnref:17:5&quot;&gt;&lt;a href=&quot;#fn:17&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt; 和Gupta等人 &lt;sup id=&quot;fnref:15:3&quot;&gt;&lt;a href=&quot;#fn:15&quot; class=&quot;footnote&quot;&gt;16&lt;/a&gt;&lt;/sup&gt; 也改编深度分类网到语义分割，但是也在混合proposal-classifier模型中这么做了。这些方法通过采样边界框和region proposal进行微调了R-CNN系统 &lt;sup id=&quot;fnref:12:2&quot;&gt;&lt;a href=&quot;#fn:12&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; ,用于检测、语义分割和实例分割。这两种办法都不能进行端到端的学习。他们分别在PASCAL VOC和NYUDv2实现了最好的分割效果，所以在第5节中我们直接将我们的独立的、端到端的FCN和他们的语义分割结果进行比较。&lt;/p&gt;

&lt;p&gt;我们通过跨层和融合特征来定义一种非线性的局部到整体的表述用来协调端到端。在现今的工作中Hariharan等人 &lt;sup id=&quot;fnref:18&quot;&gt;&lt;a href=&quot;#fn:18&quot; class=&quot;footnote&quot;&gt;25&lt;/a&gt;&lt;/sup&gt; 也在语义分割的混合模型中使用了多层。&lt;/p&gt;

&lt;h4 id=&quot;3-全卷积网络&quot;&gt;3. 全卷积网络&lt;/h4&gt;

&lt;p&gt;卷积网的每层数据是一个h&lt;em&gt;w&lt;/em&gt;d的三维数组，其中h和w是空间维度,d是特征或通道维数。第一层是像素尺寸为h*w、颜色通道数为d的图像。高层中的locations和图像中它们连通的locations相对应，被称为接收域。&lt;/p&gt;

&lt;p&gt;卷积网是以平移不变形作为基础的。其基本组成部分(卷积，池化和激励函数)作用在局部输入域，只依赖相对空间坐标。在特定层记X_ij为在坐标(i,j)的数据向量，在following layer有Y_ij，Y_ij的计算公式如下:
&lt;img src=&quot;/images/posts/FCN/3.1.png&quot; alt=&quot;3.1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其中k为卷积核尺寸，s是步长或下采样因素，f_ks决定了层的类型：一个卷积的矩阵乘或者是平均池化，用于最大池的最大空间值或者是一个激励函数的一个非线性elementwise，亦或是层的其他种类等等。当卷积核尺寸和步长遵从转换规则，这个函数形式被表述为如下形式：
&lt;img src=&quot;/images/posts/FCN/3.2.png&quot; alt=&quot;3.2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;当一个普通深度的网络计算一个普通的非线性函数，一个网络只有这种形式的层计算非线性滤波，我们称之为深度滤波或全卷积网络。FCN理应可以计算任意尺寸的输入并产生相应（或许重采样)空间维度的输出。一个实值损失函数有FCN定义了task。如果损失函数是一个最后一层的空间维度总和,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/FCN/3.3.png&quot; alt=&quot;3.3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;它的梯度将是它的每层空间组成梯度总和。所以在全部图像上的基于l的随机梯度下降计算将和基于l’的梯度下降结果一样，将最后一层的所有接收域作为minibatch（分批处理）。在这些接收域重叠很大的情况下，前反馈计算和反向传播计算整图的叠层都比独立的patch-by-patch有效的多。&lt;/p&gt;

&lt;p&gt;我们接下来将解释怎么将分类网络转换到能产生粗输出图的全卷积网络。对于像素级预测，我们需要连接这些粗略的输出结果到像素。3.2节描述了一种技巧，快速扫描&lt;sup id=&quot;fnref:13&quot;&gt;&lt;a href=&quot;#fn:13&quot; class=&quot;footnote&quot;&gt;26&lt;/a&gt;&lt;/sup&gt;因此被引入。我们通过将它解释为一个等价网络修正而获得了关于这个技巧的一些领悟。作为一个高效的替换，我们引入了去卷积层用于上采样见3.3节。在3.4节，我们考虑通过patchwise取样训练，便在4.3节证明我们的全图式训练更快且同样有效。&lt;/p&gt;

&lt;h5 id=&quot;31-改编分类用于dense-prediction&quot;&gt;3.1 改编分类用于dense prediction&lt;/h5&gt;

&lt;p&gt;典型的识别网络，包括LeNet &lt;sup id=&quot;fnref:23:1&quot;&gt;&lt;a href=&quot;#fn:23&quot; class=&quot;footnote&quot;&gt;20&lt;/a&gt;&lt;/sup&gt; , AlexNet &lt;sup id=&quot;fnref:22:4&quot;&gt;&lt;a href=&quot;#fn:22&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; , 和一些后继者 &lt;sup id=&quot;fnref:34:4&quot;&gt;&lt;a href=&quot;#fn:34&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, &lt;sup id=&quot;fnref:35:4&quot;&gt;&lt;a href=&quot;#fn:35&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; ，表面上采用的是固定尺寸的输入产生了非空间的输出。这些网络的全连接层有确定的位数并丢弃空间坐标。然而，这些全连接层也被看做是覆盖全部输入域的核卷积。需要将它们加入到可以采用任何尺寸输入并输出分类图的全卷积网络中。这种转换如图2所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/FCN/3.1.1.png&quot; alt=&quot;3.1.1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;此外，当作为结果的图在特殊的输入patches上等同于原始网络的估计，计算是高度摊销的在那些patches的重叠域上。例如，当AlexNet花费了1.2ms（在标准的GPU上)推算一个227&lt;em&gt;227图像的分类得分，全卷积网络花费22ms从一张500&lt;/em&gt;500的图像上产生一个10*10的输出网格，比朴素法快了5倍多。&lt;/p&gt;

&lt;p&gt;这些卷积化模式的空间输出图可以作为一个很自然的选择对于dense问题，比如语义分割。每个输出单元ground truth可用，正推法和逆推法都是直截了当的，都利用了卷积的固有的计算效率(和可极大优化性)。对于AlexNet例子相应的逆推法的时间为单张图像时间2.4ms，全卷积的10*10输出图为37ms，结果是相对于顺推法速度加快了。&lt;/p&gt;

&lt;p&gt;当我们将分类网络重新解释为任意输出尺寸的全卷积域输出图，输出维数也通过下采样显著的减少了。分类网络下采样使filter保持小规模同时计算要求合理。这使全卷积式网络的输出结果变得粗糙，通过输入尺寸因为一个和输出单元的接收域的像素步长等同的因素来降低它。&lt;/p&gt;

&lt;h5 id=&quot;32-shift-and-stitch是滤波稀疏&quot;&gt;3.2 Shift-and stitch是滤波稀疏&lt;/h5&gt;

&lt;p&gt;dense prediction能从粗糙输出中通过从输入的平移版本中将输出拼接起来获得。如果输出是因为一个因子f降低采样，平移输入的x像素到左边，y像素到下面，一旦对于每个(x,y)满足0&amp;lt;=x,y&amp;lt;=f.处理f^2个输入，并将输出交错以便预测和它们接收域的中心像素一致。&lt;/p&gt;

&lt;p&gt;尽管单纯地执行这种转换增加了f^2的这个因素的代价，有一个非常有名的技巧用来高效的产生完全相同的结果 &lt;sup id=&quot;fnref:13:1&quot;&gt;&lt;a href=&quot;#fn:13&quot; class=&quot;footnote&quot;&gt;26&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:32:3&quot;&gt;&lt;a href=&quot;#fn:32&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; ，这个在小波领域被称为多孔算法 &lt;sup id=&quot;fnref:27&quot;&gt;&lt;a href=&quot;#fn:27&quot; class=&quot;footnote&quot;&gt;27&lt;/a&gt;&lt;/sup&gt; 。考虑一个层（卷积或者池化）中的输入步长s,和后面的滤波权重为f_ij的卷积层（忽略不相关的特征维数）。设置更低层的输入步长到l上采样它的输出影响因子为s。然而，将原始的滤波和上采样的输出卷积并没有产生和shift-and-stitch相同的结果，因为原始的滤波只看得到（已经上采样）输入的简化的部分。为了重现这种技巧，通过扩大来稀疏滤波，如下:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/FCN/3.2.1.png&quot; alt=&quot;3.2.1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如果s能除以i和j，除非i和j都是0。重现该技巧的全网输出需要重复一层一层放大这个filter知道所有的下采样被移除。（在练习中，处理上采样输入的下采样版本可能会更高效。）&lt;/p&gt;

&lt;p&gt;在网内减少二次采样是一种折衷的做法：filter能看到更细节的信息，但是接受域更小而且需要花费很长时间计算。Shift-and -stitch技巧是另外一种折衷做法：输出更加密集且没有减小filter的接受域范围，但是相对于原始的设计filter不能感受更精细的信息。&lt;/p&gt;

&lt;p&gt;尽管我们已经利用这个技巧做了初步的实验，但是我们没有在我们的模型中使用它。正如在下一节中描述的，我们发现从上采样中学习更有效和高效，特别是接下来要描述的结合了跨层融合。&lt;/p&gt;

&lt;h5 id=&quot;33-上采样是向后向卷积&quot;&gt;3.3 上采样是向后向卷积&lt;/h5&gt;
&lt;p&gt;另一种连接粗糙输出到dense像素的方法就是插值法。比如，简单的双线性插值计算每个输出y_ij来自只依赖输入和输出单元的相对位置的线性图最近的四个输入。&lt;/p&gt;

&lt;p&gt;从某种意义上，伴随因子f的上采样是对步长为1/f的分数式输入的卷积操作。只要f是整数，一种自然的方法进行上采样就是向后卷积（有时称为去卷积）伴随输出步长为f。这样的操作实现是不重要的，因为它只是简单的调换了卷积的顺推法和逆推法。所以上采样在网内通过计算像素级别的损失的反向传播用于端到端的学习。&lt;/p&gt;

&lt;p&gt;需要注意的是去卷积滤波在这种层面上不需要被固定不变（比如双线性上采样）但是可以被学习。一堆反褶积层和激励函数甚至能学习一种非线性上采样。在我们的实验中，我们发现在网内的上采样对于学习dense prediction是快速且有效的。我们最好的分割架构利用了这些层来学习上采样用以微调预测，见4.2节。&lt;/p&gt;

&lt;h5 id=&quot;34-patchwise训练是一种损失采样&quot;&gt;3.4 patchwise训练是一种损失采样&lt;/h5&gt;
&lt;p&gt;在随机优化中，梯度计算是由训练分布支配的。patchwise 训练和全卷积训练能被用来产生任意分布，尽管他们相对的计算效率依赖于重叠域和minibatch的大小。在每一个由所有的单元接受域组成的批次在图像的损失之下（或图像的集合）整张图像的全卷积训练等同于patchwise训练。当这种方式比patches的均匀取样更加高效的同时，它减少了可能的批次数量。然而在一张图片中随机选择patches可能更容易被重新找到。限制基于它的空间位置随机取样子集产生的损失（或者可以说应用输入和输出之间的DropConnect mask &lt;sup id=&quot;fnref:39&quot;&gt;&lt;a href=&quot;#fn:39&quot; class=&quot;footnote&quot;&gt;28&lt;/a&gt;&lt;/sup&gt; ）排除来自梯度计算的patches。&lt;/p&gt;

&lt;p&gt;如果保存下来的patches依然有重要的重叠，全卷积计算依然将加速训练。如果梯度在多重逆推法中被积累，batches能包含几张图的patches。patcheswise训练中的采样能纠正分类失调 &lt;sup id=&quot;fnref:30:5&quot;&gt;&lt;a href=&quot;#fn:30&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:9:9&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:3:6&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt; 和减轻密集空间相关性的影响&lt;sup id=&quot;fnref:31:8&quot;&gt;&lt;a href=&quot;#fn:31&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:17:6&quot;&gt;&lt;a href=&quot;#fn:17&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt;。在全卷积训练中，分类平衡也能通过给损失赋权重实现，对损失采样能被用来标识空间相关。&lt;/p&gt;

&lt;p&gt;我们研究了4.3节中的伴有采样的训练，没有发现对于dense prediction它有更快或是更好的收敛效果。全图式训练是有效且高效的。&lt;/p&gt;

&lt;h4 id=&quot;4-分割架构&quot;&gt;4 分割架构&lt;/h4&gt;

&lt;p&gt;我们将ILSVRC分类应用到FCNs增大它们用于dense prediction结合网内上采样和像素级损失。我们通过微调为分割进行训练。接下来我们增加了跨层来融合粗的、语义的和局部的表征信息。这种跨层式架构能学习端到端来改善输出的语义和空间预测。&lt;/p&gt;

&lt;p&gt;为此，我们训练和在PASCAL VOC 2011分割挑战赛&lt;sup id=&quot;fnref:8&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot;&gt;29&lt;/a&gt;&lt;/sup&gt;中验证。我们训练逐像素的多项式逻辑损失和验证标准度量的在集合中平均像素交集还有基于所有分类上的平均接收，包括背景。这个训练忽略了那些在groud truth中被遮盖的像素（模糊不清或者很难辨认）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/FCN/4.1.png&quot; alt=&quot;4.1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;注：不是每个可能的patch被包含在这种方法中，因为最后一层单位的的接收域依赖一个固定的、步长大的网格。然而，对该图像进行向左或向下随机平移接近该步长个单位，从所有可能的patches 中随机选取或许可以修复这个问题。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/FCN/4.2.png&quot; alt=&quot;4.2&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;41-从分类到dense-fcn&quot;&gt;4.1 从分类到dense FCN&lt;/h5&gt;

&lt;p&gt;我们在第3节中以卷积证明分类架构的。我们认为拿下了ILSVRC12的AlexNet3架构 &lt;sup id=&quot;fnref:22:5&quot;&gt;&lt;a href=&quot;#fn:22&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; 和VGG nets &lt;sup id=&quot;fnref:34:5&quot;&gt;&lt;a href=&quot;#fn:34&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; 、GoogLeNet4 &lt;sup id=&quot;fnref:35:5&quot;&gt;&lt;a href=&quot;#fn:35&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; 一样在ILSVRC14上表现的格外好。我们选择VGG 16层的网络5，发现它和19层的网络在这个任务（分类）上相当。对于GoogLeNet,我们仅仅使用的最后的损失层，通过丢弃了最后的平均池化层提高了表现能力。我们通过丢弃最后的分类切去每层网络头，然后将全连接层转化成卷积层。我们附加了一个1*1的、通道维数为21的卷积来预测每个PASCAL分类（包括背景）的得分在每个粗糙的输出位置，后面紧跟一个去卷积层用来双线性上采样粗糙输出到像素密集输出如3.3.节中描述。表1将初步验证结果和每层的基础特性比较。我们发现最好的结果在以一个固定的学习速率得到（最少175个epochs)。&lt;/p&gt;

&lt;p&gt;从分类到分割的微调对每层网络有一个合理的预测。甚至最坏的模型也能达到大约75%的良好表现。内设分割的VGG网络（FCN-VGG16）已经在val上平均IU 达到了56.0取得了最好的成绩，相比于52.6 &lt;sup id=&quot;fnref:17:7&quot;&gt;&lt;a href=&quot;#fn:17&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt; 。在额外数据上的训练将FCN-VGG16提高到59.4，将FCN-AlexNet提高到48.0。尽管相同的分类准确率，我们的用GoogLeNet并不能和VGG16的分割结果相比较。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/FCN/4.1.1.png&quot; alt=&quot;4.1.1&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;42-结合是什么和在哪里&quot;&gt;4.2 结合“是什么”和“在哪里”&lt;/h5&gt;

&lt;p&gt;我们定义了一个新的全卷积网用于结合了特征层级的分割并提高了输出的空间精度，见图3。当全卷积分类能被微调用于分割如4.1节所示，甚至在标准度量上得分更高，它们的输出不是很粗糙（见图4）。最后预测层的32像素步长限制了上采样输入的细节的尺寸。&lt;/p&gt;

&lt;p&gt;我们提出增加结合了最后预测层和有更细小步长的更低层的跨层信息&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;30&lt;/a&gt;&lt;/sup&gt;，将一个线划拓扑结构转变成DAG(有向无环图)，并且边界将从更底层向前跳跃到更高（图3）。因为它们只能获取更少的像素点，更精细的尺寸预测应该需要更少的层，所以从更浅的网中将它们输出是有道理的。结合了精细层和粗糙层让模型能做出遵从全局结构的局部预测。与Koenderick 和an Doorn &lt;sup id=&quot;fnref:21&quot;&gt;&lt;a href=&quot;#fn:21&quot; class=&quot;footnote&quot;&gt;31&lt;/a&gt;&lt;/sup&gt;的jet类似，我们把这种非线性特征层称之为deep jet。&lt;/p&gt;

&lt;p&gt;我们首先将输出步长分为一半，通过一个16像素步长层预测。我们增加了一个1*1的卷积层在pool4的顶部来产生附加的类别预测。我们将输出和预测融合在conv7（fc7的卷积化）的顶部以步长32计算，通过增加一个2×的上采样层和预测求和（见图3）。我们初始化这个2×上采样到双线性插值，但是允许参数能被学习，如3.3节所描述、最后，步长为16的预测被上采样回图像，我们把这种网结构称为FCN-16s。FCN-16s用来学习端到端，能被最后的参数初始化。这种新的、在pool4上生效的参数是初始化为0 的，所以这种网结构是以未变性的预测开始的。这种学习速率是以100倍的下降的。&lt;/p&gt;

&lt;p&gt;学习这种跨层网络能在3.0平均IU的有效集合上提高到62.4。图4展示了在精细结构输出上的提高。我们将这种融合学习和仅仅从pool4层上学习进行比较，结果表现糟糕，而且仅仅降低了学习速率而没有增加跨层，导致了没有提高输出质量的没有显著提高表现。&lt;/p&gt;

&lt;p&gt;我们继续融合pool3和一个融合了pool4和conv7的2×上采样预测，建立了FCN-8s的网络结构。在平均IU上我们获得了一个较小的附加提升到62.7，然后发现了一个在平滑度和输出细节上的轻微提高。这时我们的融合提高已经得到了一个衰减回馈，既在强调了大规模正确的IU度量的层面上，也在提升显著度上得到反映，如图4所示，所以即使是更低层我们也不需要继续融合。&lt;/p&gt;

&lt;p&gt;其他方式精炼化减少池层的步长是最直接的一种得到精细预测的方法。然而这么做对我们的基于VGG16的网络带来问题。设置pool5的步长到1，要求我们的卷积fc6核大小为14*14来维持它的接收域大小。另外它们的计算代价，通过如此大的滤波器学习非常困难。我们尝试用更小的滤波器重建pool5之上的层，但是并没有得到有可比性的结果；一个可能的解释是ILSVRC在更上层的初始化时非常重要的。&lt;/p&gt;

&lt;p&gt;另一种获得精细预测的方法就是利用3.2节中描述的shift-and-stitch技巧。在有限的实验中，我们发现从这种方法的提升速率比融合层的方法花费的代价更高。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/FCN/4.2.1.png&quot; alt=&quot;4.2.1&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;43-实验框架&quot;&gt;4.3 实验框架&lt;/h5&gt;

&lt;p&gt;优化我们利用momentum训练了GSD。我们利用了一个minibatch大小的20张图片，然后固定学习速率为10-3,10-4，和5-5用于FCN-AlexNet, FCN-VGG16,和FCN-GoogLeNet，通过各自的线性搜索选择。我们利用了0.9的momentum,权值衰减在5-4或是2-4，而且对于偏差的学习速率加倍了，尽管我们发现训练对单独的学习速率敏感。我们零初始化类的得分层，随机初始化既不能产生更好的表现也没有更快的收敛。Dropout被包含在用于原始分类的网络中。&lt;/p&gt;

&lt;p&gt;微调我们通过反向传播微调整个网络的所有层。经过表2的比较，微调单独的输出分类表现只有全微调的70%。考虑到学习基础分类网络所需的时间，从scratch中训练不是可行的。（注意VGG网络的训练是阶段性的，当我们从全16层初始化后）。对于粗糙的FCN-32s，在单GPU上，微调要花费三天的时间，而且大约每隔一天就要更新到FCN-16s和FCN-8s版本。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/FCN/4.3.1.png&quot; alt=&quot;4.3.1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;更多的训练数据PASCAL VOC 2011分割训练设置1112张图片的标签。Hariharan等人 &lt;sup id=&quot;fnref:16&quot;&gt;&lt;a href=&quot;#fn:16&quot; class=&quot;footnote&quot;&gt;32&lt;/a&gt;&lt;/sup&gt; 为一个更大的8498的PASCAL训练图片集合收集标签，被用于训练先前的先进系统,SDS &lt;sup id=&quot;fnref:17:8&quot;&gt;&lt;a href=&quot;#fn:17&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt; 。训练数据将FCV-VGG16得分提高了3.4个百分点到59.4。&lt;/p&gt;

&lt;p&gt;patch取样正如3.4节中解释的，我们的全图有效地训练每张图片batches到常规的、大的、重叠的patches网格。相反的，先前工作随机样本patches在一整个数据集 &lt;sup id=&quot;fnref:30:6&quot;&gt;&lt;a href=&quot;#fn:30&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:3:7&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:9:10&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:31:9&quot;&gt;&lt;a href=&quot;#fn:31&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:11:8&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt; ，可能导致更高的方差batches，可能加速收敛 &lt;sup id=&quot;fnref:24&quot;&gt;&lt;a href=&quot;#fn:24&quot; class=&quot;footnote&quot;&gt;33&lt;/a&gt;&lt;/sup&gt; 。我们通过空间采样之前方式描述的损失研究这种折中，以1-p的概率做出独立选择来忽略每个最后层单元。为了避免改变有效的批次尺寸，我们同时以因子1/p增加每批次图像的数量。注意的是因为卷积的效率，在足够大的p值下，这种拒绝采样的形式依旧比patchwose训练要快（比如，根据3.1节的数量，最起码p&amp;gt;0.2）图5展示了这种收敛的采样的效果。我们发现采样在收敛速率上没有很显著的效果相对于全图式训练，但是由于每个每个批次都需要大量的图像，很明显的需要花费更多的时间。&lt;/p&gt;

&lt;p&gt;分类平衡全卷积训练能通过按权重或对损失采样平衡类别。尽管我们的标签有轻微的不平衡（大约3/4是背景），我们发现类别平衡不是必要的。dense prediction分数是通过网内的去卷积层上采样到输出维度。最后层去卷积滤波被固定为双线性插值，当中间采样层是被初始化为双线性上采样，然后学习。扩大我们尝试通过随机反射扩大训练数据，”jettering”图像通过将它们在每个方向上转化成32像素（最粗糙预测的尺寸）。这并没有明显的改善。实现所有的模型都是在单NVIDIA Tesla K40c上用Caffe&lt;sup id=&quot;fnref:20&quot;&gt;&lt;a href=&quot;#fn:20&quot; class=&quot;footnote&quot;&gt;34&lt;/a&gt;&lt;/sup&gt;训练和学习。&lt;/p&gt;

&lt;h4 id=&quot;5-结果&quot;&gt;5 结果&lt;/h4&gt;

&lt;p&gt;我们训练FCN在语义分割和场景解析，研究了PASCAL VOC, NYUDv2和 SIFT Flow。尽管这些任务在以前主要是用在物体和区域上，我们都一律将它们视为像素预测。我们在这些数据集中都进行测试用来评估我们的FCN跨层式架构，然后对于NYUDv2将它扩展成一个多模型的输出，对于SIFT Flow则扩展成多任务的语义和集合标签。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;度量&lt;/strong&gt; 我们从常见的语义分割和场景解析评估中提出四种度量，它们在像素准确率和在联合的区域交叉上是不同的。令n_ij为类别i的被预测为类别j的像素数量，有n_ij个不同的类别，令
&lt;img src=&quot;/images/posts/FCN/5.1.png&quot; alt=&quot;5.1&quot; /&gt;
为类别i的像素总的数量。我们将计算：
&lt;img src=&quot;/images/posts/FCN/5.2.png&quot; alt=&quot;5.2&quot; /&gt;
&lt;strong&gt;PASCAL VOC&lt;/strong&gt; 表3给出了我们的FCN-8s的在PASCAL VOC2011和2012测试集上的表现，然后将它和之前的先进方法SDS[17]和著名的R-CNN[12]进行比较。我们在平均IU上取得了最好的结果相对提升了20%。推理时间被降低了114×（只有卷积网，没有proposals和微调)或者286×（全部都有）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/FCN/5.3.png&quot; alt=&quot;5.3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;NVUDv2&lt;/strong&gt; &lt;sup id=&quot;fnref:33&quot;&gt;&lt;a href=&quot;#fn:33&quot; class=&quot;footnote&quot;&gt;35&lt;/a&gt;&lt;/sup&gt;是一种通过利用Microsoft Kinect收集到的RGB-D数据集，含有已经被合并进Gupt等人[14]的40类别的语义分割任务的pixelwise标签。我们报告结果基于标准分离的795张图片和654张测试图片。（注意：所有的模型选择将展示在PASCAL 2011 val上)。表4给出了我们模型在一些变化上的表现。首先我们在RGB图片上训练我们的未经修改的粗糙模型（FCN-32s）。为了添加深度信息，我们训练模型升级到能采用4通道RGB-Ds的输入（早期融合）。这提供了一点便利，也许是由于模型一直要传播有意义的梯度的困难。紧随Gupta等人&lt;sup id=&quot;fnref:15:4&quot;&gt;&lt;a href=&quot;#fn:15&quot; class=&quot;footnote&quot;&gt;16&lt;/a&gt;&lt;/sup&gt;的成功，我们尝试3维的HHA编码深度，只在这个信息上（即深度）训练网络，和RGB与HHA的“后期融合”一样来自这两个网络中的预测将在最后一层进行总结，结果的双流网络将进行端到端的学习。最后我们将这种后期融合网络升级到16步长的版本。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/FCN/5.4.png&quot; alt=&quot;5.4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SIFT Flow&lt;/strong&gt;是一个带有33语义范畴（“桥”、“山”、“太阳”）的像素标签的2688张图片的数据集和3个几何分类（“水平”、“垂直”和“sky”)一样。一个FCN能自然学习共同代表权，即能同时预测标签的两种类别。我们学习FCN-16s的一种双向版本结合语义和几何预测层和损失。这种学习模型在这两种任务上作为独立的训练模型表现很好，同时它的学习和推理基本上和每个独立的模型一样快。表5的结果显示，计算在标准分离的2488张训练图片和200张测试图片上计算，在这两个任务上都表现的极好。&lt;/p&gt;

&lt;h4 id=&quot;6-结论&quot;&gt;6 结论&lt;/h4&gt;

&lt;p&gt;全卷积网络是模型非常重要的部分，是现代化分类网络中一个特殊的例子。认识到这个，将这些分类网络扩展到分割并通过多分辨率的层结合显著提高先进的技术，同时简化和加速学习和推理。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/FCN/6.1.png&quot; alt=&quot;6.1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;鸣谢&lt;/strong&gt; 这项工作有以下部分支持DARPA’s MSEE和SMISC项目，NSF awards IIS-1427425, IIS-1212798, IIS-1116411, 还有NSF GRFP,Toyota, 还有 Berkeley Vision和Learning Center。我们非常感谢NVIDIA捐赠的GPU。我们感谢Bharath Hariharan 和Saurabh Gupta的建议和数据集工具;我们感谢Sergio Guadarrama 重构了Caffe里的GoogLeNet;我们感谢Jitendra Malik的有帮助性评论;感谢Wei Liu指出了我们SIFT Flow平均IU计算上的一个问题和频率权重平均IU公式的错误。&lt;/p&gt;

&lt;h4 id=&quot;附录a-iu上界&quot;&gt;附录A IU上界&lt;/h4&gt;
&lt;p&gt;在这篇论文中，我们已经在平均IU分割度量上取到了很好的效果，即使是粗糙的语义预测。为了更好的理解这种度量还有关于这种方法的限制，我们在计算不同的规模上预测的表现的大致上界。我们通过下采样ground truth图像，然后再次对它们进行上采样，来模拟可以获得最好的结果，其伴随着特定的下采样因子。下表给出了不同下采样因子在PASCAL2011 val的一个子集上的平均IU。pixel-perfect预测很显然在取得最最好效果上不是必须的，而且，相反的，平均IU不是一个好的精细准确度的测量标准。&lt;/p&gt;

&lt;h4 id=&quot;附录b-更多的结果&quot;&gt;附录B 更多的结果&lt;/h4&gt;
&lt;p&gt;我们将我们的FCN用于语义分割进行了更进一步的评估。PASCAL-Context [29] 提供了PASCAL VOC 2011的全部场景注释。有超过400中不同的类别，我们遵循了 [29] 定义的被引用最频繁的59种类任务。我们分别训练和评估了训练集和val集。在表6中，我们将联合对象和Convolutional Feature Masking [4] 的stuff variation进行比较，后者是之前这项任务中最好的方法。FCN-8s在平均IU上得分为37.8，相对提高了20%。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/FCN/7.1.png&quot; alt=&quot;7.1&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;变更记录&quot;&gt;变更记录&lt;/h4&gt;
&lt;p&gt;论文的arXiv版本保持着最新的修正和其他的相关材料，接下来给出一份简短的变更历史。v2 添加了附录A和附录B。修正了PASCAL的有效数量（之前一些val图像被包含在训练中），SIFT Flow平均IU（用的不是很规范的度量），还有频率权重平均IU公式的一个错误。添加了模型和更新时间数字来反映改进的实现的链接（公开可用的）。&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;参考文献&quot;&gt;参考文献&lt;/h4&gt;

&lt;p&gt;arXiv:1408.5093, 2014. 7&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:22&quot;&gt;
      &lt;p&gt;A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012. 1, 2, 3, 5 &lt;a href=&quot;#fnref:22&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:22:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:22:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:22:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:22:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:22:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:34&quot;&gt;
      &lt;p&gt;K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR,abs/1409.1556, 2014. 1, 2, 3, 5 &lt;a href=&quot;#fnref:34&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:34:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:34:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:34:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:34:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:34:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:35&quot;&gt;
      &lt;p&gt;C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A.Rabinovich. Going deeper with convolutions. CoRR, abs/1409.4842,2014. 1, 2, 3, 5 &lt;a href=&quot;#fnref:35&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:35:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:35:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:35:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:35:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:35:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,E. Tzeng, and T. Darrell. DeCAF: A deep convolutional activation feature for generic visual recognition. In ICML, 2014.1, 2 &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:5:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:32&quot;&gt;
      &lt;p&gt;P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2014.1, 2, 4 &lt;a href=&quot;#fnref:32&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:32:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:32:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:32:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:12&quot;&gt;
      &lt;p&gt;R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Computer Vision and Pattern Recognition,2014. 1, 2, 7 &lt;a href=&quot;#fnref:12&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:12:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:12:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:19&quot;&gt;
      &lt;p&gt;K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV, 2014. 1, 2 &lt;a href=&quot;#fnref:19&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:19:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:42&quot;&gt;
      &lt;p&gt;N. Zhang, J. Donahue, R. Girshick, and T. Darrell. Partbased r-cnns for fine-grained category detection. In Computer Vision–ECCV 2014, pages 834–849. Springer, 2014.1 &lt;a href=&quot;#fnref:42&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:26&quot;&gt;
      &lt;p&gt;J. Long, N. Zhang, and T. Darrell. Do convnets learn correspondence?In NIPS, 2014. 1 &lt;a href=&quot;#fnref:26&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:26:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:10&quot;&gt;
      &lt;p&gt;P. Fischer, A. Dosovitskiy, and T. Brox. Descriptor matching with convolutional neural networks: a comparison to SIFT.CoRR, abs/1405.5769, 2014. 1 &lt;a href=&quot;#fnref:10&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:30&quot;&gt;
      &lt;p&gt;F. Ning, D. Delhomme, Y. LeCun, F. Piano, L. Bottou, and P. E. Barbano. Toward automatic phenotyping of developing embryos from videos. Image Processing, IEEE Transactions on, 14(9):1360–1371, 2005. 1, 2, 4, 7 &lt;a href=&quot;#fnref:30&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:30:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:30:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:30:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:30:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:30:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:30:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;D. C. Ciresan, A. Giusti, L. M. Gambardella, and J. Schmidhuber.Deep neural networks segment neuronal membranes in electron microscopy images. In NIPS, pages 2852–2860,2012. 1, 2, 4, 7 &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:3:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:3:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:3:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:3:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:3:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:3:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:3:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:9&quot;&gt;
      &lt;p&gt;C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Learning hierarchical features for scene labeling. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 2013. 1, 2, 4,7, 8 &lt;a href=&quot;#fnref:9&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:9:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:9:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:9:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:9:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:9:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:9:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:9:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:9:8&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:9:9&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:9:10&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;11&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:31&quot;&gt;
      &lt;p&gt;P. H. Pinheiro and R. Collobert. Recurrent convolutional neural networks for scene labeling. In ICML, 2014. 1, 2,4, 7, 8 &lt;a href=&quot;#fnref:31&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:31:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:31:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:31:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:31:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:31:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:31:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:31:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:31:8&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:31:9&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:17&quot;&gt;
      &lt;p&gt;B. Hariharan, P. Arbel´aez, R. Girshick, and J. Malik. Simultaneous detection and segmentation. In European Conference on Computer Vision (ECCV), 2014. 1, 2, 4, 5, 7, 8 &lt;a href=&quot;#fnref:17&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:17:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:17:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:17:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:17:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:17:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:17:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:17:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:17:8&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:15&quot;&gt;
      &lt;p&gt;S. Gupta, R. Girshick, P. Arbelaez, and J. Malik. Learning rich features from RGB-D images for object detection and segmentation. In ECCV. Springer, 2014. 1, 2, 8 &lt;a href=&quot;#fnref:15&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:15:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:15:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:15:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:15:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:11&quot;&gt;
      &lt;p&gt;Y. Ganin and V. Lempitsky. N4-fields: Neural network nearest neighbor fields for image transforms. In ACCV, 2014. 1,2, 7 &lt;a href=&quot;#fnref:11&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:11:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:11:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:11:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:11:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:11:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:11:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:11:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:11:8&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:41&quot;&gt;
      &lt;p&gt;M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In Computer Vision–ECCV 2014,pages 818–833. Springer, 2014. 2 &lt;a href=&quot;#fnref:41&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:28&quot;&gt;
      &lt;p&gt;O. Matan, C. J. Burges, Y. LeCun, and J. S. Denker. Multidigit recognition using a space displacement neural network.In NIPS, pages 488–495. Citeseer, 1991. 2 &lt;a href=&quot;#fnref:28&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:23&quot;&gt;
      &lt;p&gt;Y. LeCun, B. Boser, J. Denker, D. Henderson, R. E. Howard,W. Hubbard, and L. D. Jackel. Backpropagation applied to hand-written zip code recognition. In Neural Computation,1989. 2, 3 &lt;a href=&quot;#fnref:23&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:23:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:40&quot;&gt;
      &lt;p&gt;R. Wolf and J. C. Platt. Postal address block location using a convolutional locator network. Advances in Neural Information Processing Systems, pages 745–745, 1994. 2 &lt;a href=&quot;#fnref:40&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;D. Eigen, D. Krishnan, and R. Fergus. Restoring an image taken through a window covered with dirt or rain. In Computer Vision (ICCV), 2013 IEEE International Conference on, pages 633–640. IEEE, 2013. 2 &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:6:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:6:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:38&quot;&gt;
      &lt;p&gt;J. Tompson, A. Jain, Y. LeCun, and C. Bregler. Joint training of a convolutional network and a graphical model for human pose estimation. CoRR, abs/1406.2984, 2014. 2 &lt;a href=&quot;#fnref:38&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot;&gt;
      &lt;p&gt;D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction from a single image using a multi-scale deep network. arXiv preprint arXiv:1406.2283, 2014. 2 &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:7:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:18&quot;&gt;
      &lt;p&gt;B. Hariharan, P. Arbel´aez, R. Girshick, and J. Malik. Hypercolumns for object segmentation and fine-grained localization.In Computer Vision and Pattern Recognition, 2015.2 &lt;a href=&quot;#fnref:18&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:13&quot;&gt;
      &lt;p&gt;A. Giusti, D. C. Cires¸an, J. Masci, L. M. Gambardella, and J. Schmidhuber. Fast image scanning with deep max-pooling convolutional neural networks. In ICIP, 2013. 3, 4 &lt;a href=&quot;#fnref:13&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:13:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:27&quot;&gt;
      &lt;p&gt;S. Mallat. A wavelet tour of signal processing. Academic press, 2nd edition, 1999. 4 &lt;a href=&quot;#fnref:27&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:39&quot;&gt;
      &lt;p&gt;L. Wan, M. Zeiler, S. Zhang, Y. L. Cun, and R. Fergus. Regularization of neural networks using dropconnect. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 1058–1066, 2013. 4 &lt;a href=&quot;#fnref:39&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot;&gt;
      &lt;p&gt;M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2011 (VOC2011) Results. &lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;C. M. Bishop. Pattern recognition and machine learning,page 229. Springer-Verlag New York, 2006. 6 &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:21&quot;&gt;
      &lt;p&gt;J. J. Koenderink and A. J. van Doorn. Representation of local geometry in the visual system. Biological cybernetics,55(6):367–375, 1987. 6 &lt;a href=&quot;#fnref:21&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:16&quot;&gt;
      &lt;p&gt;B. Hariharan, P. Arbelaez, L. Bourdev, S. Maji, and J. Malik.Semantic contours from inverse detectors. In International Conference on Computer Vision (ICCV), 2011. 7 &lt;a href=&quot;#fnref:16&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:24&quot;&gt;
      &lt;p&gt;Y. A. LeCun, L. Bottou, G. B. Orr, and K.-R. M¨uller. Efficient backprop. In Neural networks: Tricks of the trade,pages 9–48. Springer, 1998. 7 &lt;a href=&quot;#fnref:24&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:20&quot;&gt;
      &lt;p&gt;Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint &lt;a href=&quot;#fnref:20&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:33&quot;&gt;
      &lt;p&gt;N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, 2012. 8 &lt;a href=&quot;#fnref:33&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Mon, 07 Jan 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/01/FCN/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/01/FCN/</guid>
        
        <category>深度学习-视觉</category>
        
        
        <category>深度学习-视觉</category>
        
      </item>
    
      <item>
        <title>github pages添加阅读量</title>
        <description>&lt;!--more--&gt;

&lt;p&gt;标签： github&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;前提是已经搭建好github-pages&quot;&gt;前提是已经搭建好github pages&lt;/h3&gt;

&lt;h3 id=&quot;1注册leancloud&quot;&gt;1.注册LeanCloud&lt;/h3&gt;

&lt;h3 id=&quot;2创建应用申请appid和appkey&quot;&gt;2.创建应用，申请appid和appkey&lt;/h3&gt;

&lt;h3 id=&quot;3创建一个class随便起名例如counter&quot;&gt;3.创建一个class，随便起名，例如Counter&lt;/h3&gt;

&lt;h3 id=&quot;4修改代码&quot;&gt;4.修改代码:&lt;/h3&gt;

&lt;h5 id=&quot;1-_configyml文件&quot;&gt;1. _config.yml文件&lt;/h5&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;leancloud:
enable: true
app_id: your_id
app_key: your_key
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;2添加includeleancloud-analyticshtml&quot;&gt;2.添加include/leancloud-analytics.html&lt;/h5&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;

&amp;lt;script src=&quot;https://code.jquery.com/jquery-3.2.0.min.js&quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;script src=&quot;https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js&quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;script&amp;gt;AV.initialize(&quot;TKFClR9mFN7woW6NwHuQAxDb-gzGzoHsz&quot;, &quot;8NibCKPlTVw5om1DF1dMaQvN&quot;);&amp;lt;/script&amp;gt;
&amp;lt;script&amp;gt;
function showHitCount(Counter) {
var query = new AV.Query(Counter);
var entries = [];
var $visitors = $(&quot;.leancloud_visitors&quot;);
$visitors.each(function () {
entries.push( $(this).attr(&quot;id&quot;).trim() );
});
query.containedIn('url', entries);
query.find()
.done(function (results) {
console.log(&quot;results&quot;,results);
var COUNT_CONTAINER_REF = '.leancloud-visitors-count';
if (results.length === 0) {
$visitors.find(COUNT_CONTAINER_REF).text(0);
return;
}
for (var i = 0; i &amp;lt; results.length; i++) {
var item = results[i];
var url = item.get('url');
var hits = item.get('hits');
var element = document.getElementById(url);
$(element).find(COUNT_CONTAINER_REF).text(hits);
}
for(var i = 0; i &amp;lt; entries.length; i++) {
var url = entries[i];
var element = document.getElementById(url);
var countSpan = $(element).find(COUNT_CONTAINER_REF);
if( countSpan.text() == '') {
countSpan.text(0);
}
}
})
.fail(function (object, error) {
console.log(&quot;Error: &quot; + error.code + &quot; &quot; + error.message);
});
}
function addCount(Counter) {
var $visitors = $(&quot;.leancloud_visitors&quot;);
var url = $visitors.attr('id').trim();
var title = $visitors.attr('data-flag-title').trim();
var query = new AV.Query(Counter);
query.equalTo(&quot;url&quot;, url);
query.find({
success: function(results) {
if (results.length &amp;gt; 0) {
var counter = results[0];
counter.fetchWhenSave(true);
counter.increment(&quot;hits&quot;);
counter.save(null, {
success: function(counter) {
var $element = $(document.getElementById(url));
$element.find('.leancloud-visitors-count').text(counter.get('hits'));
},
error: function(counter, error) {
console.log('Failed to save Visitor num, with error message: ' + error.message);
}
});
} else {
var newcounter = new Counter();
/* Set ACL */
var acl = new AV.ACL();
acl.setPublicReadAccess(true);
acl.setPublicWriteAccess(true);
newcounter.setACL(acl);
/* End Set ACL */
newcounter.set(&quot;title&quot;, title);
newcounter.set(&quot;url&quot;, url);
newcounter.set(&quot;hits&quot;, 1);
newcounter.save(null, {
success: function(newcounter) {
var $element = $(document.getElementById(url));
$element.find('.leancloud-visitors-count').text(newcounter.get('hits'));
},
error: function(newcounter, error) {
console.log('Failed to create');
}
});
}
},
error: function(error) {
console.log('Error:' + error.code + &quot; &quot; + error.message);
}
});
}
$(function() {
var Counter = AV.Object.extend(&quot;Counter&quot;);
if ($('.leancloud_visitors').length == 1) {
// in post.html, so add 1 to hit counts
addCount(Counter);
} else if ($('.post-link').length &amp;gt; 1){
// in index.html, there are many 'leancloud_visitors' and 'post-link', so just show hit counts.
showHitCount(Counter);
}
});
&amp;lt;/script&amp;gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;3_layoutsdefaulthtml&quot;&gt;3._layouts/default.html&lt;/h5&gt;
&lt;p&gt;添加&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;

&lt;span class=&quot;nt&quot;&gt;&amp;lt;script &lt;/span&gt;&lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;https://code.jquery.com/jquery-3.2.0.min.js&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;script &lt;/span&gt;&lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;script&amp;gt;&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;AV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;initialize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;TKFClR9mFN7woW6NwHuQAxDb-gzGzoHsz&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;8NibCKPlTVw5om1DF1dMaQvN&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;&amp;lt;!--&amp;lt;script&amp;gt;console.log(&quot;Error: &quot; + error.code + &quot; &quot; + error.message);&amp;lt;/script&amp;gt;--&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;script&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;showHitCount&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;Counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;i was called&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;query&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;AV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;Query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;Counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;entries&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[];&lt;/span&gt;
        &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;$visitors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;.leancloud_visitors&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;$visitors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;each&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;nx&quot;&gt;entries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;push&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;attr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;trim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;containedIn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'url'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;entries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;find&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                    &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;results&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                    &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;COUNT_CONTAINER_REF&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'.leancloud-visitors-count'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;===&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                        &lt;span class=&quot;nx&quot;&gt;$visitors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;find&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;COUNT_CONTAINER_REF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
                    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                        &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
                        &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'url'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                        &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;hits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'hits'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                        &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;element&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;document&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;getElementById&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                        &lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;element&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;find&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;COUNT_CONTAINER_REF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;hits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;entries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                        &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;entries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
                        &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;element&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;document&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;getElementById&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                        &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;countSpan&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;element&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;find&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;COUNT_CONTAINER_REF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;countSpan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                            &lt;span class=&quot;nx&quot;&gt;countSpan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
                    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;fail&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                    &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Error: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;code&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot; &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;addCount&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;Counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;$visitors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;.leancloud_visitors&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;$visitors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;attr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'id'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;trim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
        &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;title&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;$visitors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;attr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'data-flag-title'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;trim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
        &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;query&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;AV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;Query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;Counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;equalTo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;url&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;find&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;success&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                    &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;counter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
                    &lt;span class=&quot;nx&quot;&gt;counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;fetchWhenSave&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                    &lt;span class=&quot;nx&quot;&gt;counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;increment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;hits&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                    &lt;span class=&quot;nx&quot;&gt;counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                        &lt;span class=&quot;na&quot;&gt;success&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                            &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;$element&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;document&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;getElementById&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
                            &lt;span class=&quot;nx&quot;&gt;$element&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;find&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'.leancloud-visitors-count'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'hits'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
                        &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
                        &lt;span class=&quot;na&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                            &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Failed to save Visitor num, with error message: '&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
                    &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                    &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;newcounter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;Counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
                    &lt;span class=&quot;cm&quot;&gt;/* Set ACL */&lt;/span&gt;
                    &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;acl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;AV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;ACL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
                    &lt;span class=&quot;nx&quot;&gt;acl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;setPublicReadAccess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                    &lt;span class=&quot;nx&quot;&gt;acl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;setPublicWriteAccess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                    &lt;span class=&quot;nx&quot;&gt;newcounter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;setACL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;acl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                    &lt;span class=&quot;cm&quot;&gt;/* End Set ACL */&lt;/span&gt;
                    &lt;span class=&quot;nx&quot;&gt;newcounter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;title&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                    &lt;span class=&quot;nx&quot;&gt;newcounter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;url&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                    &lt;span class=&quot;nx&quot;&gt;newcounter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;hits&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                    &lt;span class=&quot;nx&quot;&gt;newcounter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                        &lt;span class=&quot;na&quot;&gt;success&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;newcounter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                            &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;$element&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;document&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;getElementById&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
                            &lt;span class=&quot;nx&quot;&gt;$element&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;find&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'.leancloud-visitors-count'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;newcounter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'hits'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
                        &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
                        &lt;span class=&quot;na&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;newcounter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                            &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Failed to create'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
                    &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Error:'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;code&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot; &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;Counter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;AV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;Object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;extend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Counter&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'this is a test'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'this is a test-add'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'.leancloud_visitors'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'this is a test-show'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'.post-link'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'.leancloud_visitors'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;// in post.html, so add 1 to hit counts&lt;/span&gt;
            &lt;span class=&quot;nx&quot;&gt;addCount&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;Counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'.post-link'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;// in index.html, there are many 'leancloud_visitors' and 'post-link', so just show hit counts.&lt;/span&gt;
            &lt;span class=&quot;nx&quot;&gt;showHitCount&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;Counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/script&amp;gt;&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;4_layoutsposthtml&quot;&gt;4._layouts/post.html&lt;/h5&gt;
&lt;p&gt;添加&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&amp;lt;span id=&quot;/2019/01/addreadvalue/&quot; class=&quot;leancloud_visitors&quot; data-flag-title=&quot;github pages添加阅读量&quot;&amp;gt;
&amp;lt;span class=&quot;post-meta-divider&quot;&amp;gt;|&amp;lt;/span&amp;gt;
&amp;lt;span class=&quot;post-meta-item-text&quot;&amp;gt; 阅读量:  &amp;lt;/span&amp;gt;
&amp;lt;span class=&quot;leancloud-visitors-count&quot;&amp;gt;&amp;lt;/span&amp;gt;
&amp;lt;/span&amp;gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;reference:&lt;/p&gt;

&lt;p&gt;[1]&lt;a href=&quot;https://blog.csdn.net/u013553529/article/details/63357382&quot;&gt;在个人博客中添加文章点击次数&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2]&lt;a href=&quot;https://github.com/galian123/galian123.github.io&quot;&gt;github&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 06 Jan 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/01/addreadvalue/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/01/addreadvalue/</guid>
        
        <category>github</category>
        
        
        <category>github</category>
        
      </item>
    
      <item>
        <title>NVIDIA tx2刷机教程</title>
        <description>&lt;!--more--&gt;

&lt;p&gt;上个月买了个tx2开发板，跑一些深度模型，记录一下刷机教程。&lt;/p&gt;

</description>
        <pubDate>Sat, 05 Jan 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/01/tx2shuaji/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/01/tx2shuaji/</guid>
        
        <category>硬件</category>
        
        
        <category>硬件</category>
        
      </item>
    
      <item>
        <title>MarkDown支持的格式样例（from作业部落）</title>
        <description>&lt;!--more--&gt;

&lt;p&gt;标签： 杂记&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;我们理解您需要更便捷更高效的工具记录思想，整理笔记、知识，并将其中承载的价值传播给他人，&lt;strong&gt;Cmd Markdown&lt;/strong&gt; 是我们给出的答案 —— 我们为记录思想和分享知识提供更专业的工具。 您可以使用 Cmd Markdown：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;整理知识，学习笔记&lt;/li&gt;
    &lt;li&gt;发布日记，杂文，所见所想&lt;/li&gt;
    &lt;li&gt;撰写发布技术文稿（代码支持）&lt;/li&gt;
    &lt;li&gt;撰写发布学术论文（LaTeX 公式支持）&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://www.zybuluo.com/static/img/logo.png&quot; alt=&quot;cmd-markdown-logo&quot; /&gt;&lt;/p&gt;

&lt;p&gt;除了您现在看到的这个 Cmd Markdown 在线版本，您还可以前往以下网址下载：&lt;/p&gt;

&lt;h3 id=&quot;windowsmaclinux-全平台客户端&quot;&gt;&lt;a href=&quot;https://www.zybuluo.com/cmd/&quot;&gt;Windows/Mac/Linux 全平台客户端&lt;/a&gt;&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;请保留此份 Cmd Markdown 的欢迎稿兼使用说明，如需撰写新稿件，点击顶部工具栏右侧的 &lt;i class=&quot;icon-file&quot;&gt;&lt;/i&gt; &lt;strong&gt;新文稿&lt;/strong&gt; 或者使用快捷键 &lt;code class=&quot;highlighter-rouge&quot;&gt;Ctrl+Alt+N&lt;/code&gt;。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;什么是-markdown&quot;&gt;什么是 Markdown&lt;/h2&gt;

&lt;p&gt;Markdown 是一种方便记忆、书写的纯文本标记语言，用户可以使用这些标记符号以最小的输入代价生成极富表现力的文档：譬如您正在阅读的这份文档。它使用简单的符号标记不同的标题，分割不同的段落，&lt;strong&gt;粗体&lt;/strong&gt; 或者 &lt;em&gt;斜体&lt;/em&gt; 某些文字，更棒的是，它还可以&lt;/p&gt;

&lt;h3 id=&quot;1-制作一份待办事宜-todo-列表&quot;&gt;1. 制作一份待办事宜 &lt;a href=&quot;https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#13-待办事宜-todo-列表&quot;&gt;Todo 列表&lt;/a&gt;&lt;/h3&gt;

&lt;ul class=&quot;task-list&quot;&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;支持以 PDF 格式导出文稿&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;改进 Cmd 渲染算法，使用局部渲染技术提高渲染效率&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;新增 Todo 列表功能&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;修复 LaTex 公式渲染问题&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;新增 LaTex 公式编号功能&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-书写一个质能守恒公式&quot;&gt;2. 书写一个质能守恒公式&lt;sup id=&quot;fnref:LaTeX&quot;&gt;&lt;a href=&quot;#fn:LaTeX&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E=mc^2&lt;/script&gt;

&lt;h3 id=&quot;3-高亮一段代码&quot;&gt;3. 高亮一段代码&lt;sup id=&quot;fnref:code&quot;&gt;&lt;a href=&quot;#fn:code&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nd&quot;&gt;@requires_authorization&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;SomeClass&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'__main__'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# A comment&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'hello world'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;4-高效绘制-流程图暂不支持&quot;&gt;4. 高效绘制 &lt;a href=&quot;https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#7-流程图&quot;&gt;流程图&lt;/a&gt;（暂不支持）&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-flow&quot;&gt;st=&amp;gt;start: Start
op=&amp;gt;operation: Your Operation
cond=&amp;gt;condition: Yes or No?
e=&amp;gt;end

st-&amp;gt;op-&amp;gt;cond
cond(yes)-&amp;gt;e
cond(no)-&amp;gt;op
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;5-高效绘制-序列图暂不支持&quot;&gt;5. 高效绘制 &lt;a href=&quot;https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#8-序列图&quot;&gt;序列图&lt;/a&gt;（暂不支持）&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-seq&quot;&gt;Alice-&amp;gt;Bob: Hello Bob, how are you?
Note right of Bob: Bob thinks
Bob--&amp;gt;Alice: I am good thanks!
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;6-高效绘制-甘特图-暂不支持&quot;&gt;6. 高效绘制 &lt;a href=&quot;https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#9-甘特图&quot;&gt;甘特图&lt;/a&gt; (暂不支持)&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-gantt&quot;&gt;    title 项目开发流程
    section 项目确定
        需求分析       :a1, 2016-06-22, 3d
        可行性报告     :after a1, 5d
        概念验证       : 5d
    section 项目实施
        概要设计      :2016-07-05  , 5d
        详细设计      :2016-07-08, 10d
        编码          :2016-07-15, 10d
        测试          :2016-07-22, 5d
    section 发布验收
        发布: 2d
        验收: 3d
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;7-绘制表格&quot;&gt;7. 绘制表格&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;项目&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;价格&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;数量&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;计算机&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;$1600&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;手机&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;$12&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;管线&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;$1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;234&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;8-更详细语法说明&quot;&gt;8. 更详细语法说明&lt;/h3&gt;

&lt;p&gt;想要查看更详细的语法说明，可以参考我们准备的 &lt;a href=&quot;https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown&quot;&gt;Cmd Markdown 简明语法手册&lt;/a&gt;，进阶用户可以参考 &lt;a href=&quot;https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#cmd-markdown-高阶语法手册&quot;&gt;Cmd Markdown 高阶语法手册&lt;/a&gt; 了解更多高级功能。&lt;/p&gt;

&lt;p&gt;总而言之，不同于其它 &lt;em&gt;所见即所得&lt;/em&gt; 的编辑器：你只需使用键盘专注于书写文本内容，就可以生成印刷级的排版格式，省却在键盘和工具栏之间来回切换，调整内容和格式的麻烦。&lt;strong&gt;Markdown 在流畅的书写和印刷级的阅读体验之间找到了平衡。&lt;/strong&gt; 目前它已经成为世界上最大的技术分享网站 GitHub 和 技术问答网站 StackOverFlow 的御用书写格式。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;什么是-cmd-markdown&quot;&gt;什么是 Cmd Markdown&lt;/h2&gt;

&lt;p&gt;您可以使用很多工具书写 Markdown，但是 Cmd Markdown 是这个星球上我们已知的、最好的 Markdown 工具——没有之一 ：）因为深信文字的力量，所以我们和你一样，对流畅书写，分享思想和知识，以及阅读体验有极致的追求，我们把对于这些诉求的回应整合在 Cmd Markdown，并且一次，两次，三次，乃至无数次地提升这个工具的体验，最终将它演化成一个 &lt;strong&gt;编辑/发布/阅读&lt;/strong&gt; Markdown 的在线平台——您可以在任何地方，任何系统/设备上管理这里的文字。&lt;/p&gt;

&lt;h3 id=&quot;1-实时同步预览&quot;&gt;1. 实时同步预览&lt;/h3&gt;

&lt;p&gt;我们将 Cmd Markdown 的主界面一分为二，左边为&lt;strong&gt;编辑区&lt;/strong&gt;，右边为&lt;strong&gt;预览区&lt;/strong&gt;，在编辑区的操作会实时地渲染到预览区方便查看最终的版面效果，并且如果你在其中一个区拖动滚动条，我们有一个巧妙的算法把另一个区的滚动条同步到等价的位置，超酷！&lt;/p&gt;

&lt;h3 id=&quot;2-编辑工具栏&quot;&gt;2. 编辑工具栏&lt;/h3&gt;

&lt;p&gt;也许您还是一个 Markdown 语法的新手，在您完全熟悉它之前，我们在 &lt;strong&gt;编辑区&lt;/strong&gt; 的顶部放置了一个如下图所示的工具栏，您可以使用鼠标在工具栏上调整格式，不过我们仍旧鼓励你使用键盘标记格式，提高书写的流畅度。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.zybuluo.com/static/img/toolbar-editor.png&quot; alt=&quot;tool-editor&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3-编辑模式&quot;&gt;3. 编辑模式&lt;/h3&gt;

&lt;p&gt;完全心无旁骛的方式编辑文字：点击 &lt;strong&gt;编辑工具栏&lt;/strong&gt; 最右侧的拉伸按钮或者按下 &lt;code class=&quot;highlighter-rouge&quot;&gt;Ctrl + M&lt;/code&gt;，将 Cmd Markdown 切换到独立的编辑模式，这是一个极度简洁的写作环境，所有可能会引起分心的元素都已经被挪除，超清爽！&lt;/p&gt;

&lt;h3 id=&quot;4-实时的云端文稿&quot;&gt;4. 实时的云端文稿&lt;/h3&gt;

&lt;p&gt;为了保障数据安全，Cmd Markdown 会将您每一次击键的内容保存至云端，同时在 &lt;strong&gt;编辑工具栏&lt;/strong&gt; 的最右侧提示 &lt;code class=&quot;highlighter-rouge&quot;&gt;已保存&lt;/code&gt; 的字样。无需担心浏览器崩溃，机器掉电或者地震，海啸——在编辑的过程中随时关闭浏览器或者机器，下一次回到 Cmd Markdown 的时候继续写作。&lt;/p&gt;

&lt;h3 id=&quot;5-离线模式&quot;&gt;5. 离线模式&lt;/h3&gt;

&lt;p&gt;在网络环境不稳定的情况下记录文字一样很安全！在您写作的时候，如果电脑突然失去网络连接，Cmd Markdown 会智能切换至离线模式，将您后续键入的文字保存在本地，直到网络恢复再将他们传送至云端，即使在网络恢复前关闭浏览器或者电脑，一样没有问题，等到下次开启 Cmd Markdown 的时候，她会提醒您将离线保存的文字传送至云端。简而言之，我们尽最大的努力保障您文字的安全。&lt;/p&gt;

&lt;h3 id=&quot;6-管理工具栏&quot;&gt;6. 管理工具栏&lt;/h3&gt;

&lt;p&gt;为了便于管理您的文稿，在 &lt;strong&gt;预览区&lt;/strong&gt; 的顶部放置了如下所示的 &lt;strong&gt;管理工具栏&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.zybuluo.com/static/img/toolbar-manager.jpg&quot; alt=&quot;tool-manager&quot; /&gt;&lt;/p&gt;

&lt;p&gt;通过管理工具栏可以：&lt;/p&gt;

&lt;p&gt;&lt;i class=&quot;icon-share&quot;&gt;&lt;/i&gt; 发布：将当前的文稿生成固定链接，在网络上发布，分享
&lt;i class=&quot;icon-file&quot;&gt;&lt;/i&gt; 新建：开始撰写一篇新的文稿
&lt;i class=&quot;icon-trash&quot;&gt;&lt;/i&gt; 删除：删除当前的文稿
&lt;i class=&quot;icon-cloud&quot;&gt;&lt;/i&gt; 导出：将当前的文稿转化为 Markdown 文本或者 Html 格式，并导出到本地
&lt;i class=&quot;icon-reorder&quot;&gt;&lt;/i&gt; 列表：所有新增和过往的文稿都可以在这里查看、操作
&lt;i class=&quot;icon-pencil&quot;&gt;&lt;/i&gt; 模式：切换 普通/Vim/Emacs 编辑模式&lt;/p&gt;

&lt;h3 id=&quot;7-阅读工具栏&quot;&gt;7. 阅读工具栏&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://www.zybuluo.com/static/img/toolbar-reader.jpg&quot; alt=&quot;tool-manager&quot; /&gt;&lt;/p&gt;

&lt;p&gt;通过 &lt;strong&gt;预览区&lt;/strong&gt; 右上角的 &lt;strong&gt;阅读工具栏&lt;/strong&gt;，可以查看当前文稿的目录并增强阅读体验。&lt;/p&gt;

&lt;p&gt;工具栏上的五个图标依次为：&lt;/p&gt;

&lt;p&gt;&lt;i class=&quot;icon-list&quot;&gt;&lt;/i&gt; 目录：快速导航当前文稿的目录结构以跳转到感兴趣的段落
&lt;i class=&quot;icon-chevron-sign-left&quot;&gt;&lt;/i&gt; 视图：互换左边编辑区和右边预览区的位置
&lt;i class=&quot;icon-adjust&quot;&gt;&lt;/i&gt; 主题：内置了黑白两种模式的主题，试试 &lt;strong&gt;黑色主题&lt;/strong&gt;，超炫！
&lt;i class=&quot;icon-desktop&quot;&gt;&lt;/i&gt; 阅读：心无旁骛的阅读模式提供超一流的阅读体验
&lt;i class=&quot;icon-fullscreen&quot;&gt;&lt;/i&gt; 全屏：简洁，简洁，再简洁，一个完全沉浸式的写作和阅读环境&lt;/p&gt;

&lt;h3 id=&quot;8-阅读模式&quot;&gt;8. 阅读模式&lt;/h3&gt;

&lt;p&gt;在 &lt;strong&gt;阅读工具栏&lt;/strong&gt; 点击 &lt;i class=&quot;icon-desktop&quot;&gt;&lt;/i&gt; 或者按下 &lt;code class=&quot;highlighter-rouge&quot;&gt;Ctrl+Alt+M&lt;/code&gt; 随即进入独立的阅读模式界面，我们在版面渲染上的每一个细节：字体，字号，行间距，前背景色都倾注了大量的时间，努力提升阅读的体验和品质。&lt;/p&gt;

&lt;h3 id=&quot;9-标签分类和搜索&quot;&gt;9. 标签、分类和搜索&lt;/h3&gt;

&lt;p&gt;在编辑区任意行首位置输入以下格式的文字可以标签当前文档：&lt;/p&gt;

&lt;p&gt;标签： 未分类&lt;/p&gt;

&lt;p&gt;标签以后的文稿在【文件列表】（Ctrl+Alt+F）里会按照标签分类，用户可以同时使用键盘或者鼠标浏览查看，或者在【文件列表】的搜索文本框内搜索标题关键字过滤文稿，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.zybuluo.com/static/img/file-list.png&quot; alt=&quot;file-list&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;10-文稿发布和分享&quot;&gt;10. 文稿发布和分享&lt;/h3&gt;

&lt;p&gt;在您使用 Cmd Markdown 记录，创作，整理，阅读文稿的同时，我们不仅希望它是一个有力的工具，更希望您的思想和知识通过这个平台，连同优质的阅读体验，将他们分享给有相同志趣的人，进而鼓励更多的人来到这里记录分享他们的思想和知识，尝试点击 &lt;i class=&quot;icon-share&quot;&gt;&lt;/i&gt; (Ctrl+Alt+P) 发布这份文档给好友吧！&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;再一次感谢您花费时间阅读这份欢迎稿，点击 &lt;i class=&quot;icon-file&quot;&gt;&lt;/i&gt; (Ctrl+Alt+N) 开始撰写新的文稿吧！祝您在这里记录、阅读、分享愉快！&lt;/p&gt;

&lt;p&gt;作者 &lt;a href=&quot;http://weibo.com/ghosert&quot;&gt;@ghosert&lt;/a&gt;   &lt;br /&gt;
2016 年 07月 07日&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:LaTeX&quot;&gt;
      &lt;p&gt;支持 &lt;strong&gt;LaTeX&lt;/strong&gt; 编辑显示支持，例如：$\sum_{i=1}^n a_i=0$， 访问 &lt;a href=&quot;http://meta.math.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference&quot;&gt;MathJax&lt;/a&gt; 参考更多使用方法。 &lt;a href=&quot;#fnref:LaTeX&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:code&quot;&gt;
      &lt;p&gt;代码高亮功能支持包括 Java, Python, JavaScript 在内的，&lt;strong&gt;四十一&lt;/strong&gt;种主流编程语言。 &lt;a href=&quot;#fnref:code&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Fri, 04 Jan 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/01/markdown/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/01/markdown/</guid>
        
        <category>拾遗</category>
        
        
        <category>拾遗</category>
        
      </item>
    
      <item>
        <title>blog push到远程仓库（github pages）出错</title>
        <description>&lt;!--more--&gt;

&lt;p&gt;本地仓库更新blog后，需要远程推送到github仓库。用命令&lt;code class=&quot;highlighter-rouge&quot;&gt;git push xxx(远程库名字，有的默认origin) master&lt;/code&gt;。但是出现错误。
我怀疑是因为远程仓库被访问后，有些修改，所以需要pull到本地，再push，还没验证。
但是也可以&lt;code class=&quot;highlighter-rouge&quot;&gt;git push -f xxx master&lt;/code&gt;强制覆盖，反正也是你自己一个人在维护blog，–force也不会有人想要砍死你😎&lt;/p&gt;
</description>
        <pubDate>Thu, 03 Jan 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/01/pushgithubpage/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/01/pushgithubpage/</guid>
        
        <category>github</category>
        
        
        <category>github</category>
        
      </item>
    
      <item>
        <title>制作github pages</title>
        <description>&lt;!--more--&gt;

&lt;p&gt;标签： github&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;之前在阿里云租借服务器搭建网站，需要昂贵的费用很麻烦。所以改在用github上搭建自己的个人博客，既方便又美观（免费）。&lt;/p&gt;
&lt;h3 id=&quot;1-环境&quot;&gt;1. 环境&lt;/h3&gt;

&lt;ul class=&quot;task-list&quot;&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Mac&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;linux、windows没试&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;github账号&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;git&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;ruby&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;jekyll&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-配置&quot;&gt;2. 配置&lt;/h3&gt;
&lt;p&gt;为了方便，直接上&lt;a href=&quot;http://baixin.io/2016/10/jekyll_tutorials1/&quot;&gt;大神教程&lt;/a&gt;。内容很详细，直接按着来就行了。我这里就只写一些遇到的问题。&lt;/p&gt;
&lt;h5 id=&quot;1-jekyll-server报错&quot;&gt;1) jekyll server报错：&lt;/h5&gt;
&lt;p&gt;我的原因是ruby版本太老，于是更新。安装rvm（ruby版本管理工具Ruby Version Manager）&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl -L get.rvm.io | bash -s stable
source ~/.bashrc  
source ~/.bash_profile   (如果你的终端个性化配置过，可能会出差错，不要怕，退出重开就好了)
rvm -v  查看版本
rvm list known 查看可用版本
rvm install 2.4.1（可以换你需要的版本）
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;如果缺少一些包，比如xxx，直接直接执行&lt;code class=&quot;highlighter-rouge&quot;&gt;gem install xxx&lt;/code&gt;就行了（我缺少 minima）&lt;/p&gt;
&lt;h5 id=&quot;2-gem-源的问题&quot;&gt;2) gem 源的问题：&lt;/h5&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;gem sources -a http://gems.ruby-china.com/&lt;/code&gt;
(淘宝源没了，http://gems.ruby-china.org也没了，这是最新的，2019.1.3实验可用)&lt;/p&gt;

&lt;h3 id=&quot;3-本地运行效果&quot;&gt;3. 本地运行效果&lt;/h3&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$ jekyll server &lt;/code&gt;就行了
浏览器输入&lt;a href=&quot;http://127.0.0.1:4000&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;http://127.0.0.1:4000&lt;/code&gt;&lt;/a&gt;查看效果。&lt;/p&gt;

&lt;h3 id=&quot;4-push到github仓库&quot;&gt;4. push到github仓库&lt;/h3&gt;
&lt;p&gt;建一个username.github.io的仓库，把本地的项目push上去。浏览器访问&lt;code class=&quot;highlighter-rouge&quot;&gt;www.username.github.io&lt;/code&gt;就可以了。注意username一定要是你的github账号名字！&lt;/p&gt;

&lt;h3 id=&quot;5-更新文章&quot;&gt;5. 更新文章&lt;/h3&gt;
&lt;p&gt;直接更新_post内的.md文件就行,然后push到远程库上。&lt;/p&gt;

&lt;h3 id=&quot;6-推荐一个编辑器&quot;&gt;6. 推荐一个编辑器&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://www.zybuluo.com/mdeditor#&quot;&gt;MarkDown&lt;/a&gt;非常好用,墙裂推荐！！！童叟无欺！！！&lt;/p&gt;

&lt;h3 id=&quot;7-用自己的域名解析到github-page上&quot;&gt;7. 用自己的域名解析到github page上&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;注册到阿里云注册一个域名&lt;/li&gt;
  &lt;li&gt;解析域名，解析方式可以选择A，ip的话在终端ping username.github.io,得到IP地址。&lt;/li&gt;
  &lt;li&gt;github仓库新建一个文件名为CNAME内容为注册的域名（不用www前缀）的文件&lt;/li&gt;
  &lt;li&gt;over
 如果想看图文并茂的的，推荐此&lt;a href=&quot;https://www.cnblogs.com/olddoublemoon/p/6629398.html&quot;&gt;博客&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Thu, 03 Jan 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/01/page/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/01/page/</guid>
        
        <category>github</category>
        
        
        <category>github</category>
        
      </item>
    
      <item>
        <title>yolo</title>
        <description>&lt;!--more--&gt;

&lt;p&gt;标签： CNN，深度学习，检测&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1506.02640&quot;&gt;You Only Look Once: Unified, Real-Time Object Detection&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;abstract&quot;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.&lt;/p&gt;

&lt;h4 id=&quot;摘要&quot;&gt;摘要&lt;/h4&gt;
&lt;p&gt;我们提出了YOLO，一种新的目标检测方法。以前的目标检测工作重新利用分类器来执行检测。相反，我们将目标检测框架看作回归问题从空间上分割边界框和相关的类别概率。单个神经网络在一次评估中直接从完整图像上预测边界框和类别概率。由于整个检测流水线是单一网络，因此可以直接对检测性能进行端到端的优化。&lt;/p&gt;

&lt;p&gt;Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.&lt;/p&gt;

&lt;p&gt;我们的统一架构非常快。我们的基础YOLO模型以45帧/秒的速度实时处理图像。网络的一个较小版本，快速YOLO，每秒能处理惊人的155帧，同时实现其它实时检测器两倍的mAP。与最先进的检测系统相比，YOLO产生了更多的定位误差，但不太可能在背景上的预测假阳性。最后，YOLO学习目标非常通用的表示。当从自然图像到艺术品等其它领域泛化时，它都优于其它检测方法，包括DPM和R-CNN。&lt;/p&gt;

&lt;h3 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h3&gt;
&lt;p&gt;Humans glance at an image and instantly know what objects are in the image, where they are, and how they interact. The human visual system is fast and accurate, allowing us to perform complex tasks like driving with little conscious thought. Fast, accurate algorithms for object detection would allow computers to drive cars without specialized sensors, enable assistive devices to convey real-time scene information to human users, and unlock the potential for general purpose, responsive robotic systems.&lt;/p&gt;

&lt;h4 id=&quot;1-引言&quot;&gt;1. 引言&lt;/h4&gt;
&lt;p&gt;人们瞥一眼图像，立即知道图像中的物体是什么，它们在哪里以及它们如何相互作用。人类的视觉系统是快速和准确的，使我们能够执行复杂的任务，如驾驶时没有多少有意识的想法。快速，准确的目标检测算法可以让计算机在没有专门传感器的情况下驾驶汽车，使辅助设备能够向人类用户传达实时的场景信息，并表现出对一般用途和响应机器人系统的潜力。&lt;/p&gt;

&lt;p&gt;Current detection systems repurpose classifiers to perform detection. To detect an object, these systems take a classifier for that object and evaluate it at various locations and scales in a test image. Systems like deformable parts models (DPM) use a sliding window approach where the classifier is run at evenly spaced locations over the entire image [10].&lt;/p&gt;

&lt;p&gt;目前的检测系统重用分类器来执行检测。为了检测目标，这些系统为该目标提供一个分类器，并在不同的位置对其进行评估，并在测试图像中进行缩放。像可变形部件模型（DPM）这样的系统使用滑动窗口方法，其分类器在整个图像的均匀间隔的位置上运行[10]。&lt;/p&gt;

&lt;p&gt;More recent approaches like R-CNN use region proposal methods to first generate potential bounding boxes in an image and then run a classifier on these proposed boxes. After classification, post-processing is used to refine the bounding boxes, eliminate duplicate detections, and rescore the boxes based on other objects in the scene [13]. These complex pipelines are slow and hard to optimize because each individual component must be trained separately.&lt;/p&gt;

&lt;p&gt;最近的方法，如R-CNN使用区域提出方法首先在图像中生成潜在的边界框，然后在这些提出的框上运行分类器。在分类之后，后处理用于细化边界框，消除重复的检测，并根据场景中的其它目标重新定位边界框[13]。这些复杂的流程很慢，很难优化，因为每个单独的组件都必须单独进行训练。&lt;/p&gt;

&lt;p&gt;We reframe object detection as a single regression problem, straight from image pixels to bounding box coordinates and class probabilities. Using our system, you only look once (YOLO) at an image to predict what objects are present and where they are.&lt;/p&gt;

&lt;p&gt;我们将目标检测重新看作单一的回归问题，直接从图像像素到边界框坐标和类概率。使用我们的系统，您只需要在图像上看一次（YOLO），以预测出现的目标和位置。&lt;/p&gt;

&lt;p&gt;YOLO is refreshingly simple: see Figure 1. A single convolutional network simultaneously predicts multiple bounding boxes and class probabilities for those boxes. YOLO trains on full images and directly optimizes detection performance. This unified model has several benefits over traditional methods of object detection.&lt;/p&gt;

&lt;p&gt;YOLO很简单：参见图1。单个卷积网络同时预测这些盒子的多个边界框和类概率。YOLO在全图像上训练并直接优化检测性能。这种统一的模型比传统的目标检测方法有一些好处。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/yolo/1.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 1: The YOLO Detection System. Processing images with YOLO is simple and straightforward. Our system (1) resizes the input image to 448 × 448, (2) runs a single convolutional network on the image, and (3) thresholds the resulting detections by the model’s confidence.&lt;/p&gt;

&lt;p&gt;图1：YOLO检测系统。用YOLO处理图像简单直接。我们的系统（1）将输入图像调整为448×448，（2）在图像上运行单个卷积网络，以及（3）由模型的置信度对所得到的检测进行阈值处理。&lt;/p&gt;

&lt;p&gt;First, YOLO is extremely fast. Since we frame detection as a regression problem we don’t need a complex pipeline. We simply run our neural network on a new image at test time to predict detections. Our base network runs at 45 frames per second with no batch processing on a Titan X GPU and a fast version runs at more than 150 fps. This means we can process streaming video in real-time with less than 25 milliseconds of latency. Furthermore, YOLO achieves more than twice the mean average precision of other real-time systems. For a demo of our system running in real-time on a webcam please see our project webpage: &lt;a href=&quot;http://pjreddie.com/yolo/&quot;&gt;http://pjreddie.com/yolo/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;首先，YOLO速度非常快。由于我们将检测视为回归问题，所以我们不需要复杂的流程。测试时我们在一张新图像上简单的运行我们的神经网络来预测检测。我们的基础网络以每秒45帧的速度运行，在Titan X GPU上没有批处理，快速版本运行速度超过150fps。这意味着我们可以在不到25毫秒的延迟内实时处理流媒体视频。此外，YOLO实现了其它实时系统两倍以上的平均精度。关于我们的系统在网络摄像头上实时运行的演示，请参阅我们的项目网页：&lt;a href=&quot;http://pjreddie.com/yolo/&quot;&gt;http://pjreddie.com/yolo/&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;Second, YOLO reasons globally about the image when making predictions. Unlike sliding window and region proposal-based techniques, YOLO sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance. Fast R-CNN, a top detection method [14], mistakes background patches in an image for objects because it can’t see the larger context. YOLO makes less than half the number of background errors compared to Fast R-CNN.&lt;/p&gt;

&lt;p&gt;其次，YOLO在进行预测时，会对图像进行全面地推理。与基于滑动窗口和区域提出的技术不同，YOLO在训练期间和测试时会看到整个图像，所以它隐式地编码了关于类的上下文信息以及它们的外观。fast R-CNN是一种顶级的检测方法[14]，因为它看不到更大的上下文，所以在图像中会将背景块误检为目标。与快速R-CNN相比，YOLO的背景误检数量少了一半。&lt;/p&gt;

&lt;p&gt;Third, YOLO learns generalizable representations of objects. When trained on natural images and tested on artwork, YOLO outperforms top detection methods like DPM and R-CNN by a wide margin. Since YOLO is highly generalizable it is less likely to break down when applied to new domains or unexpected inputs.&lt;/p&gt;

&lt;p&gt;第三，YOLO学习目标的泛化表示。当在自然图像上进行训练并对艺术作品进行测试时，YOLO大幅优于DPM和R-CNN等顶级检测方法。由于YOLO具有高度泛化能力，因此在应用于新领域或碰到意外的输入时不太可能出故障。&lt;/p&gt;

&lt;p&gt;YOLO still lags behind state-of-the-art detection systems in accuracy. While it can quickly identify objects in images it struggles to precisely localize some objects, especially small ones. We examine these tradeoffs further in our experiments.&lt;/p&gt;

&lt;p&gt;YOLO在精度上仍然落后于最先进的检测系统。虽然它可以快速识别图像中的目标，但它仍在努力精确定位一些目标，尤其是小的目标。我们在实验中会进一步检查这些权衡。&lt;/p&gt;

&lt;p&gt;All of our training and testing code is open source. A variety of pretrained models are also available to download.&lt;/p&gt;

&lt;p&gt;我们所有的训练和测试代码都是开源的。各种预训练模型也都可以下载。&lt;/p&gt;

&lt;h3 id=&quot;2-unified-detection&quot;&gt;2. Unified Detection&lt;/h3&gt;
&lt;p&gt;We unify the separate components of object detection into a single neural network. Our network uses features from the entire image to predict each bounding box. It also predicts all bounding boxes across all classes for an image simultaneously. This means our network reasons globally about the full image and all the objects in the image. The YOLO design enables end-to-end training and real-time speeds while maintaining high average precision.&lt;/p&gt;

&lt;h4 id=&quot;2-统一检测&quot;&gt;2. 统一检测&lt;/h4&gt;
&lt;p&gt;我们将目标检测的单独组件集成到单个神经网络中。我们的网络使用整个图像的特征来预测每个边界框。它还可以同时预测一张图像中的所有类别的所有边界框。这意味着我们的网络全面地推理整张图像和图像中的所有目标。YOLO设计可实现端到端训练和实时的速度，同时保持较高的平均精度。&lt;/p&gt;

&lt;p&gt;Our system divides the input image into an S×S grid. If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object.&lt;/p&gt;

&lt;p&gt;我们的系统将输入图像分成S×S的网格。如果一个目标的中心落入一个网格单元中，该网格单元负责检测该目标。&lt;/p&gt;

&lt;p&gt;Each grid cell predicts B bounding boxes and confidence scores for those boxes. These confidence scores reflect how confident the model is that the box contains an object and also how accurate it thinks the box is that it predicts. Formally we define confidence as $\Pr(\textrm{Object}) * \textrm{IOU}_{\textrm{pred}}^{\textrm{truth}}$ . If no object exists in that cell, the confidence scores should be zero. Otherwise we want the confidence score to equal the intersection over union (IOU) between the predicted box and the ground truth.&lt;/p&gt;

&lt;p&gt;每个网格单元预测这些盒子的B个边界框和置信度分数。这些置信度分数反映了该模型对盒子是否包含目标的信心，以及它预测盒子的准确程度。在形式上，我们将置信度定义为$\Pr(\textrm{Object}) * \textrm{IOU}_{\textrm{pred}}^{\textrm{truth}}$。如果该单元格中不存在目标，则置信度分数应为零。否则，我们希望置信度分数等于预测框与真实值之间联合部分的交集（IOU）。&lt;/p&gt;

&lt;p&gt;Each bounding box consists of 5 predictions: $x$, $y$, $w$, $h$, and confidence. The $(x,y)$ coordinates represent the center of the box relative to the bounds of the grid cell. The width and height are predicted relative to the whole image. Finally the confidence prediction represents the IOU between the predicted box and any ground truth box.&lt;/p&gt;

&lt;p&gt;每个边界框包含5个预测：$x$，$y$，$w$，$h$和置信度。$(x，y)$坐标表示边界框相对于网格单元边界框的中心。宽度和高度是相对于整张图像预测的。最后，置信度预测表示预测框与实际边界框之间的IOU。&lt;/p&gt;

&lt;p&gt;Each grid cell also predicts $C$ conditional class probabilities,
$Pr(Class_i|Object)$. These probabilities are conditioned on the grid cell containing an object. We only predict one set of class probabilities per grid cell, regardless of the number of boxes $B$.&lt;/p&gt;

&lt;p&gt;每个网格单元还预测$C$个条件类别概率
$Pr(Class_i|Object)$。这些概率以包含目标的网格单元为条件。每个网格单元我们只预测的一组类别概率，而不管边界框的的数量$B$是多少。&lt;/p&gt;

&lt;p&gt;At test time we multiply the conditional class probabilities and the individual box confidence predictions,
&lt;script type=&quot;math/tex&quot;&gt;\Pr(\textrm{Class}_i | \textrm{Object}) * \Pr(\textrm{Object}) * \textrm{IOU}_{\textrm{pred}}^{\textrm{truth}} = \Pr(\textrm{Class}_i)*\textrm{IOU}_{\textrm{pred}}^{\textrm{truth}}&lt;/script&gt;which gives us class-specific confidence scores for each box. These scores encode both the probability of that class appearing in the box and how well the predicted box fits the object.&lt;/p&gt;

&lt;p&gt;在测试时，我们乘以条件类概率和单个盒子的置信度预测，
&lt;script type=&quot;math/tex&quot;&gt;\Pr(\textrm{Class}_i | \textrm{Object}) * \Pr(\textrm{Object}) * \textrm{IOU}_{\textrm{pred}}^{\textrm{truth}} = \Pr(\textrm{Class}_i)*\textrm{IOU}_{\textrm{pred}}^{\textrm{truth}}&lt;/script&gt;它为我们提供了每个框特定类别的置信度分数。这些分数编码了该类出现在框中的概率以及预测框拟合目标的程度。&lt;/p&gt;

&lt;p&gt;For evaluating YOLO on Pascal VOC, we use $S=7$, $B=2$. Pascal VOC has 20 labelled classes so $C=20$. Our final prediction is a $7×7×30$ tensor.&lt;/p&gt;

&lt;p&gt;为了在Pascal VOC上评估YOLO，我们使用S=7，B=2。Pascal VOC有20个标注类，所以C=20。我们最终的预测是7×7×30的张量。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/yolo/2.png&quot; alt=&quot;2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Model&lt;/strong&gt;. Our system models detection as a regression problem. It divides the image into an $S×S$ grid and for each grid cell predicts $B$ bounding boxes, confidence for those boxes, and $C$ class probabilities. These predictions are encoded as an $S×S×(B∗5+C)$ tensor.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;模型。&lt;/strong&gt; 我们的系统将检测建模为回归问题。它将图像分成$S×S$的网格，并且每个网格单元预测$B$个边界框，这些边界框的置信度以及$C$个类别概率。这些预测被编码为$S×S×(B∗5+C)$的张量。&lt;/p&gt;

&lt;h4 id=&quot;21-network-design&quot;&gt;2.1. Network Design&lt;/h4&gt;

&lt;p&gt;We implement this model as a convolutional neural network and evaluate it on the Pascal VOC detection dataset [9]. The initial convolutional layers of the network extract features from the image while the fully connected layers predict the output probabilities and coordinates.&lt;/p&gt;

&lt;h4 id=&quot;21-网络设计&quot;&gt;2.1. 网络设计&lt;/h4&gt;
&lt;p&gt;我们将此模型作为卷积神经网络来实现，并在Pascal VOC检测数据集[9]上进行评估。网络的初始卷积层从图像中提取特征，而全连接层预测输出概率和坐标。&lt;/p&gt;

&lt;p&gt;Our network architecture is inspired by the GoogLeNet model for image classification [34]. Our network has 24 convolutional layers followed by 2 fully connected layers. Instead of the inception modules used by GoogLeNet, we simply use 1×1 reduction layers followed by 3×3 convolutional layers, similar to Lin et al [22]. The full network is shown in Figure 3.&lt;/p&gt;

&lt;p&gt;我们的网络架构受到GoogLeNet图像分类模型的启发[34]。我们的网络有24个卷积层，后面是2个全连接层。我们只使用1×1降维层，后面是3×3卷积层，这与Lin等人[22]类似，而不是GoogLeNet使用的Inception模块。完整的网络如图3所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/yolo/3.png&quot; alt=&quot;3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 3: The Architecture.&lt;/strong&gt; Our detection network has 24 convolutional layers followed by 2 fully connected layers. Alternating 1×1 convolutional layers reduce the features space from preceding layers. We pretrain the convolutional layers on the ImageNet classification task at half the resolution (224×224 input image) and then double the resolution for detection.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;图3：架构。&lt;/strong&gt;我们的检测网络有24个卷积层，其次是2个全连接层。交替1×1卷积层减少了前面层的特征空间。我们在ImageNet分类任务上以一半的分辨率（224×224的输入图像）预训练卷积层，然后将分辨率加倍来进行检测。&lt;/p&gt;

&lt;p&gt;We also train a fast version of YOLO designed to push the boundaries of fast object detection. Fast YOLO uses a neural network with fewer convolutional layers (9 instead of 24) and fewer filters in those layers. Other than the size of the network, all training and testing parameters are the same between YOLO and Fast YOLO.&lt;/p&gt;

&lt;p&gt;我们还训练了快速版本的YOLO，旨在推动快速目标检测的界限。快速YOLO使用具有较少卷积层（9层而不是24层）的神经网络，在这些层中使用较少的滤波器。除了网络规模之外，YOLO和快速YOLO的所有训练和测试参数都是相同的。&lt;/p&gt;

&lt;p&gt;The final output of our network is the 7×7×30 tensor of predictions.&lt;/p&gt;

&lt;p&gt;我们网络的最终输出是7×7×30的预测张量。&lt;/p&gt;

&lt;h4 id=&quot;22-training&quot;&gt;2.2. Training&lt;/h4&gt;
&lt;p&gt;We pretrain our convolutional layers on the ImageNet 1000-class competition dataset [30]. For pretraining we use the first 20 convolutional layers from Figure 3 followed by a average-pooling layer and a fully connected layer. We train this network for approximately a week and achieve a single crop &lt;code class=&quot;highlighter-rouge&quot;&gt;top-5&lt;/code&gt; accuracy of 88% on the ImageNet 2012 validation set, comparable to the GoogLeNet models in Caffe’s Model Zoo [24]. We use the Darknet framework for all training and inference [26].&lt;/p&gt;

&lt;h4 id=&quot;22-训练&quot;&gt;2.2. 训练&lt;/h4&gt;
&lt;p&gt;我们在ImageNet 1000类竞赛数据集[30]上预训练我们的卷积图层。对于预训练，我们使用图3中的前20个卷积层，接着是平均池化层和全连接层。我们对这个网络进行了大约一周的训练，并且在ImageNet 2012验证集上获得了单一裁剪图像88%的&lt;code class=&quot;highlighter-rouge&quot;&gt;top-5&lt;/code&gt;准确率，与Caffe模型池中的GoogLeNet模型相当。我们使用Darknet框架进行所有的训练和推断[26]。&lt;/p&gt;

&lt;p&gt;We then convert the model to perform detection. Ren et al. show that adding both convolutional and connected layers to pretrained networks can improve performance [29]. Following their example, we add four convolutional layers and two fully connected layers with randomly initialized weights. Detection often requires fine-grained visual information so we increase the input resolution of the network from 224×224 to 448×448.&lt;/p&gt;

&lt;p&gt;然后我们转换模型来执行检测。Ren等人表明，预训练网络中增加卷积层和连接层可以提高性能[29]。按照他们的例子，我们添加了四个卷积层和两个全连接层，并且具有随机初始化的权重。检测通常需要细粒度的视觉信息，因此我们将网络的输入分辨率从224×224变为448×448。&lt;/p&gt;

&lt;p&gt;Our final layer predicts both class probabilities and bounding box coordinates. We normalize the bounding box width and height by the image width and height so that they fall between 0 and 1. We parametrize the bounding box x and y coordinates to be offsets of a particular grid cell location so they are also bounded between 0 and 1.&lt;/p&gt;

&lt;p&gt;我们的最后一层预测类概率和边界框坐标。我们通过图像宽度和高度来规范边界框的宽度和高度，使它们落在0和1之间。我们将边界框x和y坐标参数化为特定网格单元位置的偏移量，所以它们边界也在0和1之间。&lt;/p&gt;

&lt;p&gt;We use a linear activation function for the final layer and all other layers use the following leaky rectified linear activation:&lt;script type=&quot;math/tex&quot;&gt;\phi(x) = \begin{cases}     x, if x &gt; 0 \\     0.1x, otherwise \end{cases}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;我们对最后一层使用线性激活函数，所有其它层使用下面的漏泄修正线性激活：&lt;script type=&quot;math/tex&quot;&gt;\phi(x) = \begin{cases}     x, if x &gt; 0 \\     0.1x, otherwise \end{cases}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We optimize for sum-squared error in the output of our model. We use sum-squared error because it is easy to optimize, however it does not perfectly align with our goal of maximizing average precision. It weights localization error equally with classification error which may not be ideal. Also, in every image many grid cells do not contain any object. This pushes the “confidence” scores of those cells towards zero, often overpowering the gradient from cells that do contain objects. This can lead to model instability, causing training to diverge early on.&lt;/p&gt;

&lt;p&gt;我们优化了模型输出中的平方和误差。我们使用平方和误差，因为它很容易进行优化，但是它并不完全符合我们最大化平均精度的目标。分类误差与定位误差的权重是一样的，这可能并不理想。另外，在每张图像中，许多网格单元不包含任何对象。这将这些单元格的“置信度”分数推向零，通常压倒了包含目标的单元格的梯度。这可能导致模型不稳定，从而导致训练早期发散。&lt;/p&gt;

&lt;p&gt;To remedy this, we increase the loss from bounding box coordinate predictions and decrease the loss from confidence predictions for boxes that don’t contain objects. We use two parameters, $\lambda_\textrm{coord}$ and $\lambda_\textrm{noobj}$ to accomplish this. We set $\lambda_\textrm{coord}=5$ and $\lambda_\textrm{noobj}=.5$.&lt;/p&gt;

&lt;p&gt;为了改善这一点，我们增加了边界框坐标预测损失，并减少了不包含目标边界框的置信度预测损失。我们使用两个参数$\lambda_\textrm{coord}$和$\lambda_\textrm{noobj}$来完成这个工作。我们设置$\lambda_\textrm{coord}=5$和$\lambda_\textrm{noobj}=.5$。&lt;/p&gt;

&lt;p&gt;Sum-squared error also equally weights errors in large boxes and small boxes. Our error metric should reflect that small deviations in large boxes matter less than in small boxes. To partially address this we predict the square root of the bounding box width and height instead of the width and height directly.&lt;/p&gt;

&lt;p&gt;平方和误差也可以在大盒子和小盒子中同样加权误差。我们的错误指标应该反映出，大盒子小偏差的重要性不如小盒子小偏差的重要性。为了部分解决这个问题，我们直接预测边界框宽度和高度的平方根，而不是宽度和高度。&lt;/p&gt;

&lt;p&gt;YOLO predicts multiple bounding boxes per grid cell. At training time we only want one bounding box predictor to be responsible for each object. We assign one predictor to be “responsible” for predicting an object based on which prediction has the highest current IOU with the ground truth. This leads to specialization between the bounding box predictors. Each predictor gets better at predicting certain sizes, aspect ratios, or classes of object, improving overall recall.&lt;/p&gt;

&lt;p&gt;YOLO每个网格单元预测多个边界框。在训练时，每个目标我们只需要一个边界框预测器来负责。我们指定一个预测器“负责”根据哪个预测与真实值之间具有当前最高的IOU来预测目标。这导致边界框预测器之间的专业化。每个预测器可以更好地预测特定大小，方向角，或目标的类别，从而改善整体召回率。&lt;/p&gt;

&lt;p&gt;During training we optimize the following, multi-part loss function:
&lt;script type=&quot;math/tex&quot;&gt;\begin{multline} \lambda_\textbf{coord} \sum_{i = 0}^{S^2}     \sum_{j = 0}^{B}     \mathbb{𝟙}_{ij}^{\text{obj}}             \left[             \left(                 x_i - \hat{x}_i             \right)^2 +             \left(                 y_i - \hat{y}_i             \right)^2             \right] \\ + \lambda_\textbf{coord} \sum_{i = 0}^{S^2}     \sum_{j = 0}^{B}         \mathbb{𝟙}_{ij}^{\text{obj}}          \left[         \left(             \sqrt{w_i} - \sqrt{\hat{w}_i}         \right)^2 +         \left(             \sqrt{h_i} - \sqrt{\hat{h}_i}         \right)^2         \right] \\ + \sum_{i = 0}^{S^2}     \sum_{j = 0}^{B}         \mathbb{𝟙}_{ij}^{\text{obj}}         \left(             C_i - \hat{C}_i         \right)^2 \\ + \lambda_\textrm{noobj} \sum_{i = 0}^{S^2}     \sum_{j = 0}^{B}     \mathbb{𝟙}_{ij}^{\text{noobj}}         \left(             C_i - \hat{C}_i         \right)^2 \\ + \sum_{i = 0}^{S^2} \mathbb{𝟙}_i^{\text{obj}}     \sum_{c \in \textrm{classes}}         \left(             p_i(c) - \hat{p}_i(c)         \right)^2 \end{multline}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;where $\mathbb{𝟙}_i^{\text{obj}}$ denotes if object appears in cell $i$ and &lt;script type=&quot;math/tex&quot;&gt;\mathbb{𝟙}_{ij}^{\text{obj}}&lt;/script&gt; denotes that the $j$th bounding box predictor in cell $i$ is “responsible” for that prediction.&lt;/p&gt;

&lt;p&gt;在训练期间，我们优化以下多部分损失函数：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{multline} \lambda_\textbf{coord} \sum_{i = 0}^{S^2}     \sum_{j = 0}^{B}     \mathbb{𝟙}_{ij}^{\text{obj}}             \left[             \left(                 x_i - \hat{x}_i             \right)^2 +             \left(                 y_i - \hat{y}_i             \right)^2             \right] \\ + \lambda_\textbf{coord} \sum_{i = 0}^{S^2}     \sum_{j = 0}^{B}         \mathbb{𝟙}_{ij}^{\text{obj}}          \left[         \left(             \sqrt{w_i} - \sqrt{\hat{w}_i}         \right)^2 +         \left(             \sqrt{h_i} - \sqrt{\hat{h}_i}         \right)^2         \right] \\ + \sum_{i = 0}^{S^2}     \sum_{j = 0}^{B}         \mathbb{𝟙}_{ij}^{\text{obj}}         \left(             C_i - \hat{C}_i         \right)^2 \\ + \lambda_\textrm{noobj} \sum_{i = 0}^{S^2}     \sum_{j = 0}^{B}     \mathbb{𝟙}_{ij}^{\text{noobj}}         \left(             C_i - \hat{C}_i         \right)^2 \\ + \sum_{i = 0}^{S^2} \mathbb{𝟙}_i^{\text{obj}}     \sum_{c \in \textrm{classes}}         \left(             p_i(c) - \hat{p}_i(c)         \right)^2 \end{multline}&lt;/script&gt;

&lt;p&gt;其中$\mathbb{𝟙}_i^{\text{obj}}$表示目标是否出现在网格单元$i$中，&lt;script type=&quot;math/tex&quot;&gt;\mathbb{𝟙}_{ij}^{\text{obj}}&lt;/script&gt;表示网格单元$i$中的第$j$个边界框预测器“负责”该预测。&lt;/p&gt;

&lt;p&gt;Note that the loss function only penalizes classification error if an object is present in that grid cell (hence the conditional class probability discussed earlier). It also only penalizes bounding box coordinate error if that predictor is “responsible” for the ground truth box (i.e. has the highest IOU of any predictor in that grid cell).&lt;/p&gt;

&lt;p&gt;注意，如果目标存在于该网格单元中（前面讨论的条件类别概率），则损失函数仅惩罚分类错误。如果预测器“负责”实际边界框（即该网格单元中具有最高IOU的预测器），则它也仅惩罚边界框坐标错误。&lt;/p&gt;

&lt;p&gt;We train the network for about 135 epochs on the training and validation data sets from Pascal VOC 2007 and 2012. When testing on 2012 we also include the VOC 2007 test data for training. Throughout training we use a batch size of 64, a momentum of 0.9 and a decay of 0.0005.&lt;/p&gt;

&lt;p&gt;我们对Pascal VOC2007和2012的训练和验证数据集进行了大约135个迭代周期的网络训练。在Pascal VOC 2012上进行测试时，我们的训练包含了Pascal VOC 2007的测试数据。在整个训练过程中，我们使用了64的批大小，0.9的动量和0.0005的衰减。&lt;/p&gt;

&lt;p&gt;Our learning rate schedule is as follows: For the first epochs we slowly raise the learning rate from $10^{−3} $to $10^{−2}$. If we start at a high learning rate our model often diverges due to unstable gradients. We continue training with $10^{−2} $ for 75 epochs, then $10^{−3} $ for 30 epochs, and finally $10^{−4} $ for 30 epochs.&lt;/p&gt;

&lt;p&gt;我们的学习率方案如下：对于第一个迭代周期，我们慢慢地将学习率从$10^{−3} $提高到$10^{−2}$。如果我们从高学习率开始，我们的模型往往会由于不稳定的梯度而发散。我们继续以$10^{−2}$的学习率训练75个迭代周期，然后用$10^{−3}$的学习率训练30个迭代周期，最后用$10^{−4}$的学习率训练30个迭代周期。&lt;/p&gt;

&lt;p&gt;To avoid overfitting we use dropout and extensive data augmentation. A dropout layer with rate =.5 after the first connected layer prevents co-adaptation between layers [18]. For data augmentation we introduce random scaling and translations of up to 20% of the original image size. We also randomly adjust the exposure and saturation of the image by up to a factor of 1.5 in the HSV color space.&lt;/p&gt;

&lt;p&gt;为了避免过度拟合，我们使用丢弃和大量的数据增强。在第一个连接层之后，丢弃层使用=.5的比例，防止层之间的互相适应[18]。对于数据增强，我们引入高达原始图像20%大小的随机缩放和转换。我们还在HSV色彩空间中使用高达1.5的因子来随机调整图像的曝光和饱和度。&lt;/p&gt;

&lt;h4 id=&quot;23-inference&quot;&gt;2.3. Inference&lt;/h4&gt;
&lt;p&gt;Just like in training, predicting detections for a test image only requires one network evaluation. On Pascal VOC the network predicts 98 bounding boxes per image and class probabilities for each box. YOLO is extremely fast at test time since it only requires a single network evaluation, unlike classifier-based methods.&lt;/p&gt;

&lt;h4 id=&quot;23-推断&quot;&gt;2.3. 推断&lt;/h4&gt;
&lt;p&gt;就像在训练中一样，预测测试图像的检测只需要一次网络评估。在Pascal VOC上，每张图像上网络预测98个边界框和每个框的类别概率。YOLO在测试时非常快，因为它只需要一次网络评估，不像基于分类器的方法。&lt;/p&gt;

&lt;p&gt;The grid design enforces spatial diversity in the bounding box predictions. Often it is clear which grid cell an object falls in to and the network only predicts one box for each object. However, some large objects or objects near the border of multiple cells can be well localized by multiple cells. Non-maximal suppression can be used to fix these multiple detections. While not critical to performance as it is for R-CNN or DPM, non-maximal suppression adds 2−3% in mAP.&lt;/p&gt;

&lt;p&gt;网格设计强化了边界框预测中的空间多样性。通常很明显一个目标落在哪一个网格单元中，而网络只能为每个目标预测一个边界框。然而，一些大的目标或靠近多个网格单元边界的目标可以被多个网格单元很好地定位。非极大值抑制可以用来修正这些多重检测。对于R-CNN或DPM而言，性能不是关键的，非最大抑制会增加2−3%的mAP。&lt;/p&gt;

&lt;h4 id=&quot;24-limitations-of-yolo&quot;&gt;2.4. Limitations of YOLO&lt;/h4&gt;

&lt;p&gt;YOLO imposes strong spatial constraints on bounding box predictions since each grid cell only predicts two boxes and can only have one class. This spatial constraint limits the number of nearby objects that our model can predict. Our model struggles with small objects that appear in groups, such as flocks of birds.&lt;/p&gt;

&lt;h4 id=&quot;24-yolo的限制&quot;&gt;2.4. YOLO的限制&lt;/h4&gt;
&lt;p&gt;YOLO对边界框预测强加空间约束，因为每个网格单元只预测两个盒子，只能有一个类别。这个空间约束限制了我们的模型可以预测的邻近目标的数量。我们的模型与群组中出现的小物体（比如鸟群）进行斗争。&lt;/p&gt;

&lt;p&gt;Since our model learns to predict bounding boxes from data, it struggles to generalize to objects in new or unusual aspect ratios or configurations. Our model also uses relatively coarse features for predicting bounding boxes since our architecture has multiple downsampling layers from the input image.&lt;/p&gt;

&lt;p&gt;由于我们的模型学习从数据中预测边界框，因此它很难泛化到新的、不常见的方向比或配置的目标。我们的模型也使用相对较粗糙的特征来预测边界框，因为我们的架构具有来自输入图像的多个下采样层。&lt;/p&gt;

&lt;p&gt;Finally, while we train on a loss function that approximates detection performance, our loss function treats errors the same in small bounding boxes versus large bounding boxes. A small error in a large box is generally benign but a small error in a small box has a much greater effect on IOU. Our main source of error is incorrect localizations.&lt;/p&gt;

&lt;p&gt;最后，当我们训练一个近似检测性能的损失函数时，我们的损失函数会同样的对待小边界框与大边界框的误差。大边界框的小误差通常是良性的，但小边界框的小误差对IOU的影响要大得多。我们的主要错误来源是不正确的定位。&lt;/p&gt;

&lt;h3 id=&quot;3-comparison-to-other-detection-systems&quot;&gt;3. Comparison to Other Detection Systems&lt;/h3&gt;
&lt;p&gt;Object detection is a core problem in computer vision. Detection pipelines generally start by extracting a set of robust features from input images (Haar [25], SIFT [23], HOG [4], convolutional features [6]). Then, classifiers [36, 21, 13, 10] or localizers [1, 32] are used to identify objects in the feature space. These classifiers or localizers are run either in sliding window fashion over the whole image or on some subset of regions in the image [35, 15, 39]. We compare the YOLO detection system to several top detection frameworks, highlighting key similarities and differences.&lt;/p&gt;

&lt;h4 id=&quot;3-与其它检测系统的比较&quot;&gt;3. 与其它检测系统的比较&lt;/h4&gt;
&lt;p&gt;目标检测是计算机视觉中的核心问题。检测流程通常从输入图像上（Haar [25]，SIFT [23]，HOG [4]，卷积特征[6]）提取一组鲁棒特征开始。然后，分类器[36,21,13,10]或定位器[1,32]被用来识别特征空间中的目标。这些分类器或定位器在整个图像上或在图像中的一些子区域上以滑动窗口的方式运行[35,15,39]。我们将YOLO检测系统与几种顶级检测框架进行比较，突出了关键的相似性和差异性。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deformable parts models.&lt;/strong&gt; Deformable parts models (DPM) use a sliding window approach to object detection [10]. DPM uses a disjoint pipeline to extract static features, classify regions, predict bounding boxes for high scoring regions, etc. Our system replaces all of these disparate parts with a single convolutional neural network. The network performs feature extraction, bounding box prediction, non-maximal suppression, and contextual reasoning all concurrently. Instead of static features, the network trains the features in-line and optimizes them for the detection task. Our unified architecture leads to a faster, more accurate model than DPM.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;可变形部件模型。&lt;/strong&gt;可变形零件模型（DPM）使用滑动窗口方法进行目标检测[10]。DPM使用不相交的流程来提取静态特征，对区域进行分类，预测高评分区域的边界框等。我们的系统用单个卷积神经网络替换所有这些不同的部分。网络同时进行特征提取，边界框预测，非极大值抑制和上下文推理。网络内嵌训练特征而不是静态特征，并为检测任务优化它们。我们的统一架构导致了比DPM更快，更准确的模型。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;R-CNN.&lt;/strong&gt; R-CNN and its variants use region proposals instead of sliding windows to find objects in images. Selective Search [35] generates potential bounding boxes, a convolutional network extracts features, an SVM scores the boxes, a linear model adjusts the bounding boxes, and non-max suppression eliminates duplicate detections. Each stage of this complex pipeline must be precisely tuned independently and the resulting system is very slow, taking more than 40 seconds per image at test time [14].&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;R-CNN.&lt;/strong&gt; R-CNN及其变种使用区域提出而不是滑动窗口来查找图像中的目标。选择性搜索[35]产生潜在的边界框，卷积网络提取特征，SVM对边界框进行评分，线性模型调整边界框，非极大值抑制消除重复检测。这个复杂流程的每个阶段都必须独立地进行精确调整，所得到的系统非常慢，测试时每张图像需要超过40秒[14]。&lt;/p&gt;

&lt;p&gt;YOLO shares some similarities with R-CNN. Each grid cell proposes potential bounding boxes and scores those boxes using convolutional features. However, our system puts spatial constraints on the grid cell proposals which helps mitigate multiple detections of the same object. Our system also proposes far fewer bounding boxes, only 98 per image compared to about 2000 from Selective Search. Finally, our system combines these individual components into a single, jointly optimized model.&lt;/p&gt;

&lt;p&gt;YOLO与R-CNN有一些相似之处。每个网格单元提出潜在的边界框并使用卷积特征对这些框进行评分。但是，我们的系统对网格单元提出进行了空间限制，这有助于缓解对同一目标的多次检测。我们的系统还提出了更少的边界框，每张图像只有98个，而选择性搜索则只有2000个左右。最后，我们的系统将这些单独的组件组合成一个单一的，共同优化的模型。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Other Fast Detectors.&lt;/strong&gt; Fast and Faster R-CNN focus on speeding up the R-CNN framework by sharing computation and using neural networks to propose regions instead of Selective Search [14] [28]. While they offer speed and accuracy improvements over R-CNN, both still fall short of real-time performance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;其它快速检测器。&lt;/strong&gt;快速和更快的R-CNN通过共享计算和使用神经网络替代选择性搜索来提出区域加速R-CNN框架[14]，[28]。虽然它们提供了比R-CNN更快的速度和更高的准确度，但两者仍然不能达到实时性能。&lt;/p&gt;

&lt;p&gt;Many research efforts focus on speeding up the DPM pipeline [31] [38] [5]. They speed up HOG computation, use cascades, and push computation to GPUs. However, only 30Hz DPM [31] actually runs in real-time.&lt;/p&gt;

&lt;p&gt;许多研究工作集中在加快DPM流程上[31] [38] [5]。它们加速HOG计算，使用级联，并将计算推动到GPU上。但是，实际上只有30Hz的DPM [31]可以实时运行。&lt;/p&gt;

&lt;p&gt;Instead of trying to optimize individual components of a large detection pipeline, YOLO throws out the pipeline entirely and is fast by design.&lt;/p&gt;

&lt;p&gt;YOLO不是试图优化大型检测流程的单个组件，而是完全抛弃流程，被设计为快速检测。&lt;/p&gt;

&lt;p&gt;Detectors for single classes like faces or people can be highly optimized since they have to deal with much less variation [37]. YOLO is a general purpose detector that learns to detect a variety of objects simultaneously.&lt;/p&gt;

&lt;p&gt;像人脸或行人等单类别的检测器可以高度优化，因为他们必须处理更少的变化[37]。YOLO是一种通用的检测器，可以学习同时检测多个目标。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deep MultiBox.&lt;/strong&gt; Unlike R-CNN, Szegedy et al. train a convolutional neural network to predict regions of interest [8] instead of using Selective Search. MultiBox can also perform single object detection by replacing the confidence prediction with a single class prediction. However, MultiBox cannot perform general object detection and is still just a piece in a larger detection pipeline, requiring further image patch classification. Both YOLO and MultiBox use a convolutional network to predict bounding boxes in an image but YOLO is a complete detection system.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deep MultiBox。&lt;/strong&gt;与R-CNN不同，Szegedy等人训练了一个卷积神经网络来预测感兴趣区域[8]，而不是使用选择性搜索。MultiBox还可以通过用单类预测替换置信度预测来执行单目标检测。然而，MultiBox无法执行通用的目标检测，并且仍然只是一个较大的检测流程中的一部分，需要进一步的图像块分类。YOLO和MultiBox都使用卷积网络来预测图像中的边界框，但是YOLO是一个完整的检测系统。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;OverFeat.&lt;/strong&gt; Sermanet et al. train a convolutional neural network to perform localization and adapt that localizer to perform detection [32]. OverFeat efficiently performs sliding window detection but it is still a disjoint system. OverFeat optimizes for localization, not detection performance. Like DPM, the localizer only sees local information when making a prediction. OverFeat cannot reason about global context and thus requires significant post-processing to produce coherent detections.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;OverFeat。&lt;/strong&gt; Sermanet等人训练了一个卷积神经网络来执行定位，并使该定位器进行检测[32]。OverFeat高效地执行滑动窗口检测，但它仍然是一个不相交的系统。OverFeat优化了定位，而不是检测性能。像DPM一样，定位器在进行预测时只能看到局部信息。OverFeat不能推断全局上下文，因此需要大量的后处理来产生连贯的检测。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MultiGrasp.&lt;/strong&gt; Our work is similar in design to work on grasp detection by Redmon et al [27]. Our grid approach to bounding box prediction is based on the MultiGrasp system for regression to grasps. However, grasp detection is a much simpler task than object detection. MultiGrasp only needs to predict a single graspable region for an image containing one object. It doesn’t have to estimate the size, location, or boundaries of the object or predict it’s class, only find a region suitable for grasping. YOLO predicts both bounding boxes and class probabilities for multiple objects of multiple classes in an image.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MultiGrasp。&lt;/strong&gt;我们的工作在设计上类似于Redmon等[27]的抓取检测。我们对边界框预测的网格方法是基于MultiGrasp系统抓取的回归分析。然而，抓取检测比目标检测任务要简单得多。MultiGrasp只需要为包含一个目标的图像预测一个可以抓取的区域。不必估计目标的大小，位置或目标边界或预测目标的类别，只找到适合抓取的区域。YOLO预测图像中多个类别的多个目标的边界框和类别概率。&lt;/p&gt;

&lt;h3 id=&quot;4-experiments&quot;&gt;4. Experiments&lt;/h3&gt;
&lt;p&gt;First we compare YOLO with other real-time detection systems on PASCAL VOC 2007. To understand the differences between YOLO and R-CNN variants we explore the errors on VOC 2007 made by YOLO and Fast R-CNN, one of the highest performing versions of R-CNN [14]. Based on the different error profiles we show that YOLO can be used to rescore Fast R-CNN detections and reduce the errors from background false positives, giving a significant performance boost. We also present VOC 2012 results and compare mAP to current state-of-the-art methods. Finally, we show that YOLO generalizes to new domains better than other detectors on two artwork datasets.&lt;/p&gt;

&lt;h4 id=&quot;4-实验&quot;&gt;4. 实验&lt;/h4&gt;
&lt;p&gt;首先，我们在PASCAL VOC 2007上比较YOLO和其它的实时检测系统。为了理解YOLO和R-CNN变种之间的差异，我们探索了YOLO和R-CNN性能最高的版本之一Fast R-CNN[14]在VOC 2007上错误率。根据不同的误差曲线，我们显示YOLO可以用来重新评估Fast R-CNN检测，并减少背景假阳性带来的错误，从而显著提升性能。我们还展示了在VOC 2012上的结果，并与目前最先进的方法比较了mAP。最后，在两个艺术品数据集上我们显示了YOLO可以比其它检测器更好地泛化到新领域。&lt;/p&gt;

&lt;h4 id=&quot;41-comparison-to-other-real-time-systems&quot;&gt;4.1. Comparison to Other Real-Time Systems&lt;/h4&gt;
&lt;p&gt;Many research efforts in object detection focus on making standard detection pipelines fast [5] [38] [31] [14] [17] [28]. However, only Sadeghi et al. actually produce a detection system that runs in real-time (30 frames per second or better) [31]. We compare YOLO to their GPU implementation of DPM which runs either at 30Hz or 100Hz. While the other efforts don’t reach the real-time milestone we also compare their relative mAP and speed to examine the accuracy-performance tradeoffs available in object detection systems.&lt;/p&gt;

&lt;h4 id=&quot;41-与其它实时系统的比较&quot;&gt;4.1. 与其它实时系统的比较&lt;/h4&gt;
&lt;p&gt;目标检测方面的许多研究工作都集中在快速制定标准检测流程上[5]，[38]，[31]，[14]，[17]，[28]。然而，只有Sadeghi等实际上产生了一个实时运行的检测系统（每秒30帧或更好）[31]。我们将YOLO与DPM的GPU实现进行了比较，其在30Hz或100Hz下运行。虽然其它的努力没有达到实时性的里程碑，我们也比较了它们的相对mAP和速度来检查目标检测系统中精度——性能权衡。&lt;/p&gt;

&lt;p&gt;Fast YOLO is the fastest object detection method on PASCAL; as far as we know, it is the fastest extant object detector. With 52.7% mAP, it is more than twice as accurate as prior work on real-time detection. YOLO pushes mAP to 63.4% while still maintaining real-time performance.&lt;/p&gt;

&lt;p&gt;快速YOLO是PASCAL上最快的目标检测方法；据我们所知，它是现有的最快的目标检测器。具有52.7%的mAP，实时检测的精度是以前工作的两倍以上。YOLO将mAP推到63.4%的同时保持了实时性能。&lt;/p&gt;

&lt;p&gt;We also train YOLO using VGG-16. This model is more accurate but also significantly slower than YOLO. It is useful for comparison to other detection systems that rely on VGG-16 but since it is slower than real-time the rest of the paper focuses on our faster models.&lt;/p&gt;

&lt;p&gt;我们还使用VGG-16训练YOLO。这个模型比YOLO更准确，但也比它慢得多。对于依赖于VGG-16的其它检测系统来说，它是比较有用的，但由于它比实时的YOLO更慢，本文的其它部分将重点放在我们更快的模型上。&lt;/p&gt;

&lt;p&gt;Fastest DPM effectively speeds up DPM without sacrificing much mAP but it still misses real-time performance by a factor of 2 [38]. It also is limited by DPM’s relatively low accuracy on detection compared to neural network approaches.&lt;/p&gt;

&lt;p&gt;最快的DPM可以在不牺牲太多mAP的情况下有效地加速DPM，但仍然会将实时性能降低2倍[38]。与神经网络方法相比，DPM相对低的检测精度也受到限制。&lt;/p&gt;

&lt;p&gt;R-CNN minus R replaces Selective Search with static bounding box proposals [20]. While it is much faster than R-CNN, it still falls short of real-time and takes a significant accuracy hit from not having good proposals.&lt;/p&gt;

&lt;p&gt;减去R的R-CNN用静态边界框提出取代选择性搜索[20]。虽然速度比R-CNN更快，但仍然不能实时，并且由于没有好的边界框提出，准确性受到了严重影响。&lt;/p&gt;

&lt;p&gt;Fast R-CNN speeds up the classification stage of R-CNN but it still relies on selective search which can take around 2 seconds per image to generate bounding box proposals. Thus it has high mAP but at 0.5 fps it is still far from real-time.&lt;/p&gt;

&lt;p&gt;快速R-CNN加快了R-CNN的分类阶段，但是仍然依赖选择性搜索，每张图像需要花费大约2秒来生成边界框提出。因此，它具有很高的mAP，但是0.5的fps仍离实时性很远。&lt;/p&gt;

&lt;p&gt;The recent Faster R-CNN replaces selective search with a neural network to propose bounding boxes, similar to Szegedy et al. [8]. In our tests, their most accurate model achieves 7 fps while a smaller, less accurate one runs at 18 fps. The VGG-16 version of Faster R-CNN is 10 mAP higher but is also 6 times slower than YOLO. The Zeiler-Fergus Faster R-CNN is only 2.5 times slower than YOLO but is also less accurate.&lt;/p&gt;

&lt;p&gt;最近更快的R-CNN用神经网络替代了选择性搜索来提出边界框，类似于Szegedy等[8]。在我们的测试中，他们最精确的模型达到了7fps，而较小的，不太精确的模型以18fps运行。VGG-16版本的Faster R-CNN要高出10mAP，但比YOLO慢6倍。Zeiler-Fergus的Faster R-CNN只比YOLO慢了2.5倍，但也不太准确。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/yolo/4.png&quot; alt=&quot;4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;表1：Pascal VOC 2007上的实时系统。&lt;/strong&gt;比较快速检测器的性能和速度。快速YOLO是Pascal VOC检测记录中速度最快的检测器，其精度仍然是其它实时检测器的两倍。YOLO比快速版本更精确10mAP，同时在速度上仍保持实时性。&lt;/p&gt;

&lt;h4 id=&quot;42-voc-2007-error-analysis&quot;&gt;4.2. VOC 2007 Error Analysis&lt;/h4&gt;
&lt;p&gt;To further examine the differences between YOLO and state-of-the-art detectors, we look at a detailed breakdown of results on VOC 2007. We compare YOLO to Fast R-CNN since Fast R-CNN is one of the highest performing detectors on PASCAL and it’s detections are publicly available.&lt;/p&gt;

&lt;h4 id=&quot;42-voc-2007错误分析&quot;&gt;4.2. VOC 2007错误分析&lt;/h4&gt;
&lt;p&gt;为了进一步检查YOLO和最先进的检测器之间的差异，我们详细分析了VOC 2007的结果。我们将YOLO与Fast R-CNN进行比较，因为Fast R-CNN是PASCAL上性能最高的检测器之一并且它的检测代码是可公开得到的。&lt;/p&gt;

&lt;p&gt;We use the methodology and tools of Hoiem et al. [19] For each category at test time we look at the top N predictions for that category. Each prediction is either correct or it is classified based on the type of error:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Correct: correct class and IOU &amp;gt;.5&lt;/li&gt;
  &lt;li&gt;Localization: correct class, .1&amp;lt;IOU&amp;lt;.5&lt;/li&gt;
  &lt;li&gt;Similar: class is similar, IOU &amp;gt;.1&lt;/li&gt;
  &lt;li&gt;Other: class is wrong, IOU &amp;gt;.1&lt;/li&gt;
  &lt;li&gt;Background: IOU &amp;lt;.1 for any object&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Figure 4 shows the breakdown of each error type averaged across all 20 classes.&lt;/p&gt;

&lt;p&gt;我们使用Hoiem等人[19]的方法和工具。对于测试时的每个类别，我们看这个类别的前N个预测。每个预测或者是正确的，或者根据错误类型进行分类：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Correct：正确的类别且IOU&amp;gt;0.5。&lt;/li&gt;
  &lt;li&gt;Localization：正确的类别，0.1&amp;lt;IOU&amp;lt;0.5。&lt;/li&gt;
  &lt;li&gt;Similar：类别相似，IOU &amp;gt;0.1。&lt;/li&gt;
  &lt;li&gt;Other：类别错误，IOU &amp;gt;0.1。&lt;/li&gt;
  &lt;li&gt;Background：任何IOU &amp;lt;0.1的目标。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;图4显示了在所有的20个类别上每种错误类型平均值的分解图。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/yolo/5.png&quot; alt=&quot;5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 4: Error Analysis: Fast R-CNN vs. YOLO&lt;/strong&gt; These charts show the percentage of localization and background errors in the top N detections for various categories (N = # objects in that category).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;图4，误差分析：Fast R-CNN vs. YOLO。&lt;/strong&gt;这些图显示了各种类别的前N个预测中定位错误和背景错误的百分比（N = #表示目标在那个类别中）。&lt;/p&gt;

&lt;p&gt;YOLO struggles to localize objects correctly. Localization errors account for more of YOLO’s errors than all other sources combined. Fast R-CNN makes much fewer localization errors but far more background errors. 13.6% of it’s top detections are false positives that don’t contain any objects. Fast R-CNN is almost 3x more likely to predict background detections than YOLO.&lt;/p&gt;

&lt;p&gt;YOLO努力地正确定位目标。定位错误占YOLO错误的大多数，比其它错误源加起来都多。Fast R-CNN使定位错误少得多，但背景错误更多。它的检测的13.6%是不包含任何目标的误报。Fast R-CNN比YOLO预测背景检测的可能性高出近3倍。&lt;/p&gt;

&lt;h4 id=&quot;43-combining-fast-r-cnn-and-yolo&quot;&gt;4.3. Combining Fast R-CNN and YOLO&lt;/h4&gt;
&lt;p&gt;YOLO makes far fewer background mistakes than Fast R-CNN. By using YOLO to eliminate background detections from Fast R-CNN we get a significant boost in performance. For every bounding box that R-CNN predicts we check to see if YOLO predicts a similar box. If it does, we give that prediction a boost based on the probability predicted by YOLO and the overlap between the two boxes.&lt;/p&gt;

&lt;h4 id=&quot;43-结合fast-r-cnn和yolo&quot;&gt;4.3. 结合Fast R-CNN和YOLO&lt;/h4&gt;
&lt;p&gt;YOLO比Fast R-CNN的背景误检要少得多。通过使用YOLO消除Fast R-CNN的背景检测，我们获得了显著的性能提升。对于R-CNN预测的每个边界框，我们检查YOLO是否预测一个类似的框。如果是这样，我们根据YOLO预测的概率和两个盒子之间的重叠来对这个预测进行提升。&lt;/p&gt;

&lt;p&gt;The best Fast R-CNN model achieves a mAP of 71.8% on the VOC 2007 test set. When combined with YOLO, its mAP increases by 3.2% to 75.0%. We also tried combining the top Fast R-CNN model with several other versions of Fast R-CNN. Those ensembles produced small increases in mAP between .3 and .6%, see Table 2 for details.&lt;/p&gt;

&lt;p&gt;最好的Fast R-CNN模型在VOC 2007测试集上达到了71.8%的mAP。当与YOLO结合时，其mAP增加了3.2%达到了75.0%。我们也尝试将最好的Fast R-CNN模型与其它几个版本的Fast R-CNN结合起来。这些模型组合产生了0.3到0.6%之间的小幅增加，详见表2。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/yolo/6.png&quot; alt=&quot;6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;**Table 2: Model combination experiments on VOC 2007. We examine the effect of combining various models with the best version of Fast R-CNN. Other versions of Fast R-CNN provide only a small benefit while YOLO provides a significant performance boost.&lt;/p&gt;

&lt;p&gt;表2：VOC 2007模型组合实验。我们检验了各种模型与Fast R-CNN最佳版本结合的效果。Fast R-CNN的其它版本只提供很小的好处，而YOLO则提供了显著的性能提升。&lt;/p&gt;

&lt;p&gt;The boost from YOLO is not simply a byproduct of model ensembling since there is little benefit from combining different versions of Fast R-CNN. Rather, it is precisely because YOLO makes different kinds of mistakes at test time that it is so effective at boosting Fast R-CNN’s performance.&lt;/p&gt;

&lt;p&gt;来自YOLO的提升不仅仅是模型组合的副产品，因为组合不同版本的Fast R-CNN几乎没有什么好处。相反，正是因为YOLO在测试时出现了各种各样的错误，所以在提高Fast R-CNN的性能方面非常有效。&lt;/p&gt;

&lt;p&gt;Unfortunately, this combination doesn’t benefit from the speed of YOLO since we run each model seperately and then combine the results. However, since YOLO is so fast it doesn’t add any significant computational time compared to Fast R-CNN.&lt;/p&gt;

&lt;p&gt;遗憾的是，这个组合并没有从YOLO的速度中受益，因为我们分别运行每个模型，然后结合结果。但是，由于YOLO速度如此之快，与Fast R-CNN相比，不会增加任何显著的计算时间。&lt;/p&gt;

&lt;h4 id=&quot;44-voc-2012-results&quot;&gt;4.4. VOC 2012 Results&lt;/h4&gt;
&lt;p&gt;On the VOC 2012 test set, YOLO scores 57.9% mAP. This is lower than the current state of the art, closer to the original R-CNN using VGG-16, see Table 3. Our system struggles with small objects compared to its closest competitors. On categories like &lt;code class=&quot;highlighter-rouge&quot;&gt;bottle&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;sheep&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;tv/monitor&lt;/code&gt; YOLO scores 8−10% lower than R-CNN or Feature Edit. However, on other categories like &lt;code class=&quot;highlighter-rouge&quot;&gt;cat&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;train&lt;/code&gt; YOLO achieves higher performance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/yolo/7.png&quot; alt=&quot;7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table 3: PASCAL VOC 2012 Leaderboard.&lt;/strong&gt; YOLO compared with the full comp4 (outside data allowed) public leaderboard as of November 6th, 2015. Mean average precision and per-class average precision are shown for a variety of detection methods. YOLO is the only real-time detector. Fast R-CNN + YOLO is the forth highest scoring method, with a 2.3% boost over Fast R-CNN.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;表3：PASCAL VOC 2012排行榜。&lt;/strong&gt;截至2015年11月6日，YOLO与完整comp4（允许外部数据）公开排行榜进行了比较。显示了各种检测方法的平均精度均值和每类的平均精度。YOLO是唯一的实时检测器。Fast R-CNN + YOLO是评分第四高的方法，比Fast R-CNN提升了2.3％。&lt;/p&gt;

&lt;p&gt;Our combined Fast R-CNN + YOLO model is one of the highest performing detection methods. Fast R-CNN gets a 2.3% improvement from the combination with YOLO, boosting it 5 spots up on the public leaderboard.&lt;/p&gt;

&lt;p&gt;我们联合的Fast R-CNN + YOLO模型是性能最高的检测方法之一。Fast R-CNN从与YOLO的组合中获得了2.3%的提高，在公开排行榜上上移了5位。&lt;/p&gt;

&lt;h4 id=&quot;45-generalizability-person-detection-in-artwork&quot;&gt;4.5. Generalizability: Person Detection in Artwork&lt;/h4&gt;
&lt;p&gt;Academic datasets for object detection draw the training and testing data from the same distribution. In real-world applications it is hard to predict all possible use cases and the test data can diverge from what the system has seen before [3]. We compare YOLO to other detection systems on the Picasso Dataset [12] and the People-Art Dataset [3], two datasets for testing person detection on artwork.&lt;/p&gt;

&lt;h4 id=&quot;45-泛化能力艺术品中的行人检测&quot;&gt;4.5. 泛化能力：艺术品中的行人检测&lt;/h4&gt;
&lt;p&gt;用于目标检测的学术数据集以相同分布获取训练和测试数据。在现实世界的应用中，很难预测所有可能的用例，而且测试数据可能与系统之前看到的不同[3]。我们在Picasso数据集上[12]和People-Art数据集[3]上将YOLO与其它的检测系统进行比较，这两个数据集用于测试艺术品中的行人检测。&lt;/p&gt;

&lt;p&gt;Figure 5 shows comparative performance between YOLO and other detection methods. For reference, we give VOC 2007 detection AP on person where all models are trained only on VOC 2007 data. On Picasso models are trained on VOC 2012 while on People-Art they are trained on VOC 2010.&lt;/p&gt;

&lt;p&gt;图5显示了YOLO和其它检测方法之间的比较性能。作为参考，我们在person上提供VOC 2007的检测AP，其中所有模型仅在VOC 2007数据上训练。在Picasso数据集上的模型在VOC 2012上训练，而People-Art数据集上的模型则在VOC 2010上训练。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/yolo/8.png&quot; alt=&quot;8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 5: Generalization results on Picasso and People-Art datasets.&lt;/p&gt;

&lt;p&gt;图5：Picasso和People-Art数据集上的泛化结果。&lt;/p&gt;

&lt;p&gt;R-CNN has high AP on VOC 2007. However, R-CNN drops off considerably when applied to artwork. R-CNN uses Selective Search for bounding box proposals which is tuned for natural images. The classifier step in R-CNN only sees small regions and needs good proposals.&lt;/p&gt;

&lt;p&gt;R-CNN在VOC 2007上有高AP。然而，当应用于艺术品时，R-CNN明显下降。R-CNN使用选择性搜索来调整自然图像的边界框提出。R-CNN中的分类器步骤只能看到小区域，并且需要很好的边界框提出。&lt;/p&gt;

&lt;p&gt;DPM maintains its AP well when applied to artwork. Prior work theorizes that DPM performs well because it has strong spatial models of the shape and layout of objects. Though DPM doesn’t degrade as much as R-CNN, it starts from a lower AP.&lt;/p&gt;

&lt;p&gt;DPM在应用于艺术品时保持了其AP。之前的工作认为DPM表现良好，因为它具有目标形状和布局的强大空间模型。虽然DPM不会像R-CNN那样退化，但它开始时的AP较低。&lt;/p&gt;

&lt;p&gt;YOLO has good performance on VOC 2007 and its AP degrades less than other methods when applied to artwork. Like DPM, YOLO models the size and shape of objects, as well as relationships between objects and where objects commonly appear. Artwork and natural images are very different on a pixel level but they are similar in terms of the size and shape of objects, thus YOLO can still predict good bounding boxes and detections.&lt;/p&gt;

&lt;p&gt;YOLO在VOC 2007上有很好的性能，在应用于艺术品时其AP下降低于其它方法。像DPM一样，YOLO建模目标的大小和形状，以及目标和目标通常出现的位置之间的关系。艺术品和自然图像在像素级别上有很大不同，但是它们在目标的大小和形状方面是相似的，因此YOLO仍然可以预测好的边界框和检测结果。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/yolo/9.png&quot; alt=&quot;9&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 6: Qualitative Results.&lt;/strong&gt; YOLO running on sample artwork and natural images from the internet. It is mostly accurate although it does think one person is an airplane.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;图6：定性结果。&lt;/strong&gt; YOLO在网络采样的艺术品和自然图像上的运行结果。虽然它将人误检成了飞机，但它大部分上是准确的。&lt;/p&gt;

&lt;h3 id=&quot;5-real-time-detection-in-the-wild&quot;&gt;5. Real-Time Detection In The Wild&lt;/h3&gt;
&lt;p&gt;YOLO is a fast, accurate object detector, making it ideal for computer vision applications. We connect YOLO to a webcam and verify that it maintains real-time performance, including the time to fetch images from the camera and display the detections.&lt;/p&gt;

&lt;h4 id=&quot;5-现实环境下的实时检测&quot;&gt;5. 现实环境下的实时检测&lt;/h4&gt;
&lt;p&gt;YOLO是一种快速，精确的目标检测器，非常适合计算机视觉应用。我们将YOLO连接到网络摄像头，并验证它是否能保持实时性能，包括从摄像头获取图像并显示检测结果的时间。&lt;/p&gt;

&lt;p&gt;The resulting system is interactive and engaging. While YOLO processes images individually, when attached to a webcam it functions like a tracking system, detecting objects as they move around and change in appearance. A demo of the system and the source code can be found on our project website: &lt;a href=&quot;http://pjreddie.com/yolo/&quot;&gt;http://pjreddie.com/yolo/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;由此产生的系统是交互式和参与式的。虽然YOLO单独处理图像，但当连接到网络摄像头时，其功能类似于跟踪系统，可在目标移动和外观变化时检测目标。系统演示和源代码可以在我们的项目网站上找到：http://pjreddie.com/yolo/。&lt;/p&gt;

&lt;h3 id=&quot;6-conclusion&quot;&gt;6. Conclusion&lt;/h3&gt;
&lt;p&gt;We introduce YOLO, a unified model for object detection. Our model is simple to construct and can be trained directly on full images. Unlike classifier-based approaches, YOLO is trained on a loss function that directly corresponds to detection performance and the entire model is trained jointly.&lt;/p&gt;

&lt;h4 id=&quot;6-结论&quot;&gt;6. 结论&lt;/h4&gt;
&lt;p&gt;我们介绍了YOLO，一种统一的目标检测模型。我们的模型构建简单，可以直接在整张图像上进行训练。与基于分类器的方法不同，YOLO直接在对应检测性能的损失函数上训练，并且整个模型联合训练。&lt;/p&gt;

&lt;p&gt;Fast YOLO is the fastest general-purpose object detector in the literature and YOLO pushes the state-of-the-art in real-time object detection. YOLO also generalizes well to new domains making it ideal for applications that rely on fast, robust object detection.&lt;/p&gt;

&lt;p&gt;快速YOLO是文献中最快的通用目的的目标检测器，YOLO推动了实时目标检测的最新技术。YOLO还很好地泛化到新领域，使其成为依赖快速，强大的目标检测应用的理想选择。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Acknowledgements:&lt;/strong&gt; This work is partially supported by ONR N00014-13-1-0720, NSF IIS-1338054, and The Allen Distinguished Investigator Award.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;致谢：&lt;/strong&gt;这项工作得到了ONR N00014-13-1-0720，NSF IIS-1338054和艾伦杰出研究者奖的部分支持。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;References
[1] M. B. Blaschko and C. H. Lampert. Learning to localize objects with structured output regression. In Computer Vision–ECCV 2008, pages 2–15. Springer, 2008. 4&lt;/p&gt;

&lt;p&gt;[2] L. Bourdev and J. Malik. Poselets: Body part detectors trained using 3d human pose annotations. In International Conference on Computer Vision (ICCV), 2009. 8&lt;/p&gt;

&lt;p&gt;[3] H. Cai, Q. Wu, T. Corradi, and P. Hall. The cross-depiction problem: Computer vision algorithms for recognising objects in artwork and in photographs. arXiv preprint arXiv:1505.00110, 2015. 7&lt;/p&gt;

&lt;p&gt;[4] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 886–893. IEEE, 2005. 4, 8&lt;/p&gt;

&lt;p&gt;[5] T. Dean, M. Ruzon, M. Segal, J. Shlens, S. Vijaya-narasimhan, J. Yagnik, et al. Fast, accurate detection of 100,000 object classes on a single machine. In Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pages 1814–1821. IEEE, 2013. 5&lt;/p&gt;

&lt;p&gt;[6] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. arXiv preprint arXiv:1310.1531, 2013. 4&lt;/p&gt;

&lt;p&gt;[7] J. Dong, Q. Chen, S. Yan, and A. Yuille. Towards unified object detection and semantic segmentation. In Computer Vision–ECCV 2014, pages 299–314. Springer, 2014. 7&lt;/p&gt;

&lt;p&gt;[8] D.Erhan, C.Szegedy, A.Toshev, and D.Anguelov. Scalable object detection using deep neural networks. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 2155–2162. IEEE, 2014. 5, 6&lt;/p&gt;

&lt;p&gt;[9] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object classes challenge: A retrospective. International Journal of Computer Vision, 111(1):98–136, Jan. 2015. 2&lt;/p&gt;

&lt;p&gt;[10] P.F.Felzenszwalb, R.B.Girshick, D.McAllester, and D.Ramanan. Object detection with discriminatively trained part based models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(9):1627–1645, 2010. 1, 4&lt;/p&gt;

&lt;p&gt;[11] S. Gidaris and N. Komodakis. Object detection via a multi-region &amp;amp; semantic segmentation-aware CNN model. CoRR, abs/1505.01749, 2015. 7&lt;/p&gt;

&lt;p&gt;[12] S. Ginosar, D. Haas, T. Brown, and J. Malik. Detecting people in cubist art. In Computer Vision-ECCV 2014 Workshops, pages 101–116. Springer, 2014. 7&lt;/p&gt;

&lt;p&gt;[13] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 580–587. IEEE, 2014. 1, 4, 7&lt;/p&gt;

&lt;p&gt;[14] R. B. Girshick. Fast R-CNN. CoRR, abs/1504.08083, 2015. 2, 5, 6, 7&lt;/p&gt;

&lt;p&gt;[15] S. Gould, T. Gao, and D. Koller. Region-based segmentation and object detection. In Advances in neural information processing systems, pages 655–663, 2009. 4&lt;/p&gt;

&lt;p&gt;[16] B. Hariharan, P. Arbeláez, R. Girshick, and J. Malik. Simultaneous detection and segmentation. In Computer Vision–ECCV 2014, pages 297–312. Springer, 2014. 7&lt;/p&gt;

&lt;p&gt;[17] K.He, X.Zhang, S.Ren, and J.Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. arXiv preprint arXiv:1406.4729, 2014. 5&lt;/p&gt;

&lt;p&gt;[18] G.E.Hinton, N.Srivastava, A.Krizhevsky, I.Sutskever, and R. R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012. 4&lt;/p&gt;

&lt;p&gt;[19] D.Hoiem, Y.Chodpathumwan, and Q.Dai. Diagnosing error in object detectors. In Computer Vision–ECCV 2012, pages 340–353. Springer, 2012. 6&lt;/p&gt;

&lt;p&gt;[20] K. Lenc and A. Vedaldi. R-cnn minus r. arXiv preprint arXiv:1506.06981, 2015. 5, 6&lt;/p&gt;

&lt;p&gt;[21] R. Lienhart and J. Maydt. An extended set of haar-like features for rapid object detection. In Image Processing. 2002. Proceedings. 2002
International Conference on, volume 1, pages I–900. IEEE, 2002. 4&lt;/p&gt;

&lt;p&gt;[22] M. Lin, Q. Chen, and S. Yan. Network in network. CoRR, abs/1312.4400, 2013. 2&lt;/p&gt;

&lt;p&gt;[23] D. G. Lowe. Object recognition from local scale-invariant features. In Computer vision, 1999. The proceedings of the seventh IEEE international conference on, volume 2, pages 1150–1157. Ieee, 1999. 4&lt;/p&gt;

&lt;p&gt;[24] D. Mishkin. Models accuracy on imagenet 2012 val. https://github.com/BVLC/caffe/wiki/ Models-accuracy-on-ImageNet-2012-val. Accessed: 2015-10-2. 3&lt;/p&gt;

&lt;p&gt;[25] C. P. Papageorgiou, M. Oren, and T. Poggio. A general framework for object detection. In Computer vision, 1998. sixth international conference on, pages 555–562. IEEE, 1998. 4&lt;/p&gt;

&lt;p&gt;[26] J. Redmon. Darknet: Open source neural networks in c. http://pjreddie.com/darknet/, 2013–2016. 3&lt;/p&gt;

&lt;p&gt;[27] J.Redmon and A.Angelova. Real-time grasp detection using convolutional neural networks. CoRR, abs/1412.3128, 2014. 5&lt;/p&gt;

&lt;p&gt;[28] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. arXiv preprint arXiv:1506.01497, 2015. 5, 6, 7&lt;/p&gt;

&lt;p&gt;[29] S. Ren, K. He, R. B. Girshick, X. Zhang, and J. Sun. Object detection networks on convolutional feature maps. CoRR, abs/1504.06066, 2015. 3, 7&lt;/p&gt;

&lt;p&gt;[30] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 2015. 3&lt;/p&gt;

&lt;p&gt;[31] M. A. Sadeghi and D. Forsyth. 30hz object detection with dpm v5. In Computer Vision–ECCV 2014, pages 65–79. Springer, 2014. 5, 6&lt;/p&gt;

&lt;p&gt;[32] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. CoRR, abs/1312.6229, 2013. 4, 5&lt;/p&gt;

&lt;p&gt;[33] Z.Shen and X.Xue. Do more dropouts in pool5 feature maps for better object detection. arXiv preprint arXiv:1409.6911, 2014. 7&lt;/p&gt;

&lt;p&gt;[34] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. CoRR, abs/1409.4842, 2014. 2&lt;/p&gt;

&lt;p&gt;[35] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeulders. Selective search for object recognition. International journal of computer vision, 104(2):154–171, 2013. 4, 5&lt;/p&gt;

&lt;p&gt;[36] P. Viola and M. Jones. Robust real-time object detection. International Journal of Computer Vision, 4:34–47, 2001. 4&lt;/p&gt;

&lt;p&gt;[37] P. Viola and M. J. Jones. Robust real-time face detection. International journal of computer vision, 57(2):137–154, 2004. 5&lt;/p&gt;

&lt;p&gt;[38] J. Yan, Z. Lei, L. Wen, and S. Z. Li. The fastest deformable part model for object detection. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 2497–2504. IEEE, 2014. 5, 6&lt;/p&gt;

&lt;p&gt;[39] C.L.Zitnick and P.Dollár.Edgeboxes:Locating object proposals from edges. In Computer Vision–ECCV 2014, pages 391–405. Springer, 2014. 4&lt;/p&gt;

</description>
        <pubDate>Mon, 31 Dec 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/12/yolo/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/12/yolo/</guid>
        
        <category>深度学习-视觉</category>
        
        
        <category>深度学习-视觉</category>
        
      </item>
    
      <item>
        <title>SSD</title>
        <description>&lt;!--more--&gt;

&lt;p&gt;标签： 深度学习-视觉&lt;/p&gt;

&lt;h1 id=&quot;ssd&quot;&gt;SSD&lt;/h1&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;ssd-single-shot-multibox-detector&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1512.02325&quot;&gt;SSD: Single Shot MultiBox Detector&lt;/a&gt;&lt;/h3&gt;

&lt;h4 id=&quot;ssd单发多盒检测器&quot;&gt;SSD:单发多盒检测器&lt;/h4&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;abstract&quot;&gt;Abstract.&lt;/h3&gt;
&lt;h4 id=&quot;摘要&quot;&gt;摘要&lt;/h4&gt;
&lt;p&gt;We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines pre- dictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into sys- tems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For 300 × 300 in- put, SSD achieves 74.3% mAP1 on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512 × 512 input, SSD achieves 76.9% mAP, outperforming a compa- rable state-of-the-art Faster R-CNN model. Compared to other single stage meth- ods, SSD has much better accuracy even with a smaller input image size. Code is available at: https://github.com/weiliu89/caffe/tree/ssd .&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;我们提出了一种使用单个深度神经网络来检测图像中的目标的方法。我们的方法命名为SSD，将边界框的输出空间离散化为不同长宽比的一组默认框和并缩放每个特征映射的位置。在预测时，网络会在每个默认框中为每个目标类别的出现生成分数，并对框进行调整以更好地匹配目标形状。此外，网络还结合了不同分辨率的多个特征映射的预测，自然地处理各种尺寸的目标。相对于需要目标候选（object proposals）的方法，SSD非常简单，因为它完全消除了提出生成和随后的像素或特征重新采样阶段，并将所有计算封装到单个网络中。这使得SSD易于训练和直接集成到需要检测组件的系统中。PASCAL VOC，COCO和ILSVRC数据集上的实验结果证实，SSD对于利用额外的目标提出步骤的方法具有竞争性的准确性，并且速度更快，同时为训练和推断提供了统一的框架。对于300×300的输入，SSD在VOC2007测试中以59FPS的速度在Nvidia Titan X上达到74.3%的mAP，对于512×512的输入，SSD达到了76.9%的mAP，优于参照的最先进的Faster R-CNN模型。与其他单阶段方法相比，即使输入图像尺寸较小，SSD也具有更高的精度。代码获取：https://github.com/weiliu89/caffe/tree/ssd。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Keywords: Real-time Object Detection; Convolutional Neural Network&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;关键词： 实时目标检测；卷积神经网络&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;1-introduction&quot;&gt;1 Introduction&lt;/h3&gt;
&lt;h4 id=&quot;1-引言&quot;&gt;1. 引言&lt;/h4&gt;

&lt;p&gt;Current state-of-the-art object detection systems are variants of the following approach: hypothesize bounding boxes, resample pixels or features for each box, and apply a high- quality classifier. This pipeline has prevailed on detection benchmarks since the Selec- tive Search work [1] through the current leading results on PASCAL VOC, COCO, and ILSVRC detection all based on Faster R-CNN[2] albeit with deeper features such as [3]. While accurate, these approaches have been too computationally intensive for em- bedded systems and, even with high-end hardware, too slow for real-time applications.Often detection speed for these approaches is measured in seconds per frame (SPF), and even the fastest high-accuracy detector, Faster R-CNN, operates at only 7 frames per second (FPS). There have been many attempts to build faster detectors by attacking each stage of the detection pipeline (see related work in Sec. 4), but so far, significantly increased speed comes only at the cost of significantly decreased detection accuracy.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;目前最先进的目标检测系统是以下方法的变种：假设边界框（hypothesize bounding boxes），每个框重采样像素或特征，并应用一个高质量的分类器。自从选择性搜索[1]通过在PASCAL VOC，COCO和ILSVRC上所有基于Faster R-CNN[2]的检测都取得了当前领先的结果（尽管具有更深的特征如[3]），这种流程在检测基准数据上流行开来。尽管这些方法准确，但对于嵌入式系统而言，这些方法的计算量过大，即使是高端硬件，对于实时应用而言也太慢。通常，这些方法的检测速度是以每帧秒（SPF）度量，甚至最快的高精度检测器，Faster R-CNN，仅以每秒7帧（FPS）的速度运行。已经有很多尝试通过处理检测流程中的每个阶段来构建更快的检测器（参见第4节中的相关工作），但是到目前为止，显著提高的速度仅以显著降低的检测精度为代价。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This paper presents the first deep network based object detector that does not re- sample pixels or features for bounding box hypotheses and and is as accurate as ap- proaches that do. This results in a significant improvement in speed for high-accuracy detection (59 FPS with mAP 74.3% on VOC2007 test, vs. Faster R-CNN 7 FPS with mAP 73.2% or YOLO 45 FPS with mAP 63.4%). The fundamental improvement in speed comes from eliminating bounding box proposals and the subsequent pixel or fea- ture resampling stage. We are not the first to do this (cf [4,5]), but by adding a series of improvements, we manage to increase the accuracy significantly over previous at- tempts. Our improvements include using a small convolutional filter to predict object categories and offsets in bounding box locations, using separate predictors (filters) for different aspect ratio detections, and applying these filters to multiple feature maps from the later stages of a network in order to perform detection at multiple scales. With these modifications—especially using multiple layers for prediction at different scales—we can achieve high-accuracy using relatively low resolution input, further increasing de- tection speed. While these contributions may seem small independently, we note that the resulting system improves accuracy on real-time detection for PASCAL VOC from 63.4% mAP for YOLO to 74.3% mAP for our SSD. This is a larger relative improve- ment in detection accuracy than that from the recent, very high-profile work on residual networks [3]. Furthermore, significantly improving the speed of high-quality detection can broaden the range of settings where computer vision is useful.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;本文提出了第一个基于深度网络的目标检测器，它不对边界框假设的像素或特征进行重采样，并且与其它方法有一样精确度。这对高精度检测在速度上有显著提高（在VOC2007测试中，59FPS和74.3%的mAP，与Faster R-CNN 7FPS和73.2%的mAP或者YOLO 45 FPS和63.4%的mAP相比）。速度的根本改进来自消除边界框提出和随后的像素或特征重采样阶段。我们并不是第一个这样做的人（查阅[4,5]），但是通过增加一系列改进，我们设法比以前的尝试显著提高了准确性。我们的改进包括使用小型卷积滤波器来预测边界框位置中的目标类别和偏移量，使用不同长宽比检测的单独预测器（滤波器），并将这些滤波器应用于网络后期的多个特征映射中，以执行多尺度检测。通过这些修改——特别是使用多层进行不同尺度的预测——我们可以使用相对较低的分辨率输入实现高精度，进一步提高检测速度。虽然这些贡献可能单独看起来很小，但是我们注意到由此产生的系统将PASCAL VOC实时检测的准确度从YOLO的63.4%的mAP提高到我们的SSD的74.3%的mAP。相比于最近备受瞩目的残差网络方面的工作[3]，在检测精度上这是相对更大的提高。而且，显著提高的高质量检测速度可以扩大计算机视觉使用的设置范围。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We summarize our contributions as follows:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;我们总结我们的贡献如下：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We introduce SSD, a single-shot detector for multiple categories that is faster than the previous state-of-the-art for single shot detectors (YOLO), and significantly more accurate, in fact as accurate as slower techniques that perform explicit region proposals and pooling (including Faster R-CNN).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;我们引入了SSD，这是一种针对多个类别的单次检测器，比先前的先进的单次检测器（YOLO）更快，并且准确得多，事实上，与执行显式区域提出和池化的更慢的技术具有相同的精度（包括Faster R-CNN）。&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;The core of SSD is predicting category scores and box offsets for a fixed set of default bounding boxes using small convolutional filters applied to feature maps.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;SSD的核心是预测固定的一系列默认边界框的类别分数和边界框偏移，使用更小的卷积滤波器应用到特征映射上。&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;To achieve high detection accuracy we produce predictions of different scales from feature maps of different scales, and explicitly separate predictions by aspect ratio.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;为了实现高检测精度，我们根据不同尺度的特征映射生成不同尺度的预测，并通过纵横比明确分开预测。&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;These design features lead to simple end-to-end training and high accuracy, even on low resolution input images, further improving the speed vs accuracy trade-off.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;这些设计功能使得即使在低分辨率输入图像上也能实现简单的端到端训练和高精度，从而进一步提高速度与精度之间的权衡。&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Experiments include timing and accuracy analysis on models with varying input size evaluated on PASCAL VOC, COCO, and ILSVRC and are compared to a range of recent state-of-the-art approaches.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;实验包括在PASCAL VOC，COCO和ILSVRC上评估具有不同输入大小的模型的时间和精度分析，并与最近的一系列最新方法进行比较。&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-the-single-shot-detector-ssd&quot;&gt;2 The Single Shot Detector (SSD)&lt;/h3&gt;
&lt;h4 id=&quot;2-单次检测器&quot;&gt;2 单次检测器&lt;/h4&gt;

&lt;p&gt;This section describes our proposed SSD framework for detection (Sec. 2.1) and the associated training methodology (Sec. 2.2). Afterwards, Sec. 3 presents dataset-specific model details and experimental results.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;本节描述我们提出的SSD检测框架（2.1节）和相关的训练方法（2.2节）。之后，2.3节介绍了数据集特有的模型细节和实验结果。&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;21-model&quot;&gt;2.1 Model&lt;/h4&gt;
&lt;h4 id=&quot;21-模型&quot;&gt;2.1 模型&lt;/h4&gt;

&lt;p&gt;The SSD approach is based on a feed-forward convolutional network that produces a fixed-size collection of bounding boxes and scores for the presence of object class instances in those boxes, followed by a non-maximum suppression step to produce the final detections. The early network layers are based on a standard architecture used for high quality image classification (truncated before any classification layers), which we will call the base network2. We then add auxiliary structure to the network to produce detections with the following key features:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SSD方法基于前馈卷积网络，该网络产生固定大小的边界框集合，并对这些边界框中存在的目标类别实例进行评分，然后进行非极大值抑制步骤来产生最终的检测结果。早期的网络层基于用于高质量图像分类的标准架构（在任何分类层之前被截断），我们将其称为基础网络。然后，我们将辅助结构添加到网络中以产生具有以下关键特征的检测：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Multi-scale feature maps for detection&lt;/code&gt; We add convolutional feature layers to the end of the truncated base network. These layers decrease in size progressively and allow predictions of detections at multiple scales. The convolutional model for predicting detections is different for each feature layer (cf Overfeat[4] and YOLO[5] that operate on a single scale feature map).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;用于检测的多尺度特征映射&lt;/code&gt;。我们将卷积特征层添加到截取的基础网络的末端。这些层在尺寸上逐渐减小，并允许在多个尺度上对检测结果进行预测。用于预测检测的卷积模型对于每个特征层都是不同的（查阅Overfeat[4]和YOLO[5]在单尺度特征映射上的操作）。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Convolutional predictors for detection&lt;/code&gt; Each added feature layer (or optionally an ex- isting feature layer from the base network) can produce a fixed set of detection predic- tions using a set of convolutional filters. These are indicated on top of the SSD network architecture in Fig. 2. For a feature layer of size m × n with p channels, the basic el- ement for predicting parameters of a potential detection is a 3 × 3 × p small kernel that produces either a score for a category, or a shape offset relative to the default box coordinates. At each of the m × n locations where the kernel is applied, it produces an output value. The bounding box offset output values are measured relative to a default box position relative to each feature map location (cf the architecture of YOLO[5] that uses an intermediate fully connected layer instead of a convolutional filter for this step).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;用于检测的卷积预测器&lt;/code&gt;。每个添加的特征层（或者任选的来自基础网络的现有特征层）可以使用一组卷积滤波器产生固定的检测预测集合。这些在图2中的SSD网络架构的上部指出。对于具有p通道的大小为m×n的特征层，潜在检测的预测参数的基本元素是3×3×p的小核得到某个类别的分数，或者相对于默认框坐标的形状偏移。在应用卷积核的m×n的每个位置，它会产生一个输出值。边界框偏移输出值是相对每个特征映射位置的相对默认框位置来度量的（查阅YOLO[5]的架构，该步骤使用中间全连接层而不是卷积滤波器）。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/ssd/1.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Fig. 2: A comparison between two single shot detection models: SSD and YOLO [5]. Our SSD model adds several feature layers to the end of a base network, which predict the offsets to default boxes of different scales and aspect ratios and their associated confidences. SSD with a 300 × 300 input size significantly outperforms its 448 × 448 YOLO counterpart in accuracy on VOC2007 test while also improving the speed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;图2：两个单次检测模型的比较：SSD和YOLO[5]。我们的SSD模型在基础网络的末端添加了几个特征层，它预测了不同尺度和长宽比的默认边界框的偏移量及其相关的置信度。300×300输入尺寸的SSD在VOC2007 test上的准确度上明显优于448×448的YOLO的准确度，同时也提高了速度。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Default boxes and aspect ratios&lt;/code&gt; We associate a set of default bounding boxes with each feature map cell, for multiple feature maps at the top of the network. The default boxes tile the feature map in a convolutional manner, so that the position of each box relative to its corresponding cell is fixed. At each feature map cell, we predict the offsets relative to the default box shapes in the cell, as well as the per-class scores that indicate the presence of a class instance in each of those boxes. Specifically, for each box out of k at a given location, we compute c class scores and the 4 offsets relative to the original default box shape. This results in a total of (c + 4)k filters that are applied around each location in the feature map, yielding (c + 4)kmn outputs for a m × n feature map. For an illustration of default boxes, please refer to Fig. 1. Our default boxes are similar to the anchor boxes used in Faster R-CNN [2], however we apply them to several feature maps of different resolutions. Allowing different default box shapes in several feature maps let us efficiently discretize the space of possible output box shapes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;默认边界框和长宽比&lt;/code&gt;。对于网络顶部的多个特征映射，我们将一组默认边界框与每个特征映射单元相关联。默认边界框以卷积的方式平铺特征映射，以便每个边界框相对于其对应单元的位置是固定的。在每个特征映射单元中，我们预测单元中相对于默认边界框形状的偏移量，以及指出每个边界框中存在的每个类别实例的类别分数。具体而言，对于给定位置处的$k$个边界框中的每一个，我们计算c个类别分数和相对于原始默认边界框形状的$4$个偏移量。这导致在特征映射中的每个位置周围应用总共$(c+4)k$个滤波器，对于m×n的特征映射取得$(c+4)kmn$个输出。有关默认边界框的说明，请参见图1。我们的默认边界框与Faster R-CNN[2]中使用的锚边界框相似，但是我们将它们应用到不同分辨率的几个特征映射上。在几个特征映射中允许不同的默认边界框形状让我们有效地离散可能的输出框形状的空间。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/ssd/2.png&quot; alt=&quot;2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Fig. 1: SSD framework.&lt;/code&gt; (a) SSD only needs an input image and ground truth boxes for each object during training. In a convolutional fashion, we evaluate a small set (e.g. 4) of default boxes of different aspect ratios at each location in several feature maps with different scales (e.g. 8 × 8 and 4 × 4 in (b) and (c)). For each default box, we predict both the shape offsets and the confidences for all object categories ((c1 , c2 , · · · , cp )). At training time, we first match these default boxes to the ground truth boxes. For example, we have matched two default boxes with the cat and one with the dog, which are treated as positives and the rest as negatives. The model loss is a weighted sum between localization loss (e.g. Smooth L1 [6]) and confidence loss (e.g. Softmax).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;图1：SSD框架&lt;/code&gt;。（a）在训练期间，SSD仅需要每个目标的输入图像和真实边界框。以卷积方式，我们评估具有不同尺度（例如（b）和（c）中的8×8和4×4）的几个特征映射中每个位置处不同长宽比的默认框的小集合（例如4个）。对于每个默认边界框，我们预测所有目标类别$（(c1,c2,…,cp)）$的形状偏移量和置信度。在训练时，我们首先将这些默认边界框与实际的边界框进行匹配。例如，我们已经与猫匹配两个默认边界框，与狗匹配了一个，这被视为积极的，其余的是消极的。模型损失是&lt;code class=&quot;highlighter-rouge&quot;&gt;定位损失&lt;/code&gt;（例如，Smooth L1[6]）和&lt;code class=&quot;highlighter-rouge&quot;&gt;置信度损失&lt;/code&gt;（例如Softmax）之间的加权和。&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;22-training&quot;&gt;2.2 Training&lt;/h4&gt;
&lt;h4 id=&quot;22-训练&quot;&gt;2.2 训练&lt;/h4&gt;

&lt;p&gt;The key difference between training SSD and training a typical detector that uses region proposals, is that ground truth information needs to be assigned to specific outputs in the fixed set of detector outputs. Some version of this is also required for training in YOLO[5] and for the region proposal stage of Faster R-CNN[2] and MultiBox[7]. Once this assignment is determined, the loss function and back propagation are applied end- to-end. Training also involves choosing the set of default boxes and scales for detection as well as the hard negative mining and data augmentation strategies.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;训练SSD和训练使用区域提出(region proposal)的典型检测器之间的关键区别在于，需要将真实信息分配给固定的检测器输出集合中的特定输出。在YOLO[5]的训练中、Faster R-CNN[2]和MultiBox[7]的区域提出阶段，一些版本也需要这样的操作。一旦确定了这个分配，损失函数和反向传播就可以应用端到端了。训练也涉及选择默认边界框集合和缩放进行检测，以及难例挖掘和数据增强策略。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Matching strategy``` During training we need to determine which default boxes corre- spond to a ground truth detection and train the network accordingly. For each ground truth box we are selecting from default boxes that vary over location, aspect ratio, and scale. We begin by matching each ground truth box to the default box with the best jaccard overlap (as in MultiBox [7]). Unlike MultiBox, we then match default boxes to any ground truth with jaccard overlap higher than a threshold (0.5). This simplifies the learning problem, allowing the network to predict high scores for multiple overlapping default boxes rather than requiring it to pick only the one with maximum overlap.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;匹配策略&lt;/code&gt;。在训练过程中，我们需要确定哪些默认边界框对应实际边界框的检测，并相应地训练网络。对于每个实际边界框，我们从默认边界框中选择，这些框会在位置，长宽比和尺度上变化。我们首先将每个实际边界框与具有最好的Jaccard重叠（如MultiBox[7]）的边界框相匹配。与MultiBox不同的是，&lt;code class=&quot;highlighter-rouge&quot;&gt;我们将默认边界框匹配到Jaccard重叠高于阈值（0.5）的任何实际边界框&lt;/code&gt;。这简化了学习问题，允许网络为多个重叠的默认边界框预测高分，而不是要求它只挑选具有最大重叠的一个边界框。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Training objective The SSD training objective is derived from the MultiBox objective[7,8]but is extended to handle multiple object categories. Let $x_{ij}^p = \lbrace 1,0 \rbrace$ be an indicator for matching the $i$-th default box to the $j$-th ground truth box of category $p$. In the matching strategy above, we can have  $\sum_i x_{ij}^p \geq 1$. The overall objective loss function is a weighted sum of the localization loss (loc) and the confidence loss (conf):
&lt;strong&gt;训练目标函数。SSD训练目标函数来自于MultiBox目标[7,8]，但扩展到处理多个目标类别。设$x_{ij}^p = \lbrace 1,0 \rbrace$是第$i$个默认边界框匹配到类别$p$的第$j$个实际边界框的指示器。在上面的匹配策略中，我们有$\sum_i x_{ij}^p \geq 1$。总体目标损失函数是定位损失（loc）和置信度损失（conf）的加权和：&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(x, c, l, g) = \frac{1}{N}(L_{conf}(x, c) + \alpha L_{loc}(x, l, g)) \tag{1}&lt;/script&gt;

&lt;p&gt;where N is the number of matched default boxes. If N = 0, wet set the loss to 0. The localization loss is a Smooth L1 loss [6] between the predicted box (l) and the ground truth box (g) parameters. Similar to Faster R-CNN [2], we regress to offsets for the center (cx, cy) of the default bounding box (d) and for its width (w) and height (h).
&lt;strong&gt;其中$N$是匹配的默认边界框的数量。如果$N=0$，则将损失设为0。定位损失是预测框$(l)$与真实框$(g)$参数之间的Smooth L1损失[6]。类似于Faster R-CNN[2]，我们回归默认边界框$(d)$的中心偏移量$(cx,cy)$和其宽度$(w)$、高度$(h)$的偏移量。&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_{loc}(x,l,g) = \sum_{i \in Pos}^N \sum_{m \in \lbrace cx, cy, w, h \rbrace} x_{ij}^k \mathtt{smooth}_{L1}(l_{i}^m - \hat{g}_j^m) \\ \hat{g}_j^{cx} = (g_j^{cx} - d_i^{cx}) / d_i^w \quad \quad \hat{g}_j^{cy} = (g_j^{cy} - d_i^{cy}) / d_i^h \\ \hat{g}_j^{w} = \log\Big(\frac{g_j^{w}}{d_i^w}\Big) \quad \quad \hat{g}_j^{h} = \log\Big(\frac{g_j^{h}}{d_i^h}\Big)  \tag{2}&lt;/script&gt;

&lt;p&gt;The confidence loss is the softmax loss over multiple classes confidences (c).
&lt;strong&gt;置信度损失是在多类别置信度(c)上的softmax损失。&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_{conf}(x, c) = - \sum_{i\in Pos}^N x_{ij}^p log(\hat{c}_i^p) - \sum_{i\in Neg} log(\hat{c}_i^0)\quad \mathtt{where}\quad\hat{c}_i^p = \frac{\exp(c_i^p)}{\sum_p \exp(c_i^p)} \tag{3}&lt;/script&gt;

&lt;p&gt;and the weight term $α$ is set to 1 by cross validation.
&lt;strong&gt;通过交叉验证权重项$α$设为1。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Choosing scales and aspect ratios for default boxes&lt;/code&gt; To handle different object scales, some methods [4,9] suggest processing the image at different sizes and combining the results afterwards. However, by utilizing feature maps from several different layers in a single network for prediction we can mimic the same effect, while also sharing parame- ters across all object scales. Previous works [10,11] have shown that using feature maps from the lower layers can improve semantic segmentation quality because the lower layers capture more fine details of the input objects. Similarly, [12] showed that adding global context pooled from a feature map can help smooth the segmentation results.Motivated by these methods, we use both the lower and upper feature maps for detec- tion. Figure 1 shows two exemplar feature maps (8 × 8 and 4 × 4) which are used in the framework. In practice, we can use many more with small computational overhead.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;为默认边界框选择尺度和长宽比。为了处理不同的目标尺度，一些方法[4,9]建议处理不同尺寸的图像，然后将结果合并。然而，通过利用单个网络中几个不同层的特征映射进行预测，我们可以模拟相同的效果，同时还可以跨所有目标尺度共享参数。以前的工作[10,11]已经表明，使用低层的特征映射可以提高语义分割的质量，因为低层会捕获输入目标的更多细节。同样，[12]表明，从特征映射上添加全局上下文池化可以有助于平滑分割结果。受这些方法的启发，我们使用较低和较高的特征映射进行检测。图1显示了框架中使用的两个示例性特征映射（8×8和4×4）。在实践中，我们可以使用更多的具有很少计算开支的特征映射。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Feature maps from different levels within a network are known to have different (empirical) receptive field sizes [13]. Fortunately, within the SSD framework, the de- fault boxes do not necessary need to correspond to the actual receptive fields of each layer. We design the tiling of default boxes so that specific feature maps learn to be responsive to particular scales of the objects. Suppose we want to use m feature maps for prediction. The scale of the default boxes for each feature map is computed as:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;已知网络中不同层的特征映射具有不同的（经验的）感受野大小[13]。幸运的是，在SSD框架内，默认边界框不需要对应于每层的实际感受野。我们设计平铺默认边界框，以便特定的特征映射学习响应目标的特定尺度。假设我们要使用$m$个特征映射进行预测。每个特征映射默认边界框的尺度计算如下：&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;s_k = s_\text{min} + \frac{s_\text{max} - s_\text{min}}{m - 1} (k - 1),\quad k\in [1, m]&lt;/script&gt;

&lt;p&gt;where $s_\text{min}$ is 0.2 and $s_\text{max}$ is 0.9, meaning the lowest layer has a scale of 0.2 and the highest layer has a scale of 0.9, and all layers in between are regularly spaced. We impose different aspect ratios for the default boxes, and denote them as $a_r \in {1, 2, 3, \frac{1}{2}, \frac{1}{3}}$. We can compute the width ($w_k^a = s_k\sqrt{a_r}$) and height $h_k^a = s_k / \sqrt{a_r}$ for each default box. For the aspect ratio of 1, we also add a default box whose scale is
$s’&lt;em&gt;k = \sqrt{s_k s&lt;/em&gt;{k+1}}$, resulting in 6 default boxes per feature map location. We set the center of each default box to $(\frac{i+0.5}{|f_k|}, \frac{j+0.5}{|f_k|})$, where $|f_k|$ is the size of the k-th square feature map, $i, j\in [0, |f_k|)$. In practice, one can also design a distribution of default boxes to best fit a specific dataset. How to design the optimal tiling is an open question as well.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;其中$s_\text{min}$为0.2，$s_\text{max}$为0.9，意味着最低层具有0.2的尺度，最高层具有0.9的尺度，并且在它们之间的所有层是规则间隔的。我们为默认边界框添加不同的长宽比，并将它们表示为$a_r \in {1, 2, 3, \frac{1}{2}, \frac{1}{3}}$。我们可以计算每个边界框的宽度($w_k^a = s_k\sqrt{a_r}$)和高度($h_k^a = s_k / \sqrt{a_r}$)。对于长宽比为1，我们还添加了一个默认边界框，其尺度为$s’&lt;em&gt;k = \sqrt{s_k s&lt;/em&gt;{k+1}}$，在每个特征映射位置得到6个默认边界框。我们将每个默认边界框的中心设置为($(\frac{i+0.5}{|f_k|}, \frac{j+0.5}{|f_k|})$)，其中$|f_k|$是第k个平方特征映射的大小， $i, j\in [0, |f_k|)$ 。在实践中，也可以设计默认边界框的分布以最适合特定的数据集。如何设计最佳平铺也是一个悬而未决的问题。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;By combining predictions for all default boxes with different scales and aspect ratios from all locations of many feature maps, we have a diverse set of predictions, covering various input object sizes and shapes. For example, in Fig. 1, the dog is matched to a default box in the 4 × 4 feature map, but not to any default boxes in the 8 × 8 feature map. This is because those boxes have different scales and do not match the dog box, and therefore are considered as negatives during training.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;通过将所有默认边界框的预测与许多特征映射所有位置的不同尺度和高宽比相结合，我们有不同的预测集合，涵盖各种输入目标大小和形状。例如，在图1中，狗被匹配到4×4特征映射中的默认边界框，而不是8×8特征映射中的任何默认框。这是因为那些边界框有不同的尺度，不匹配狗的边界框，因此在训练期间被认为是负例。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Hard negative mining&lt;/code&gt; After the matching step, most of the default boxes are nega- tives, especially when the number of possible default boxes is large. This introduces a significant imbalance between the positive and negative training examples. Instead of using all the negative examples, we sort them using the highest confidence loss for each default box and pick the top ones so that the ratio between the negatives and positives is at most 3:1. We found that this leads to faster optimization and a more stable training.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;难例挖掘&lt;/code&gt;。在匹配步骤之后，大多数默认边界框为负例，尤其是当可能的默认边界框数量较多时。这在正的训练实例和负的训练实例之间引入了显著的不平衡。我们不使用所有负例，而是使用每个默认边界框的最高置信度损失来排序它们，并挑选最高的置信度，以便负例和正例之间的比例至多为3:1。我们发现这会导致更快的优化和更稳定的训练。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Data augmentation&lt;/code&gt; To make the model more robust to various input object sizes and shapes, each training image is randomly sampled by one of the following options:
&lt;strong&gt;数据增强。为了使模型对各种输入目标大小和形状更鲁棒，每张训练图像都是通过以下选项之一进行随机采样的：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use the entire original input image.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;使用整个原始输入图像。&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Sample a patch so that the minimum jaccard overlap with the objects is 0.1, 0.3,0.5, 0.7, or 0.9.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;采样一个图像块，使得与目标之间的最小Jaccard重叠为0.1，0.3，0.5，0.7或0.9。&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Randomly sample a patch.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;随机采样一个图像块。&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The size of each sampled patch is [0.1, 1] of the original image size, and the aspect ratio is between $\frac {1} {2}$ and 2. We keep the overlapped part of the ground truth box if the center of 2 it is in the sampled patch. After the aforementioned sampling step, each sampled patch is resized to fixed size and is horizontally flipped with probability of 0.5, in addition to applying some photo-metric distortions similar to those described in [14].&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;每个采样图像块的大小是原始图像大小的[0.1，1]，长宽比在$\frac {1} {2}$和2之间。如果实际边界框的中心在采用的图像块中，我们保留实际边界框与采样图像块的重叠部分。在上述采样步骤之后，除了应用类似于文献[14]中描述的一些光度变形之外，将每个采样图像块调整到固定尺寸并以0.5的概率进行水平翻转。&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;3-experimental-results&quot;&gt;3 Experimental Results&lt;/h3&gt;
&lt;h4 id=&quot;3-实验结果&quot;&gt;3. 实验结果&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Base network&lt;/code&gt; Our experiments are all based on VGG16 [15], which is pre-trained on the ILSVRC CLS-LOC dataset [16]. Similar to DeepLab-LargeFOV [17], we convert fc6 and fc7 to convolutional layers, subsample parameters from fc6 and fc7, change pool5 from$2×2−s2$ to $3×3−s1$,andusethea` trousalgorithm[18]tofillthe ”holes”. We remove all the dropout layers and the fc8 layer. We fine-tune the resulting model using SGD with initial learning rate $10^{−3}$, 0.9 momentum, 0.0005 weight decay, and batch size 32. The learning rate decay policy is slightly different for each dataset, and we will describe details later. The full training and testing code is built on Caffe [19] and is open source at: https://github.com/weiliu89/caffe/tree/ssd .&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;基础网络&lt;/code&gt;。我们的实验全部基于VGG16[15]，它是在ILSVRC CLS-LOC数据集[16]上预先训练的。类似于DeepLab-LargeFOV[17]，我们将fc6和fc7转换为卷积层，从fc6和fc7中重采样参数，将pool5从$2×2−s2$更改为$3×3−s1$，并使用空洞算法[18]来填补这个“小洞”。我们删除所有的丢弃层和fc8层。我们使用SGD对得到的模型进行微调，初始学习率为$10^{−3}，动量为0.9，权重衰减为0.0005，批数据大小为32。每个数据集的学习速率衰减策略略有不同，我们将在后面详细描述。完整的训练和测试代码建立在Caffe[19]上并开源：https://github.com/weiliu89/caffe/tree/ssd。&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;31-pascal-voc2007&quot;&gt;3.1 PASCAL VOC2007&lt;/h4&gt;
&lt;h4 id=&quot;31-pascal-voc2007-1&quot;&gt;3.1 PASCAL VOC2007&lt;/h4&gt;

&lt;p&gt;On this dataset, we compare against Fast R-CNN [6] and Faster R-CNN [2] on VOC2007 test (4952 images). All methods fine-tune on the same pre-trained VGG16 network.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;在这个数据集上，我们在VOC2007 test（4952张图像）上比较了Fast R-CNN[6]和FAST R-CNN[2]。所有的方法都在相同的预训练好的VGG16网络上进行微调。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Figure 2 shows the architecture details of the SSD300 model. We use conv4 3, conv7 (fc7), conv8 2, conv9 2, conv10 2, and conv11 2 to predict both location and confidences. We set default box with scale 0.1 on conv4 33. We initialize the parameters for all the newly added convolutional layers with the ”xavier” method [20]. For conv4 3, conv10 2 and conv11 2, we only associate 4 default boxes at each feature map location——omitting aspect ratios of $\frac {1} {3}$ and 3. For all other layers, we put 6 default boxes as 3 described in Sec. 2.2. Since, as pointed out in [12], conv4 3 has a different feature scale compared to the other layers, we use the L2 normalization technique introduced in [12] to scale the feature norm at each location in the feature map to 20 and learn the scale during back propagation. We use the 10−3 learning rate for 40k iterations, then continue training for 10k iterations with 10−4 and 10−5. When training on VOC2007 trainval, Table 1 shows that our low resolution SSD300 model is already more accurate than Fast R-CNN. When we train SSD on a larger 512 × 512 input image, it is even more accurate, surpassing Faster R-CNN by 1.7% mAP. If we train SSD with more (i.e. 07+12) data, we see that SSD300 is already better than Faster R-CNN by 1.1% and that SSD512 is 3.6% better. If we take models trained on COCO trainval35k as described in Sec. 3.4 and fine-tuning them on the 07+12 dataset with SSD512, we achieve the best results: 81.6% mAP.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;图2显示了SSD300模型的架构细节。我们使用conv4_3，conv7（fc7），conv8_2，conv9_2，conv10_2和conv11_2来预测位置和置信度。我们在conv4_3上设置了尺度为0.1的默认边界框。我们使用“xavier”方法[20]初始化所有新添加的卷积层的参数。对于conv4_3，conv10_2和conv11_2，我们只在每个特征映射位置上关联了4个默认边界框——忽略$\frac {1} {3}$和3的长宽比。对于所有其它层，我们像2.2节描述的那样放置了6个默认边界框。如[12]所指出的，与其它层相比，由于conv4_3具有不同的特征尺度，所以我们使用[12]中引入的L2正则化技术将特征映射中每个位置的特征标准缩放到20，在反向传播过程中学习尺度。对于40k次迭代，我们使用$10^{−3}$的学习率，然后继续用$10^{−4}$和$10^{−5}$的学习率训练10k迭代。当对VOC2007 𝚝𝚛𝚊𝚒𝚗𝚟𝚊𝚕进行训练时，表1显示了我们的低分辨率SSD300模型已经比Fast R-CNN更准确。当我们用更大的512×512输入图像上训练SSD时，它更加准确，超过了Faster R-CNN $1.7\%$的mAP。如果我们用更多的（即07+12）数据来训练SSD，我们看到SSD300已经比Faster R-CNN好$1.1\%$，SSD512比Faster R-CNN好$3.6\%$。如果我们将SSD512用3.4节描述的COCO 𝚝𝚛𝚊𝚒𝚗𝚟𝚊𝚕𝟹𝟻𝚔来训练模型并在07+12数据集上进行微调，我们获得了最好的结果：$81.6\%$的mAP。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/ssd/3.png&quot; alt=&quot;3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Table 1: PASCAL VOC2007 test detection results. Both Fast and Faster R-CNN use input images whose minimum dimension is 600. The two SSD models have exactly the same settings except that they have different input sizes (300×300 vs. 512×512). It is obvious that larger input size leads to better results, and more data always helps. Data: ”07”: VOC2007 trainval, ”07+12”: union of VOC2007 and VOC2012 trainval. ”07+12+COCO”: first train on COCO trainval35k then fine-tune on 07+12.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;表1：PASCAL VOC2007 &lt;code class=&quot;highlighter-rouge&quot;&gt;test检测结果&lt;/code&gt;。Fast和Faster R-CNN都使用最小维度为600的输入图像。两个SSD模型使用完全相同的设置除了它们有不同的输入大小(300×300和512×512)。很明显更大的输入尺寸会导致更好的结果，并且更大的数据同样有帮助。数据：“07”：VOC2007 &lt;code class=&quot;highlighter-rouge&quot;&gt;trainval&lt;/code&gt;，“07+12”：VOC2007和VOC2012 &lt;code class=&quot;highlighter-rouge&quot;&gt;trainval&lt;/code&gt;的联合。“07+12+COCO”：首先在COCO trainval35k上训练然后在07+12上微调。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To understand the performance of our two SSD models in more details, we used the detection analysis tool from [21]. Figure 3 shows that SSD can detect various object categories with high quality (large white area). The majority of its confident detections are correct. The recall is around 85-90%, and is much higher with “weak” (0.1 jaccard overlap) criteria. Compared to R-CNN [22], SSD has less localization error, indicating that SSD can localize objects better because it directly learns to regress the object shape and classify object categories instead of using two decoupled steps. However, SSD has more confusions with similar object categories (especially for animals), partly because we share locations for multiple categories. Figure 4 shows that SSD is very sensitive to the bounding box size. In other words, it has much worse performance on smaller objects than bigger objects. This is not surprising because those small objects may not even have any information at the very top layers. Increasing the input size (e.g. from 300 × 300 to 512 × 512) can help improve detecting small objects, but there is still a lot of room to improve. On the positive side, we can clearly see that SSD performs really well on large objects. And it is very robust to different object aspect ratios because we use default boxes of various aspect ratios per feature map location.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;为了更详细地了解我们两个SSD模型的性能，我们使用了[21]中的检测分析工具。图3显示了SSD可以检测到高质量（大白色区域）的各种目标类别。它大部分的确信检测是正确的。召回约为85−90%，而“弱”（0.1 Jaccard重叠）标准则要高得多。与R-CNN[22]相比，SSD具有更小的定位误差，表明SSD可以更好地定位目标，因为它直接学习回归目标形状和分类目标类别，而不是使用两个解耦步骤。然而，SSD对类似的目标类别（特别是对于动物）有更多的混淆，部分原因是我们共享多个类别的位置。图4显示SSD对边界框大小非常敏感。换句话说，它在较小目标上比在较大目标上的性能要差得多。这并不奇怪，因为这些小目标甚至可能在顶层没有任何信息。增加输入尺寸（例如从300×300到512×512）可以帮助改进检测小目标，但仍然有很大的改进空间。积极的一面，我们可以清楚地看到SSD在大型目标上的表现非常好。而且对于不同长宽比的目标，它是非常鲁棒的，因为我们使用每个特征映射位置的各种长宽比的默认框。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/ssd/4.png&quot; alt=&quot;4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Fig. 3: Visualization of performance for SSD512 on animals, vehicles, and furni- ture from VOC2007 test. The top row shows the cumulative fraction of detections that are correct (Cor) or false positive due to poor localization (Loc), confusion with similar categories (Sim), with others (Oth), or with background (BG). The solid red line reflects the change of recall with strong criteria (0.5 jaccard overlap) as the num- ber of detections increases. The dashed red line is using the weak criteria (0.1 jaccard overlap). The bottom row shows the distribution of top-ranked false positive types.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;图3：SSD512在VOC2007 test中的动物，车辆和家具上的性能可视化。第一行显示由于定位不佳（Loc），与相似类别（Sim）混淆，与其它（Oth）或背景（BG）相关的正确检测（Cor）或假阳性的累积分数。红色的实线表示随着检测次数的增加，强标准（0.5 Jaccard重叠）下的召回变化。红色虚线是使用弱标准（0.1 Jaccard重叠）。最下面一行显示了排名靠前的假阳性类型的分布。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/ssd/5.png&quot; alt=&quot;5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Fig. 4: Sensitivity and impact of different object characteristics on VOC2007 test set using [21]. The plot on the left shows the effects of BBox Area per category, and the right plot shows the effect of Aspect Ratio. Key: BBox Area: XS=extra-small; S=small; M=medium; L=large; XL =extra-large. Aspect Ratio: XT=extra-tall/narrow; T=tall; M=medium; W=wide; XW =extra-wide.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;图4：使用[21]在VOC2007 test设置上不同目标特性的灵敏度和影响。左边的图显示了BBox面积对每个类别的影响，右边的图显示了长宽比的影响。关键：BBox区域：XS=超小；S=小；M=中等；L=大；XL=超大。长宽比：XT=超高/窄；T=高；M=中等；W=宽；XW =超宽。&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;32-model-analysis&quot;&gt;3.2 Model analysis&lt;/h4&gt;
&lt;h4 id=&quot;32-模型分析&quot;&gt;3.2 模型分析&lt;/h4&gt;

&lt;p&gt;To understand SSD better, we carried out controlled experiments to examine how each component affects performance. For all the experiments, we use the same settings and input size (300 × 300), except for specified changes to the settings or component(s).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;为了更好地了解SSD，我们进行了控制实验，以检查每个组件如何影响性能。对于所有的实验，我们使用相同的设置和输入大小（300×300），除了指定的设置或组件的更改。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/ssd/6.jpg&quot; alt=&quot;6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;表2：各种设计选择和组件对SSD性能的影响。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Data augmentation is crucial&lt;/code&gt;. Fast and Faster R-CNN use the original image and the horizontal flip to train. We use a more extensive sampling strategy, similar to YOLO [5]. Table 2 shows that we can improve 8.8% mAP with this sampling strategy. We do not know how much our sampling strategy will benefit Fast and Faster R-CNN, but they are likely to benefit less because they use a feature pooling step during classification that is relatively robust to object translation by design.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;数据增强至关重要&lt;/code&gt;。Fast和Faster R-CNN使用原始图像和水平翻转来训练。我们使用更广泛的抽样策略，类似于YOLO[5]。从表2可以看出，采样策略可以提高8.8%的mAP。我们不知道我们的采样策略将会使Fast和Faster R-CNN受益多少，但是他们可能从中受益较少，因为他们在分类过程中使用了一个特征池化步骤，这对通过设计的目标变换来说相对鲁棒。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;More default box shapes is better. As described in Sec. 2.2, by default we use 6 default boxes per location. If we remove the boxes with $\frac13$ and 3 aspect ratios, the performance drops by 0.6%. By further removing the boxes with $\frac12$ and 2 aspect ratios, the performance drops another 2.1%. Using a variety of default box shapes seems to make the task of predicting boxes easier for the network.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;更多的默认边界框形状会更好&lt;/code&gt;。如2.2节所述，默认情况下，我们每个位置使用6个默认边界框。如果我们删除长宽比为$\frac {1} {3}$和3的边界框，性能下降了0.6%。通过进一步去除$\frac {1} {2}$和2长宽比的盒子，性能再下降2.1%。使用各种默认边界框形状似乎使网络预测边界框的任务更容易。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Atrous is faster&lt;/code&gt;. As described in Sec. 3, we used the atrous version of a subsampled VGG16, following DeepLab-LargeFOV [17]. If we use the full VGG16, keeping pool5 with 2 × 2 − s2 and not subsampling parameters from fc6 and fc7, and add conv5 3 for prediction, the result is about the same while the speed is about 20% slower.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Atrous更快&lt;/code&gt;。如第3节所述，我们根据DeepLab-LargeFOV[17]使用子采样的VGG16的空洞版本。如果我们使用完整的VGG16，保持pool5为2×2-s2，并且不从fc6和fc7中子采样参数，并添加conv5_3进行预测，结果大致相同，而速度慢了大约20%。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Multiple output layers at different resolutions is better. A major contribution of SSD is using default boxes of different scales on different output layers. To measure the advantage gained, we progressively remove layers and compare results. For a fair comparison, every time we remove a layer, we adjust the default box tiling to keep the total number of boxes similar to the original (8732). This is done by stacking more scales of boxes on remaining layers and adjusting scales of boxes if needed. We do not exhaustively optimize the tiling for each setting. Table 3 shows a decrease in accuracy with fewer layers, dropping monotonically from 74.3 to 62.4. When we stack boxes of multiple scales on a layer, many are on the image boundary and need to be handled carefully. We tried the strategy used in Faster R-CNN [2], ignoring boxes which are on the boundary. We observe some interesting trends. For example, it hurts the performance by a large margin if we use very coarse feature maps (e.g. conv11 2 (1 × 1) or conv10 2 (3 × 3)). The reason might be that we do not have enough large boxes to cover large objects after the pruning. When we use primarily finer resolution maps, the performance starts increasing again because even after pruning a sufficient number of large boxes remains. If we only use conv7 for prediction, the performance is the worst, reinforcing the message that it is critical to spread boxes of different scales over dif- ferent layers. Besides, since our predictions do not rely on ROI pooling as in [6], we do not have the collapsing bins problem in low-resolution feature maps [23]. The SSD architecture combines predictions from feature maps of various resolutions to achieve comparable accuracy to Faster R-CNN, while using lower resolution input images.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;多个不同分辨率的输出层更好&lt;/code&gt;。SSD的主要贡献是在不同的输出层上使用不同尺度的默认边界框。为了衡量所获得的优势，我们逐步删除层并比较结果。为了公平比较，每次我们删除一层，我们调整默认边界框平铺，以保持类似于最初的边界框的总数（8732）。这是通过在剩余层上堆叠更多尺度的盒子并根据需要调整边界框的尺度来完成的。我们没有详尽地优化每个设置的平铺。表3显示层数较少，精度降低，从74.3单调递减至62.4。当我们在一层上堆叠多尺度的边界框时，很多边界框在图像边界上需要小心处理。我们尝试了在Faster R-CNN[2]中使用这个策略，忽略在边界上的边界框。我们观察到了一些有趣的趋势。例如，如果我们使用非常粗糙的特征映射（例如conv11_2（1×1）或conv10_2（3×3）），它会大大伤害性能。原因可能是修剪后我们没有足够大的边界框来覆盖大的目标。当我们主要使用更高分辨率的特征映射时，性能开始再次上升，因为即使在修剪之后仍然有足够数量的大边界框。如果我们只使用conv7进行预测，那么性能是最糟糕的，这就强化了&lt;code class=&quot;highlighter-rouge&quot;&gt;在不同层上扩展不同尺度的边界框是非常关键的&lt;/code&gt;信息。此外，由于我们的预测不像[6]那样依赖于ROI池化，所以我们在低分辨率特征映射中没有折叠组块的问题[23]。SSD架构将来自各种分辨率的特征映射的预测结合起来，以达到与Faster R-CNN相当的精确度，同时使用较低分辨率的输入图像。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/ssd/7.jpg&quot; alt=&quot;7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;表3：使用多个输出层的影响。&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h4 id=&quot;33-pascal-voc2012&quot;&gt;3.3 PASCAL VOC2012&lt;/h4&gt;
&lt;h4 id=&quot;33-pascal-voc2012-1&quot;&gt;3.3 PASCAL VOC2012&lt;/h4&gt;

&lt;p&gt;We use the same settings as those used for our basic VOC2007 experiments above, except that we use VOC2012 trainval and VOC2007 trainval and test (21503 images) for training, and test on VOC2012 test (10991 images). We train the models with 10−3 learning rate for 60k iterations, then 10−4 for 20k iterations. Table 4 shows the results of our SSD300 and SSD5124 model. We see the same performance trend as we observed on VOC2007 test. Our SSD300 improves accuracy over Fast/Faster R- CNN. By increasing the training and testing image size to 512 × 512, we are 4.5% more accurate than Faster R-CNN. Compared to YOLO, SSD is significantly more accurate, likely due to the use of convolutional default boxes from multiple feature maps and our matching strategy during training. When fine-tuned from models trained on COCO, our SSD512 achieves 80.0% mAP, which is 4.1% higher than Faster R-CNN.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;除了我们使用VOC2012 trainval和VOC2007 trainval，test（21503张图像）进行训练，以及在VOC2012 test（10991张图像）上进行测试之外，我们使用与上述基本的VOC2007实验相同的设置。我们用$10^{−3}$的学习率对模型进行60k次的迭代训练，然后使用$10^{−4}$的学习率进行20k次迭代训练。表4显示了我们的SSD300和SSD512模型的结果。我们看到了与我们在VOC2007 test中观察到的相同的性能趋势。我们的SSD300比Fast/Faster R-CNN提高了准确性。通过将训练和测试图像大小增加到512×512，我们比Faster R-CNN的准确率提高了4.5%。与YOLO相比，SSD更精确，可能是由于使用了来自多个特征映射的卷积默认边界框和我们在训练期间的匹配策略。当对从COCO上训练的模型进行微调后，我们的SSD512达到了80.0%的mAP，比Faster R-CNN高了4.1%。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/ssd/8.png&quot; alt=&quot;8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Table 4: PASCAL VOC2012 test detection results. Fast and Faster R-CNN use images with minimum dimension 600, while the image size for YOLO is 448 × 448. data: ”07++12”: union of VOC2007 trainval and test and VOC2012 trainval. ”07++12+COCO”: first train on COCO trainval35k then fine-tune on 07++12.
&lt;strong&gt;表4： PASCAL VOC2012 test上的检测结果. Fast和Faster R-CNN使用最小维度为600的图像，而YOLO的图像大小为448× 448。数据：“07++12”：VOC2007 trainval，test和VOC2012 trainval。“07++12+COCO”：先在COCO trainval 35k上训练然后在07++12上微调。&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;34-coco&quot;&gt;3.4 COCO&lt;/h4&gt;
&lt;p&gt;To further validate the SSD framework, we trained our SSD300 and SSD512 architec- tures on the COCO dataset. Since objects in COCO tend to be smaller than PASCAL VOC, we use smaller default boxes for all layers. We follow the strategy mentioned in Sec. 2.2, but now our smallest default box has a scale of 0.15 instead of 0.2, and the scale of the default box on conv4 3 is 0.07 (e.g. 21 pixels for a 300 × 300 image)5.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;为了进一步验证SSD框架，我们在COCO数据集上对SSD300和SSD512架构进行了训练。由于COCO中的目标往往比PASCAL VOC中的更小，因此我们对所有层使用较小的默认边界框。我们遵循2.2节中提到的策略，但是现在我们最小的默认边界框尺度是0.15而不是0.2，并且conv4_3上的默认边界框尺度是0.07（例如，300×300图像中的21个像素）。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We use the trainval35k [24] for training. We first train the model with 10−3 learning rate for 160k iterations, and then continue training for 40k iterations with 10−4 and 40k iterations with 10−5. Table 5 shows the results on test-dev2015. Similar to what we observed on the PASCAL VOC dataset, SSD300 is better than Fast R-CNN in both mAP@0.5 and mAP@[0.5:0.95]. SSD300 has a similar mAP@0.75 as ION [24] and Faster R-CNN [25], but is worse in mAP@0.5. By increasing the im- age size to 512 × 512, our SSD512 is better than Faster R-CNN [25] in both criteria. Interestingly, we observe that SSD512 is 5.3% better in mAP@0.75, but is only 1.2% better in mAP@0.5. We also observe that it has much better AP (4.8%) and AR (4.6%) for large objects, but has relatively less improvement in AP (1.3%) and AR (2.0%) for small objects. Compared to ION, the improvement in AR for large and small objects is more similar (5.4% vs. 3.9%). We conjecture that Faster R-CNN is more competitive on smaller objects with SSD because it performs two box refinement steps, in both the RPN part and in the Fast R-CNN part. In Fig. 5, we show some detection examples on COCO test-dev with the SSD512 model.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;我们使用trainval35k[24]进行训练。我们首先用$10^{−3}$的学习率对模型进行训练，进行160k次迭代，然后继续以$10^{−4}$和$10^{−5}$的学习率各进行40k次迭代。表5显示了test-dev2015的结果。与我们在PASCAL VOC数据集中观察到的结果类似，SSD300在mAP@0.5和mAP@[0.5:0.95]中都优于Fast R-CNN。SSD300与ION [24]和Faster R-CNN[25]具有相似的mAP@0.75，但是mAP@0.5更差。通过将图像尺寸增加到512×512，我们的SSD512在这两个标准中都优于Faster R-CNN[25]。有趣的是，我们观察到SSD512在mAP@0.75中要好5.3%，但是在mAP@0.5中只好1.2%。我们也观察到，对于大型目标，AP（4.8%）和AR（4.6%）的效果要好得多，但对于小目标，AP（1.3%）和AR（2.0%）有相对更少的改进。与ION相比，大型和小型目标的AR改进更为相似（5.4%和3.9%）。我们推测Faster R-CNN在较小的目标上比SSD更具竞争力，因为它在RPN部分和Fast R-CNN部分都执行了两个边界框细化步骤。在图5中，我们展示了SSD512模型在COCO test-dev上的一些检测实例。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/ssd/9.jpg&quot; alt=&quot;9&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;表5：COCO test-dev2015检测结果。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/ssd/10.jpeg&quot; alt=&quot;10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Fig. 5: Detection examples on COCO test-dev with SSD512 model. We show detections with scores higher than 0.6. Each color corresponds to an object category.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;图5：SSD512模型在COCO test-dev上的检测实例。我们展示了分数高于0.6的检测。每种颜色对应一种目标类别。&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;35-preliminary-ilsvrc-results&quot;&gt;3.5 Preliminary ILSVRC results&lt;/h4&gt;
&lt;h4 id=&quot;35-初步的ilsvrc结果&quot;&gt;3.5 初步的ILSVRC结果&lt;/h4&gt;

&lt;p&gt;We applied the same network architecture we used for COCO to the ILSVRC DET dataset [16]. We train a SSD300 model using the ILSVRC2014 DET train and val1 as used in [22]. We first train the model with 10−3 learning rate for 320k iterations, and then continue training for 80k iterations with 10−4 and 40k iterations with 10−5. We can achieve 43.4 mAP on the val2 set [22]. Again, it validates that SSD is a general framework for high quality real-time detection.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;我们将在COCO上应用的相同网络架构应用于ILSVRC DET数据集[16]。我们使用[22]中使用的ILSVRC2014 DETtrain和val1来训练SSD300模型。我们首先用$10^{−3}$的学习率对模型进行训练，进行了320k次的迭代，然后以$10^{−4继续迭代80k次，以$10^−5$迭代40k次。我们可以在val2数据集上[22]实现43.4 mAP。再一次证明了SSD是用于高质量实时检测的通用框架。&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h4 id=&quot;36-data-augmentation-for-small-object-accuracy&quot;&gt;3.6 Data Augmentation for Small Object Accuracy&lt;/h4&gt;
&lt;h4 id=&quot;36-为小目标准确率进行数据增强&quot;&gt;3.6 为小目标准确率进行数据增强&lt;/h4&gt;

&lt;p&gt;Without a follow-up feature resampling step as in Faster R-CNN, the classification task for small objects is relatively hard for SSD, as demonstrated in our analysis (see Fig. 4). The data augmentation strategy described in Sec. 2.2 helps to improve the performance dramatically, especially on small datasets such as PASCAL VOC. The random crops generated by the strategy can be thought of as a ”zoom in” operation and can generate many larger training examples. To implement a ”zoom out” operation that creates more small training examples, we first randomly place an image on a canvas of 16× of the original image size filled with mean values before we do any random crop operation. Because we have more training images by introducing this new ”expansion” data aug- mentation trick, we have to double the training iterations. We have seen a consistent increase of 2%-3% mAP across multiple datasets, as shown in Table 6. In specific, Fig- ure 6 shows that the new augmentation trick significantly improves the performance on small objects. This result underscores the importance of the data augmentation strategy for the final model accuracy.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SSD没有如Faster R-CNN中后续的特征重采样步骤，小目标的分类任务对SSD来说相对困难，正如我们的分析（见图4）所示。2.2描述的数据增强有助于显著提高性能，特别是在PASCAL VOC等小数据集上。策略产生的随机裁剪可以被认为是“放大”操作，并且可以产生许多更大的训练样本。为了实现创建更多小型训练样本的“缩小”操作，我们首先将图像随机放置在填充了平均值的原始图像大小为16x的画布上，然后再进行任意的随机裁剪操作。因为通过引入这个新的“扩展”数据增强技巧，我们有更多的训练图像，所以我们必须将训练迭代次数加倍。我们已经在多个数据集上看到了一致的2%−3%的mAP增长，如表6所示。具体来说，图6显示新的增强技巧显著提高了模型在小目标上的性能。这个结果强调了数据增强策略对最终模型精度的重要性。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/ssd/11.png&quot; alt=&quot;11&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Table 6: Results on multiple datasets when we add the image expansion data aug-
mentation trick. SSD$300^&lt;em&gt;$ and SSD$512^&lt;/em&gt;$ are the models that are trained with the new data augmentation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;表6：我们使用图像扩展数据增强技巧在多个数据集上的结果。SSD$300^∗$和$SSD512^∗$是用新的数据增强训练的模型。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/ssd/12.png&quot; alt=&quot;12&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Fig.6: Sensitivity and impact of object size with new data augmentation on VOC2007 test set using [21]. The top row shows the effects of BBox Area per cat- egory for the original SSD300 and SSD512 model, and the bottom row corresponds to the SSD300* and SSD512* model trained with the new data augmentation trick. It is obvious that the new data augmentation trick helps detecting small objects significantly.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;图6：具有新的数据增强的目标尺寸在[21]中使用的VOC2007test数据集上灵敏度及影响。最上一行显示了原始SSD300和SSD512模型上每个类别的BBox面积的影响，最下面一行对应使用新的数据增强训练技巧的SSD$300^∗$和$SSD512^∗$模型。新的数据增强技巧显然有助于显著检测小目标。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;An alternative way of improving SSD is to design a better tiling of default boxes so that its position and scale are better aligned with the receptive field of each position on a feature map. We leave this for future work.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;改进SSD的另一种方法是设计一个更好的默认边界框平铺，使其位置和尺度与特征映射上每个位置的感受野更好地对齐。我们将这个留给未来工作。&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h4 id=&quot;37-inference-time&quot;&gt;3.7 Inference time&lt;/h4&gt;
&lt;h4 id=&quot;37-推断时间&quot;&gt;3.7 推断时间&lt;/h4&gt;

&lt;p&gt;Considering the large number of boxes generated from our method, it is essential to perform non-maximum suppression (nms) efficiently during inference. By using a con- fidence threshold of 0.01, we can filter out most boxes. We then apply nms with jaccard overlap of 0.45 per class and keep the top 200 detections per image. This step costs about 1.7 msec per image for SSD300 and 20 VOC classes, which is close to the total time (2.4 msec) spent on all newly added layers. We measure the speed with batch size 8 using Titan X and cuDNN v4 with Intel Xeon E5-2667v3@3.20GHz.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;考虑到我们的方法产生大量边界框，在推断期间执行非最大值抑制（nms）是必要的。通过使用0.01的置信度阈值，我们可以过滤大部分边界框。然后，我们应用nms，每个类别0.45的Jaccard重叠，并保留每张图像的前200个检测。对于SSD300和20个VOC类别，这个步骤每张图像花费大约1.7毫秒，接近在所有新增层上花费的总时间（2.4毫秒）。我们使用Titan X、cuDNN v4、Intel Xeon E5-2667v3@3.20GHz以及批大小为8来测量速度。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Table 7 shows the comparison between SSD, Faster R-CNN[2], and YOLO[5]. Both our SSD300 and SSD512 method outperforms Faster R-CNN in both speed and accu- racy. Although Fast YOLO[5] can run at 155 FPS, it has lower accuracy by almost 22% mAP. To the best of our knowledge, SSD300 is the first real-time method to achieve above 70% mAP. Note that about 80% of the forward time is spent on the base network (VGG16 in our case). Therefore, using a faster base network could even further improve the speed, which can possibly make the SSD512 model real-time as well.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;表7显示了SSD，Faster R-CNN[2]和YOLO[5]之间的比较。我们的SSD300和SSD512的速度和精度均优于Faster R-CNN。虽然Fast YOLO[5]可以以155FPS的速度运行，但其准确性却降低了近22%的mAP。就我们所知，SSD300是第一个实现70%以上mAP的实时方法。请注意，大约80%前馈时间花费在基础网络上（本例中为VGG16）。因此，使用更快的基础网络可以进一步提高速度，这也可能使SSD512模型达到实时。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;！&lt;a href=&quot;/images/posts/ssd/13.png&quot;&gt;13&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Table 7: Results on Pascal VOC2007 test. SSD300 is the only real-time detection method that can achieve above 70% mAP. By using a larger input image, SSD512 out- performs all methods on accuracy while maintaining a close to real-time speed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;表7：Pascal VOC2007 test上的结果。SSD300是唯一可以取得70%以上mAP的实时检测方法。通过使用更大的输入图像，SSD512在精度上超过了所有方法同时保持近似实时的速度。&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;4-related-work&quot;&gt;4 Related Work&lt;/h3&gt;
&lt;h4 id=&quot;4-相关工作&quot;&gt;4. 相关工作&lt;/h4&gt;

&lt;p&gt;There are two established classes of methods for object detection in images, one based on sliding windows and the other based on region proposal classification. Before the advent of convolutional neural networks, the state of the art for those two approaches – Deformable Part Model (DPM) [26] and Selective Search [1] – had comparable performance. However, after the dramatic improvement brought on by R-CNN [22], which combines selective search region proposals and convolutional network based post-classification, region proposal object detection methods became prevalent.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;在图像中有两种建立的用于目标检测的方法，一种基于滑动窗口，另一种基于区域提出分类。在卷积神经网络出现之前，这两种方法的最新技术——可变形部件模型（DPM）[26]和选择性搜索[1]——具有相当的性能。然而，在R-CNN[22]结合选择性搜索区域提出和基于后分类的卷积网络带来的显著改进后，区域提出目标检测方法变得流行。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The original R-CNN approach has been improved in a variety of ways. The first set of approaches improve the quality and speed of post-classification, since it requires the classification of thousands of image crops, which is expensive and time-consuming. SPPnet [9] speeds up the original R-CNN approach significantly. It introduces a spatial pyramid pooling layer that is more robust to region size and scale and allows the classi- fication layers to reuse features computed over feature maps generated at several image resolutions. Fast R-CNN [6] extends SPPnet so that it can fine-tune all layers end-to- end by minimizing a loss for both confidences and bounding box regression, which was first introduced in MultiBox [7] for learning objectness.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;最初的R-CNN方法已经以各种方式进行了改进。第一套方法提高了后分类的质量和速度，因为它需要对成千上万的裁剪图像进行分类，这是昂贵和耗时的。SPPnet[9]显著加快了原有的R-CNN方法。它引入了一个空间金字塔池化层，该层对区域大小和尺度更鲁棒，并允许分类层重用多个图像分辨率下生成的特征映射上计算的特征。Fast R-CNN[6]扩展了SPPnet，使得它可以通过最小化置信度和边界框回归的损失来对所有层进行端到端的微调，最初在MultiBox[7]中引入用于学习目标。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The second set of approaches improve the quality of proposal generation using deep neural networks. In the most recent works like MultiBox [7,8], the Selective Search region proposals, which are based on low-level image features, are replaced by pro- posals generated directly from a separate deep neural network. This further improves the detection accuracy but results in a somewhat complex setup, requiring the training of two neural networks with a dependency between them. Faster R-CNN [2] replaces selective search proposals by ones learned from a region proposal network (RPN), and introduces a method to integrate the RPN with Fast R-CNN by alternating between fine- tuning shared convolutional layers and prediction layers for these two networks. This way region proposals are used to pool mid-level features and the final classification step is less expensive. Our SSD is very similar to the region proposal network (RPN) in Faster R-CNN in that we also use a fixed set of (default) boxes for prediction, similar to the anchor boxes in the RPN. But instead of using these to pool features and evaluate another classifier, we simultaneously produce a score for each object category in each box. Thus, our approach avoids the complication of merging RPN with Fast R-CNN and is easier to train, faster, and straightforward to integrate in other tasks.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第二套方法使用深度神经网络提高了提出生成的质量。在最近的工作MultiBox[7,8]中，基于低级图像特征的选择性搜索区域提出直接被单独的深度神经网络生成的提出所取代。这进一步提高了检测精度，但是导致了一些复杂的设置，需要训练两个具有依赖关系的神经网络。Faster R-CNN[2]将选择性搜索提出替换为区域提出网络（RPN）学习到的区域提出，并引入了一种方法，通过交替两个网络之间的微调共享卷积层和预测层将RPN和Fast R-CNN结合在一起。通过这种方式，使用区域提出池化中级特征，并且最后的分类步骤比较便宜。我们的SSD与Faster R-CNN中的区域提出网络（RPN）非常相似，因为我们也使用一组固定的（默认）边界框进行预测，类似于RPN中的锚边界框。但是，我们不是使用这些来池化特征并评估另一个分类器，而是为每个目标类别在每个边界框中同时生成一个分数。因此，我们的方法避免了将RPN与Fast R-CNN合并的复杂性，并且更容易训练，更快且更直接地集成到其它任务中。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Another set of methods, which are directly related to our approach, skip the proposal step altogether and predict bounding boxes and confidences for multiple categories di- rectly. OverFeat [4], a deep version of the sliding window method, predicts a bounding box directly from each location of the topmost feature map after knowing the confi- dences of the underlying object categories. YOLO [5] uses the whole topmost feature map to predict both confidences for multiple categories and bounding boxes (which are shared for these categories). Our SSD method falls in this category because we do not have the proposal step but use the default boxes. However, our approach is more flexible than the existing methods because we can use default boxes of different aspect ratios on each feature location from multiple feature maps at different scales. If we only use one default box per location from the topmost feature map, our SSD would have similar architecture to OverFeat [4]; if we use the whole topmost feature map and add a fully connected layer for predictions instead of our convolutional predictors, and do not explicitly consider multiple aspect ratios, we can approximately reproduce YOLO [5].&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;与我们的方法直接相关的另一组方法，完全跳过提出（proposal）步骤，直接预测多个类别的边界框和置信度。OverFeat[4]是滑动窗口方法的深度版本，在知道了底层目标类别的置信度之后，直接从最顶层的特征映射的每个位置预测边界框。YOLO[5]使用整个最顶层的特征映射来预测多个类别和边界框（这些类别共享）的置信度。我们的SSD方法属于这一类，因为我们没有提出步骤，但使用默认边界框。然而，我们的方法比现有方法更灵活，因为我们可以在不同尺度的多个特征映射的每个特征位置上使用不同长宽比的默认边界框。如果我们只从最顶层的特征映射的每个位置使用一个默认框，我们的SSD将具有与OverFeat[4]相似的架构；如果我们使用整个最顶层的特征映射，并添加一个全连接层进行预测来代替我们的卷积预测器，并且没有明确地考虑多个长宽比，我们可以近似地再现YOLO[5]。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;### 5 Conclusions
 This paper introduces SSD, a fast single-shot object detector for multiple categories. A key feature of our model is the use of multi-scale convolutional bounding box outputs attached to multiple feature maps at the top of the network. This representation allows us to efficiently model the space of possible box shapes. We experimentally validate that given appropriate training strategies, a larger number of carefully chosen default bounding boxes results in improved performance. We build SSD models with at least an order of magnitude more box predictions sampling location, scale, and aspect ratio, than existing methods [5,7]. We demonstrate that given the same VGG-16 base architecture, SSD compares favorably to its state-of-the-art object detector counterparts in terms of both accuracy and speed. Our SSD512 model significantly outperforms the state-of-the- art Faster R-CNN [2] in terms of accuracy on PASCAL VOC and COCO, while being 3× faster. Our real time SSD300 model runs at 59 FPS, which is faster than the current real time YOLO [5] alternative, while producing markedly superior detection accuracy.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;本文介绍了SSD，一种快速的单次多类别目标检测器。我们模型的一个关键特性是使用网络顶部多个特征映射的多尺度卷积边界框输出。这种表示使我们能够高效地建模可能的边界框形状空间。我们通过实验验证，在给定合适训练策略的情况下，大量仔细选择的默认边界框会提高性能。我们构建的SSD模型比现有的方法至少要多一个数量级的边界框预测采样位置，尺度和长宽比[5,7]。我们证明了给定相同的VGG-16基础架构，SSD在准确性和速度方面与其对应的最先进的目标检测器相比毫不逊色。在PASCAL VOC和COCO上，我们的SSD512模型的性能明显优于最先进的Faster R-CNN[2]，而速度提高了3倍。我们的实时SSD300模型运行速度为59FPS，比目前的实时YOLO[5]更快，同时显著提高了检测精度。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Apart from its standalone utility, we believe that our monolithic and relatively sim- ple SSD model provides a useful building block for larger systems that employ an object detection component. A promising future direction is to explore its use as part of a sys- tem using recurrent neural networks to detect and track objects in video simultaneously.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;除了单独使用之外，我们相信我们的整体和相对简单的SSD模型为采用目标检测组件的大型系统提供了有用的构建模块。一个有前景的未来方向是探索它作为系统的一部分，使用循环神经网络来同时检测和跟踪视频中的目标。&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;6-acknowledgment&quot;&gt;6 Acknowledgment&lt;/h3&gt;
&lt;p&gt;This work was started as an internship project at Google and continued at UNC. We would like to thank Alex Toshev for helpful discussions and are indebted to the Im- age Understanding and DistBelief teams at Google. We also thank Philip Ammirato and Patrick Poirson for helpful comments. We thank NVIDIA for providing GPUs and acknowledge support from NSF 1452851, 1446631, 1526367, 1533771.&lt;/p&gt;

&lt;h4 id=&quot;6-致谢&quot;&gt;6. 致谢&lt;/h4&gt;
&lt;p&gt;这项工作是在谷歌的一个实习项目开始的，并在UNC继续。我们要感谢Alex Toshev进行有益的讨论，并感谢Google的Image Understanding和DistBelief团队。我们也感谢Philip Ammirato和Patrick Poirson提供有用的意见。我们感谢NVIDIA提供的GPU，并对NSF 1452851,1446631,1526367,1533771的支持表示感谢。&lt;/p&gt;

&lt;p&gt;References&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Uijlings, J.R., van de Sande, K.E., Gevers, T., Smeulders, A.W.: Selective search for object recognition. IJCV (2013)&lt;/li&gt;
  &lt;li&gt;Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object detection with region proposal networks. In: NIPS. (2015)&lt;/li&gt;
  &lt;li&gt;He,K.,Zhang,X.,Ren,S.,Sun,J.:Deepresiduallearningforimagerecognition.In:CVPR. (2016)&lt;/li&gt;
  &lt;li&gt;Sermanet,P.,Eigen,D.,Zhang,X.,Mathieu,M.,Fergus,R.,LeCun,Y.:Overfeat:Integrated recognition, localization and detection using convolutional networks. In: ICLR. (2014)&lt;/li&gt;
  &lt;li&gt;Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: Unified, real-time object detection. In: CVPR. (2016)&lt;/li&gt;
  &lt;li&gt;Girshick, R.: Fast R-CNN. In: ICCV. (2015)&lt;/li&gt;
  &lt;li&gt;Erhan, D., Szegedy, C., Toshev, A., Anguelov, D.: Scalable object detection using deep
neural networks. In: CVPR. (2014)&lt;/li&gt;
  &lt;li&gt;Szegedy, C., Reed, S., Erhan, D., Anguelov, D.: Scalable, high-quality object detection.
arXiv preprint arXiv:1412.1441 v3 (2015)&lt;/li&gt;
  &lt;li&gt;He, K., Zhang, X., Ren, S., Sun, J.: Spatial pyramid pooling in deep convolutional networks
for visual recognition. In: ECCV. (2014)&lt;/li&gt;
  &lt;li&gt;Long,J.,Shelhamer,E.,Darrell,T.:Fullyconvolutionalnetworksforsemanticsegmentation.
In: CVPR. (2015)&lt;/li&gt;
  &lt;li&gt;Hariharan, B., Arbela ́ez, P., Girshick, R., Malik, J.: Hypercolumns for object segmentation
and fine-grained localization. In: CVPR. (2015)&lt;/li&gt;
  &lt;li&gt;Liu,W.,Rabinovich,A.,Berg,A.C.:ParseNet:Lookingwidertoseebetter.In:ILCR.(2016)&lt;/li&gt;
  &lt;li&gt;Zhou,B.,Khosla,A.,Lapedriza,A.,Oliva,A.,Torralba,A.:Objectdetectorsemergeindeep
scene cnns. In: ICLR. (2015)&lt;/li&gt;
  &lt;li&gt;Howard, A.G.: Some improvements on deep convolutional neural network based image
classification. arXiv preprint arXiv:1312.5402 (2013)&lt;/li&gt;
  &lt;li&gt;Simonyan,K.,Zisserman,A.:Verydeepconvolutionalnetworksforlarge-scaleimagerecog-
nition. In: NIPS. (2015)&lt;/li&gt;
  &lt;li&gt;Russakovsky,O.,Deng,J.,Su,H.,Krause,J.,Satheesh,S.,Ma,S.,Huang,Z.,Karpathy,A.,
Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: Imagenet large scale visual recognition
challenge. IJCV (2015)&lt;/li&gt;
  &lt;li&gt;Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Semantic image seg-
mentation with deep convolutional nets and fully connected crfs. In: ICLR. (2015)&lt;/li&gt;
  &lt;li&gt;Holschneider,M.,Kronland-Martinet,R.,Morlet,J.,Tchamitchian,P.:Areal-timealgorithm for signal analysis with the help of the wavelet transform. In: Wavelets. Springer (1990)
286–297&lt;/li&gt;
  &lt;li&gt;Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S.,
Darrell, T.: Caffe: Convolutional architecture for fast feature embedding. In: MM. (2014)&lt;/li&gt;
  &lt;li&gt;Glorot, X., Bengio, Y.: Understanding the difficulty of training deep feedforward neural
networks. In: AISTATS. (2010)&lt;/li&gt;
  &lt;li&gt;Hoiem, D., Chodpathumwan, Y., Dai, Q.: Diagnosing error in object detectors. In: ECCV&lt;/li&gt;
  &lt;li&gt;(2012)&lt;/li&gt;
  &lt;li&gt;Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object
detection and semantic segmentation. In: CVPR. (2014)&lt;/li&gt;
  &lt;li&gt;Zhang, L., Lin, L., Liang, X., He, K.: Is faster r-cnn doing well for pedestrian detection. In:
ECCV. (2016)&lt;/li&gt;
  &lt;li&gt;Bell,S.,Zitnick,C.L.,Bala,K.,Girshick,R.:Inside-outsidenet:Detectingobjectsincontext
with skip pooling and recurrent neural networks. In: CVPR. (2016)&lt;/li&gt;
  &lt;li&gt;COCO: Common Objects in Context. http://mscoco.org/dataset/
#detections-leaderboard (2016) [Online; accessed 25-July-2016].&lt;/li&gt;
  &lt;li&gt;Felzenszwalb, P., McAllester, D., Ramanan, D.: A discriminatively trained, multiscale, de-
formable part model. In: CVPR. (2008)&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Fri, 28 Dec 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/12/ssd/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/12/ssd/</guid>
        
        <category>深度学习-视觉</category>
        
        
        <category>深度学习-视觉</category>
        
      </item>
    
  </channel>
</rss>
