<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>倔强的生命！</title>
    <description>欢迎~</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 03 Jan 2019 19:59:49 +0800</pubDate>
    <lastBuildDate>Thu, 03 Jan 2019 19:59:49 +0800</lastBuildDate>
    <generator>Jekyll v3.8.3</generator>
    
      <item>
        <title>torch的dataloader中的‘pin_memory’指的是什么</title>
        <description>&lt;!--more--&gt;

&lt;p&gt;在用dataloader加载数据的时候，发现有的时候会加’pip_memory=True’有时候是’False’，就好奇的查了一下，也找了一些资料关于这方面的介绍的。&lt;/p&gt;

&lt;p&gt;首先是关于pin_memory的介绍&lt;/p&gt;

&lt;h3 id=&quot;cuda-笔记中的&quot;&gt;cuda 笔记中的&lt;/h3&gt;

&lt;p&gt;主机端存在虚拟内存，主机内存不足是会将内存数据交换到虚拟内存中，虚拟内存就是主机中的磁盘空间，需要该页时再重新从磁盘加载回来。这样做可以使用比实际内存更大的内存空间。&lt;/p&gt;

&lt;p&gt;锁页内存允许GPU上的MDA控制器在使用主机内存时不用CPU参与。GPU上的显存都是锁页的，因为GPU上的内存时不支持交换到磁盘的。锁页内存就是分配主机内存时锁定该页，让其不与磁盘交换。&lt;/p&gt;

&lt;p&gt;CUDA中锁页内存的使用可以使用CUDA驱动API（ driver API’s）cuMemAllocHost()或者使用CUDA的运行时API（runtime API）中的cudaMallocHost()。除此之外还可以直接用主机上Malloc()分配的空间，然后将其注册为锁页内存（使用cudaHostRegister()函数完成注册）。&lt;/p&gt;

&lt;p&gt;使用锁页内存的好处有以下几点：&lt;/p&gt;

&lt;p&gt;1.设备内存与锁页内存之间的数据传输可以与内核执行并行处理。&lt;/p&gt;

&lt;p&gt;2.锁页内存可以映射到设备内存，减少设备与主机的数据传输。&lt;/p&gt;

&lt;p&gt;3.在前端总线的主机系统锁页内存与设备内存之间的数据交换会比较快；并且可以是write-combining的，此时带宽会跟大。&lt;/p&gt;

&lt;p&gt;如果要所有的线程都可以使用锁页内存的好处，需要在分配时将cudaHostAllocPortable标志传给cudaMallocHost()，或者将cudaHostRegisterPortable标志传给函数cudaHostRegister()&lt;/p&gt;

&lt;p&gt;write-combining内存，默认情况下锁页内存时可缓存的，可以再使用cudaMallocHost()函数时使用cudaHostAllocWriteCombined标志声明为write-combining的，write-combining内存没有一二级缓存，这样其他的应用可拥有更多的缓存资源。此外write-combining在PCI总线的系统中没有snooped过程，可以获得高达40%的传输加速。但是从主机读取write-combining内存速度很慢，因此应该用于主机端只写的数据。&lt;/p&gt;

&lt;p&gt;要讲锁页内存映射到设备内存的地址空间还需要在cudaMalloHost()中使用cudaHost- AllocMapped标志，或者在用cudaHostRegister()函数注册时使用标志cudaHostRegisterMapped&lt;/p&gt;

&lt;p&gt;用来分配一块被映射到设备内存空间的锁页内存。这样的锁页内存会有两个内存地址：主机上的内存地址和设备上的内存地址。主机内存地址直接由函数cudaMallocHost()或Malloc()返回，设备内存地址则由函数cudaHostGetDevicePointer()查询，用以在kernel中访问锁页内存。&lt;/p&gt;

&lt;h3 id=&quot;pytorch中的这个&quot;&gt;pytorch中的这个&lt;/h3&gt;

&lt;p&gt;pin_memory就是锁页内存，创建DataLoader时，设置pin_memory=True，则意味着生成的Tensor数据最开始是属于内存中的锁页内存，这样将内存的Tensor转义到GPU的显存就会更快一些。&lt;/p&gt;

&lt;p&gt;主机中的内存，有两种存在方式，一是锁页，二是不锁页，锁页内存存放的内容在任何情况下都不会与主机的虚拟内存进行交换（注：虚拟内存就是硬盘），而不锁页内存在主机内存不足时，数据会存放在虚拟内存中。&lt;/p&gt;

&lt;p&gt;而显卡中的显存全部是锁页内存！&lt;/p&gt;

&lt;p&gt;当计算机的内存充足的时候，可以设置pin_memory=True。当系统卡住，或者交换内存使用过多的时候，设置pin_memory=False。因为pin_memory与电脑硬件性能有关，pytorch开发者不能确保每一个炼丹玩家都有高端设备，因此pin_memory默认为False。&lt;/p&gt;
</description>
        <pubDate>Tue, 01 Jan 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/01/torch_dataloader/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/01/torch_dataloader/</guid>
        
        <category>torch</category>
        
        
        <category>torch</category>
        
      </item>
    
      <item>
        <title>torch常用总结（不断更新）</title>
        <description>&lt;!--more--&gt;

&lt;h4 id=&quot;torchchunk&quot;&gt;torch.chunk()&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a = torch.randn(10,10)

&amp;gt;&amp;gt;&amp;gt; a,b,c,d = a.chunk(4, dim=1)
&amp;gt;&amp;gt;&amp;gt; a
tensor([[-0.1161, -1.1078,  0.0536],
        [ 0.0428, -0.2843,  0.4174],
        [-1.6897,  0.0662, -2.1043],
        [-1.9352,  0.4618, -0.2195],
        [ 0.0203, -0.4590,  0.4475],
        [-0.5266, -0.5123, -1.2169],
        [-0.5326, -0.1897,  1.3600],
        [-0.5801, -0.0573, -0.9565],
        [ 0.1664, -0.1459,  0.3236],
        [-0.3293,  1.5720,  0.1008]])
&amp;gt;&amp;gt;&amp;gt; a.shape
torch.Size([10, 3])
&amp;gt;&amp;gt;&amp;gt; a.size
&amp;lt;built-in method size of Tensor object at 0x7f1bbd30fdc8&amp;gt;
&amp;gt;&amp;gt;&amp;gt; 
&amp;gt;&amp;gt;&amp;gt; 
&amp;gt;&amp;gt;&amp;gt; a.size()
torch.Size([10, 3])
&amp;gt;&amp;gt;&amp;gt; b.shape
torch.Size([10, 3])
&amp;gt;&amp;gt;&amp;gt; c.shape
torch.Size([10, 3])
&amp;gt;&amp;gt;&amp;gt; d.shape
torch.Size([10, 1])


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;意思就是把一个array分成几个块儿, 如果不整除的话，最后一个是不够的。&lt;/p&gt;

&lt;h3 id=&quot;torchclampinput-min-max-outnone&quot;&gt;torch.clamp(input, min, max, out=None)&lt;/h3&gt;
&lt;p&gt;这个最初是在bbox上面看到的，即bbox预测出来之后要调一下，&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
class ClipBoxes(nn.Module):

    def __init__(self, width=None, height=None):
        super(ClipBoxes, self).__init__()

    def forward(self, boxes, img):   # img 是原图。

        batch_size, num_channels, height, width = img.shape   # 就是为了传入shape.

        boxes[:, :, 0] = torch.clamp(boxes[:, :, 0], min=0)   # 要把xmin和ymin最小弄到0，xmax和ymax弄到width, height
        boxes[:, :, 1] = torch.clamp(boxes[:, :, 1], min=0)

        boxes[:, :, 2] = torch.clamp(boxes[:, :, 2], max=width)
        boxes[:, :, 3] = torch.clamp(boxes[:, :, 3], max=height)
      
        return boxes


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;主要是为了防止出现超过范围的，所以需要“夹”一下。&lt;/p&gt;

&lt;p&gt;看下面的例子&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a = torch.rand((10,3))
&amp;gt;&amp;gt;&amp;gt; a
tensor([[0.6708, 0.4612, 0.9626],
        [0.7909, 0.4636, 0.1710],
        [0.6627, 0.1952, 0.3010],
        [0.8106, 0.4771, 0.0902],
        [0.4339, 0.7132, 0.9819],
        [0.8299, 0.6124, 0.2905],
        [0.9245, 0.9359, 0.8453],
        [0.6994, 0.7530, 0.7778],
        [0.0879, 0.6195, 0.3117],
        [0.9031, 0.0375, 0.1836]])
&amp;gt;&amp;gt;&amp;gt; a = torch.clamp(a, min=0.2, max=0.8)
&amp;gt;&amp;gt;&amp;gt; a
tensor([[0.6708, 0.4612, 0.8000],
        [0.7909, 0.4636, 0.2000],
        [0.6627, 0.2000, 0.3010],
        [0.8000, 0.4771, 0.2000],
        [0.4339, 0.7132, 0.8000],
        [0.8000, 0.6124, 0.2905],
        [0.8000, 0.8000, 0.8000],
        [0.6994, 0.7530, 0.7778],
        [0.2000, 0.6195, 0.3117],
        [0.8000, 0.2000, 0.2000]])
&amp;gt;&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;即小于&lt;code class=&quot;highlighter-rouge&quot;&gt;min&lt;/code&gt;的给弄成最小的，大于&lt;code class=&quot;highlighter-rouge&quot;&gt;max&lt;/code&gt;的弄成最大的，这在实变函数中经常见到这种操作。&lt;/p&gt;

&lt;h3 id=&quot;torchrandn23--torchrandn23&quot;&gt;torch.randn(2,3)  torch.randn((2,3))&lt;/h3&gt;

&lt;p&gt;这个是产生标准的正态分布的随机数，即以0为均值以1为标准差的随机数。&lt;/p&gt;

&lt;h3 id=&quot;torchrand23-torchrand23&quot;&gt;torch.rand((2,3)) torch.rand((2,3))&lt;/h3&gt;
&lt;p&gt;这个是从[0,1]之间的均匀分布中抽取一组随机数&lt;/p&gt;

&lt;h3 id=&quot;torchmax&quot;&gt;torch.max()&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import torch
&amp;gt;&amp;gt;&amp;gt; a = torch.randn((2,3))
&amp;gt;&amp;gt;&amp;gt; a
tensor([[ 0.3010,  0.9597, -0.3248],
        [ 0.2432,  0.6206,  1.5890]])
&amp;gt;&amp;gt;&amp;gt; torch.max(a, dim=1)
(tensor([0.9597, 1.5890]), tensor([1, 2]))
&amp;gt;&amp;gt;&amp;gt; torch.max(a, dim=0)
(tensor([0.3010, 0.9597, 1.5890]), tensor([0, 0, 1]))


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;其中dim1的时候是求每一行的最大值，dim=0的时候是求每一列的最大值。&lt;/p&gt;

&lt;h3 id=&quot;torchlt&quot;&gt;torch.lt()&lt;/h3&gt;

&lt;p&gt;看下面的例子&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a = torch.randn((2,3))
&amp;gt;&amp;gt;&amp;gt; a
tensor([[-0.3616,  1.8169, -0.0667],
        [-0.9241,  0.2994, -0.8512]])
&amp;gt;&amp;gt;&amp;gt; a[torch.lt(a,0.4)] =1.
&amp;gt;&amp;gt;&amp;gt; a
tensor([[1.0000, 1.8169, 1.0000],
        [1.0000, 1.0000, 1.0000]])
&amp;gt;&amp;gt;&amp;gt; torch.lt(a,1.)
tensor([[0, 0, 0],
        [0, 0, 0]], dtype=torch.uint8)
&amp;gt;&amp;gt;&amp;gt; torch.lt(a,1.5)
tensor([[1, 0, 1],
        [1, 1, 1]], dtype=torch.uint8)


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;torchgt-or-torchge&quot;&gt;torch.gt() or torch.ge()&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a = torch.randn((2,3))
&amp;gt;&amp;gt;&amp;gt; a
tensor([[-1.2540,  1.4163,  0.3262],
        [ 0.8598, -0.3746,  0.4843]])
&amp;gt;&amp;gt;&amp;gt; torch.gt(a, 0.5)
tensor([[0, 1, 0],
        [1, 0, 0]], dtype=torch.uint8)
&amp;gt;&amp;gt;&amp;gt; a[torch.gt(a,0.5)]=2
&amp;gt;&amp;gt;&amp;gt; a
tensor([[-1.2540,  2.0000,  0.3262],
        [ 2.0000, -0.3746,  0.4843]])


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;torcheq&quot;&gt;torch.eq()&lt;/h3&gt;

&lt;p&gt;和上面的两个的用法差不多&lt;/p&gt;

&lt;h3 id=&quot;torchwhere&quot;&gt;torch.where()&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&amp;gt;&amp;gt;&amp;gt; a = torch.randn((2,3))
&amp;gt;&amp;gt;&amp;gt; a
tensor([[ 0.4385, -2.6319,  0.7710],
        [ 0.6759,  1.0058,  1.2362]])
&amp;gt;&amp;gt;&amp;gt; a = torch.where(torch.lt(a, 1), a, 1-a)
&amp;gt;&amp;gt;&amp;gt; a
tensor([[ 0.4385, -2.6319,  0.7710],
        [ 0.6759, -0.0058, -0.2362]])
&amp;gt;&amp;gt;&amp;gt; 

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;相当于一个分段的函数一样，条件满足的地方是a,不满足的地方是1-a。&lt;/p&gt;

</description>
        <pubDate>Fri, 28 Dec 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/12/torch-common/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/12/torch-common/</guid>
        
        <category>torch</category>
        
        
        <category>torch</category>
        
      </item>
    
      <item>
        <title>fcn论文笔记</title>
        <description>&lt;!--more--&gt;

&lt;p&gt;fcn之前读过一次，现在在想想其中的一些值得回味的东西,图像分割任务是将一幅图中各个实体的边界确定下来，这样就可以通过寻找这个实体的边界来确定实体的位置，感觉会在自动驾驶任务中用的非常多。而目前的做法是采用分类的办法来做的，即对图像的每个像素点都进行分类，分类完成后，最后把相邻并且属于同一类别的像素点聚集起来。&lt;/p&gt;

&lt;h3 id=&quot;fcn的方案&quot;&gt;fcn的方案：&lt;/h3&gt;

&lt;p&gt;fcn不包含全连接层，但是它为什么要设计成这样呢？&lt;/p&gt;

&lt;p&gt;先recall一下fc层的用处，比如在Alexnet做cifar10的分类的时候，每个图片经过提取特征转化成为了一个长为4096的向量，然后就进入了fc层去做了分类，那时候一张图上只有一个物体，而现在一张图上可能有多个物体，如果再用fc层的话，也需要把得到的特征flatten一个一维的，然后才能进入fc层，但是这样就失去了其位置的信息，而全部采用cnn就可以避免出现上面的问题，也就是说使用全cnn是一种目前想到的解决方式。&lt;/p&gt;

&lt;h3 id=&quot;新的问题&quot;&gt;新的问题：&lt;/h3&gt;

&lt;p&gt;既然要对每个像素都进行分类，那么输出的shape和输入的shape一定是一样的，不然就不会是每个像素点的分类了，那这样一个自然的想法是，在每经过一个层的时候都不要改变其shape就行了，这样输出就和输入的shape肯定是一样的了，但是这样新的问题出现了。因为都用同一个shape的话，计算量明显增加了，特别是对于输入比较大的情况，这是不符合’小，轻，快，准’的要求的.&lt;/p&gt;

&lt;p&gt;进一步想，能不能先将信息提出出来再”还原”出来，这就是现在经常看到的一种encode-decode的结构，而且还可以利用不同scale的feature,fcn也是这种架构，里面进行了卷积和反卷积的操作，卷积是变小的操作，反卷积是变大的操作，关于变大的操作有的是直接用的’upsampling’，但是这样太暴力了，所以往往会在’upsampling’之前和之后都接一个小的卷积层，让其先适应一下。fcn中用的反卷积操作也是为了防止太暴力而提出的解决方案，不过需要注意的是，其实任何形式的这种由小维度变成大维度的时候，都会有一些精度损失，为了提高精度，采用的办法是融合，即融合不同scale的特征。&lt;/p&gt;

&lt;h3 id=&quot;评测&quot;&gt;评测&lt;/h3&gt;

&lt;p&gt;虽然是对每个像素都做了分类，但是计算Loss的时候好像并不是用的所有的像素点，感觉这样是对的，因为如果所有的像素点都用到的话，首先是计算量大，其次的原因是相邻的像素点的信息可能大多数都一样（只有两个物体的边界的那些部分不太一样），比如一张图上如果有一只老虎和一只蚂蚁的话，那如果所有的像素点全部采用的话，loss将会由属于老虎的那些像素点来提供，那么就会偏重于老虎这一部分。&lt;/p&gt;

&lt;p&gt;评测的办法用的是IOU， 这个感觉也挺自然的，一方面预测的有一片区域A，而gt也有一部分区域B，所以自然地想到的是IOU，注意IOU只是一个让人来看的指标，并不是Loss，这种想法也可以用到其他的任务上面，比如缺陷检测。&lt;/p&gt;

</description>
        <pubDate>Wed, 26 Dec 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/12/fcn/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/12/fcn/</guid>
        
        <category>论文笔记</category>
        
        
        <category>论文笔记</category>
        
      </item>
    
      <item>
        <title>opencv中的一些用法积累(不断更新)</title>
        <description>&lt;!--more--&gt;

&lt;h3 id=&quot;cv2splitimg&quot;&gt;cv2.split(img)&lt;/h3&gt;
&lt;p&gt;这个是相当于是获得其中的每个通道
比如&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; img = cv2.imread(&quot;1.jpg&quot;)
&amp;gt;&amp;gt;&amp;gt; img.shape
(480, 640, 3)
&amp;gt;&amp;gt;&amp;gt; cv2.split(img)
[array([[  2,   0,   3, ...,  32,  31,  37],
       [  8,   0,   5, ...,  25,  27,  32],
       [  9,   0,   5, ...,  34,  23,  16],
       ...,
       [140, 142, 147, ...,  14,   9,   7],
       [140, 143, 150, ...,  10,   7,   8],
       [137, 140, 146, ...,  10,   7,   9]], dtype=uint8), array([[ 19,   4,   3, ...,  33,  22,  23],
       [ 17,   5,   6, ...,  21,  22,  27],
       [  7,   0,   4, ...,  24,  21,  20],
       ...,
       [112, 111, 114, ...,  12,  14,  19],
       [112, 112, 117, ...,  13,  13,  17],
       [110, 111, 115, ...,  16,  13,  15]], dtype=uint8), array([[16,  3,  3, ..., 24, 18, 24],
       [ 4,  0,  0, ..., 16, 21, 28],
       [ 6,  1, 13, ..., 24, 21, 21],
       ...,
       [77, 78, 81, ..., 18, 17, 21],
       [78, 79, 84, ..., 21, 18, 20],
       [76, 80, 84, ..., 23, 18, 20]], dtype=uint8)]
&amp;gt;&amp;gt;&amp;gt; ret = cv2.split(img)
&amp;gt;&amp;gt;&amp;gt; len(ret)
3
&amp;gt;&amp;gt;&amp;gt; ret[0].shape
(480, 640)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;cv2mergeimg1img2img3&quot;&gt;cv2.merge([img1,img2,img3])&lt;/h3&gt;

&lt;p&gt;这个和上面的刚好是相反的，即把各个通道的给merge起来。&lt;/p&gt;

&lt;p&gt;上面的两个函数&lt;code class=&quot;highlighter-rouge&quot;&gt;split&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;merge&lt;/code&gt;就可以实现一些算法，比如随机的对图像的通道进行交换。因为就6种情况，所以比较容易。&lt;/p&gt;

&lt;h3 id=&quot;cv2line-用来画线&quot;&gt;cv2.line() 用来画线&lt;/h3&gt;

&lt;p&gt;见下面的代码&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import cv2 
import matplotlib.pyplot as plt 

import numpy as np

img  = cv2.imread(&quot;1.jpg&quot;)

cv2.line(img, (10,10), (10,300), (0,255,0), 2)
cv2.line(img, (10,300), (300,300),(0,255,0), 2)
cv2.line(img,(300,300),(300,10), (0,255,0), 2)
cv2.line(img,(300,10),(10,10), (0,255,0), 2)

plt.imshow(img[...,::-1])
plt.show()
~                                                                                                               
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;要注意的是cv和plt之间要转通道,并且上面的第一条画的是竖着的。&lt;/p&gt;
</description>
        <pubDate>Wed, 26 Dec 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/12/opencv/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/12/opencv/</guid>
        
        <category>python</category>
        
        
        <category>python</category>
        
      </item>
    
      <item>
        <title>torch常用总结（不断更新）</title>
        <description>&lt;!--more--&gt;

&lt;h4 id=&quot;torchchunk&quot;&gt;torch.chunk()&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a = torch.randn(10,10)

&amp;gt;&amp;gt;&amp;gt; a,b,c,d = a.chunk(4, dim=1)
&amp;gt;&amp;gt;&amp;gt; a
tensor([[-0.1161, -1.1078,  0.0536],
        [ 0.0428, -0.2843,  0.4174],
        [-1.6897,  0.0662, -2.1043],
        [-1.9352,  0.4618, -0.2195],
        [ 0.0203, -0.4590,  0.4475],
        [-0.5266, -0.5123, -1.2169],
        [-0.5326, -0.1897,  1.3600],
        [-0.5801, -0.0573, -0.9565],
        [ 0.1664, -0.1459,  0.3236],
        [-0.3293,  1.5720,  0.1008]])
&amp;gt;&amp;gt;&amp;gt; a.shape
torch.Size([10, 3])
&amp;gt;&amp;gt;&amp;gt; a.size
&amp;lt;built-in method size of Tensor object at 0x7f1bbd30fdc8&amp;gt;
&amp;gt;&amp;gt;&amp;gt; 
&amp;gt;&amp;gt;&amp;gt; 
&amp;gt;&amp;gt;&amp;gt; a.size()
torch.Size([10, 3])
&amp;gt;&amp;gt;&amp;gt; b.shape
torch.Size([10, 3])
&amp;gt;&amp;gt;&amp;gt; c.shape
torch.Size([10, 3])
&amp;gt;&amp;gt;&amp;gt; d.shape
torch.Size([10, 1])


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;意思就是把一个array分成几个块儿, 如果不整除的话，最后一个是不够的。&lt;/p&gt;

&lt;h3 id=&quot;torchclampinput-min-max-outnone&quot;&gt;torch.clamp(input, min, max, out=None)&lt;/h3&gt;
&lt;p&gt;这个最初是在bbox上面看到的，即bbox预测出来之后要调一下，&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
class ClipBoxes(nn.Module):

    def __init__(self, width=None, height=None):
        super(ClipBoxes, self).__init__()

    def forward(self, boxes, img):   # img 是原图。

        batch_size, num_channels, height, width = img.shape   # 就是为了传入shape.

        boxes[:, :, 0] = torch.clamp(boxes[:, :, 0], min=0)   # 要把xmin和ymin最小弄到0，xmax和ymax弄到width, height
        boxes[:, :, 1] = torch.clamp(boxes[:, :, 1], min=0)

        boxes[:, :, 2] = torch.clamp(boxes[:, :, 2], max=width)
        boxes[:, :, 3] = torch.clamp(boxes[:, :, 3], max=height)
      
        return boxes


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;主要是为了防止出现超过范围的，所以需要“夹”一下。&lt;/p&gt;

&lt;p&gt;看下面的例子&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a = torch.rand((10,3))
&amp;gt;&amp;gt;&amp;gt; a
tensor([[0.6708, 0.4612, 0.9626],
        [0.7909, 0.4636, 0.1710],
        [0.6627, 0.1952, 0.3010],
        [0.8106, 0.4771, 0.0902],
        [0.4339, 0.7132, 0.9819],
        [0.8299, 0.6124, 0.2905],
        [0.9245, 0.9359, 0.8453],
        [0.6994, 0.7530, 0.7778],
        [0.0879, 0.6195, 0.3117],
        [0.9031, 0.0375, 0.1836]])
&amp;gt;&amp;gt;&amp;gt; a = torch.clamp(a, min=0.2, max=0.8)
&amp;gt;&amp;gt;&amp;gt; a
tensor([[0.6708, 0.4612, 0.8000],
        [0.7909, 0.4636, 0.2000],
        [0.6627, 0.2000, 0.3010],
        [0.8000, 0.4771, 0.2000],
        [0.4339, 0.7132, 0.8000],
        [0.8000, 0.6124, 0.2905],
        [0.8000, 0.8000, 0.8000],
        [0.6994, 0.7530, 0.7778],
        [0.2000, 0.6195, 0.3117],
        [0.8000, 0.2000, 0.2000]])
&amp;gt;&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;即小于&lt;code class=&quot;highlighter-rouge&quot;&gt;min&lt;/code&gt;的给弄成最小的，大于&lt;code class=&quot;highlighter-rouge&quot;&gt;max&lt;/code&gt;的弄成最大的，这在实变函数中经常见到这种操作。&lt;/p&gt;

</description>
        <pubDate>Mon, 24 Dec 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/12/torch-common/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/12/torch-common/</guid>
        
        <category>torch</category>
        
        
        <category>torch</category>
        
      </item>
    
      <item>
        <title>numpy之每次看每次忘(还在更新中)</title>
        <description>&lt;!--more--&gt;

&lt;h3 id=&quot;基本的&quot;&gt;基本的&lt;/h3&gt;
&lt;p&gt;ndim, shape, size（元素的总个数）, dtype, itemsize（数组中每个元素的字节大小），其它都见名知义&lt;/p&gt;

&lt;p&gt;另外关于numpy里面的数据类型也应该知道。&lt;/p&gt;

&lt;p&gt;看下面的&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import numpy as np
&amp;gt;&amp;gt;&amp;gt; a = np.arange(20)
&amp;gt;&amp;gt;&amp;gt; a
array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
       17, 18, 19])
&amp;gt;&amp;gt;&amp;gt; type(a[0])
&amp;lt;type 'numpy.int64'&amp;gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这说明numpy存的int的数值，默认是&lt;code class=&quot;highlighter-rouge&quot;&gt;int64&lt;/code&gt;，是8个节节的, 如果转成&lt;code class=&quot;highlighter-rouge&quot;&gt;int32&lt;/code&gt;的话就是4个字节了。
其实&lt;code class=&quot;highlighter-rouge&quot;&gt;int32,float32&lt;/code&gt;都是4个字节的，而&lt;code class=&quot;highlighter-rouge&quot;&gt;float64, int64&lt;/code&gt;是8个字节的。&lt;/p&gt;

&lt;h3 id=&quot;常用的&quot;&gt;常用的&lt;/h3&gt;

&lt;p&gt;reshape（），可以reshape((m, n))或者reshape(m,n)&lt;/p&gt;

&lt;h3 id=&quot;nplinspace023&quot;&gt;np.linspace(0,2,3)&lt;/h3&gt;
&lt;p&gt;意思是从0到2，产生3个数，结果是array[0.,1.,2.]
注意这个不是linespace,有次我就写错了，还找不到bug。
这个非常好用，类似于range(1,10,2)但是linspace可以产生小数的，等分的时候很好用。&lt;/p&gt;

&lt;h3 id=&quot;nprandomrandd0d1d2&quot;&gt;np.random.rand(d0,d1,d2,…)&lt;/h3&gt;

&lt;p&gt;这个是产生0到1之间的均匀分布的，可以是好几维的，里面不必加括号
和&lt;/p&gt;
&lt;h3 id=&quot;nprandomrandomsize好像是一样的&quot;&gt;np.random.random(size)好像是一样的，&lt;/h3&gt;

&lt;h3 id=&quot;nprandomrandnd0d1d2&quot;&gt;np.random.randn(d0,d1,d2,…)&lt;/h3&gt;

&lt;p&gt;举个例子吧，如果要产生一个均值为3，标准差为4的，（2，2）维的正态分布的话，可以这样
3+ 4 * np.random.randn(2,2)&lt;/p&gt;

&lt;h3 id=&quot;nprandomrandintlowhighnone-sizenone-dtypei&quot;&gt;np.random.randint(low,high=None, size=None, dtype=’i’)&lt;/h3&gt;
&lt;p&gt;这个常常用到，用于产生从low到high的size个随机的整数，注意是有放回的，不包括high,
low如果不写就认为是0.
size可以是一维的,也可以是个tuple，比如(3,4)&lt;/p&gt;

&lt;h3 id=&quot;nprandomchoicea-sizenone-replacetrue-pnone&quot;&gt;np.random.choice(a, size=None, replace=True, p=None)&lt;/h3&gt;
&lt;p&gt;这个也非常的好,常常做随机选择的时候，而且还可以给定以某种概率。
replace=False指的是无放回随机选择,这个应该用的会比较多
比如下面的例子&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import numpy as np
&amp;gt;&amp;gt;&amp;gt; np.random.choice(range(20), 10, replace=False)
array([19, 18, 16,  5, 15, 13,  1,  7,  0, 17])
&amp;gt;&amp;gt;&amp;gt; np.random.choice(range(20), 10, replace=True)
array([18, 19,  8,  5, 18, 19, 16,  5, 19, 12])
&amp;gt;&amp;gt;&amp;gt; 


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;默认的是’True’.&lt;/p&gt;

&lt;h3 id=&quot;npnewaxis&quot;&gt;np.newaxis&lt;/h3&gt;
&lt;p&gt;是添加一个新的维度的意思，
比如
a = np.array(range(10))
的shape是(10,)
但是如果b = a[:, np.newaxis]
的话，b的shape就是(10,1)了。&lt;/p&gt;

&lt;h3 id=&quot;npcumsuma-axisnone-dtypenone-outnone&quot;&gt;np.cumsum(a, axis=None, dtype=None, out=None)&lt;/h3&gt;
&lt;p&gt;这个非常的好，特别是在算roc, ap, 这此的时候，
意思是累积求和，
所以在求roc，ap的时候比较好用。&lt;/p&gt;

&lt;p&gt;见例子&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import numpy as np
&amp;gt;&amp;gt;&amp;gt; a = np.array(range(20))
&amp;gt;&amp;gt;&amp;gt; b = np.cumsum(a)
&amp;gt;&amp;gt;&amp;gt; a
array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
       17, 18, 19])
&amp;gt;&amp;gt;&amp;gt; b
array([  0,   1,   3,   6,  10,  15,  21,  28,  36,  45,  55,  66,  78,
        91, 105, 120, 136, 153, 171, 190])

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;npclipa-a_min-a_max-outnone&quot;&gt;np.clip(a, a_min, a_max, out=None)&lt;/h3&gt;

&lt;p&gt;这个是clip一个数组中的值，如果小于a_min的话，就变成a_min,大于a_max的话就变成a_max，&lt;/p&gt;

&lt;p&gt;见下面的例子&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import numpy as np
&amp;gt;&amp;gt;&amp;gt;
&amp;gt;&amp;gt;&amp;gt; a = np.arange(10)
&amp;gt;&amp;gt;&amp;gt; a
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
&amp;gt;&amp;gt;&amp;gt; np.clip(a, 3,7)
array([3, 3, 3, 3, 4, 5, 6, 7, 7, 7])
&amp;gt;&amp;gt;&amp;gt; a
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
&amp;gt;&amp;gt;&amp;gt; np.clip(a, 3,7, a)
array([3, 3, 3, 3, 4, 5, 6, 7, 7, 7])
&amp;gt;&amp;gt;&amp;gt; a
array([3, 3, 3, 3, 4, 5, 6, 7, 7, 7])
&amp;gt;&amp;gt;&amp;gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;其中最后一个参数是out，如果out为自身的话，那这个值将会改变。&lt;/p&gt;

&lt;h3 id=&quot;npmeshgrid&quot;&gt;np.meshgrid()&lt;/h3&gt;

&lt;p&gt;见下面的例子。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a = np.arange(3)
&amp;gt;&amp;gt;&amp;gt; b = np.linspace(0,1,5)
&amp;gt;&amp;gt;&amp;gt; np.meshgrid(a,b)
[array([[0, 1, 2],
       [0, 1, 2],
       [0, 1, 2],
       [0, 1, 2],
       [0, 1, 2]]), array([[0.  , 0.  , 0.  ],
       [0.25, 0.25, 0.25],
       [0.5 , 0.5 , 0.5 ],
       [0.75, 0.75, 0.75],
       [1.  , 1.  , 1.  ]])]
&amp;gt;&amp;gt;&amp;gt; c, d = np.meshgrid(a,b)
&amp;gt;&amp;gt;&amp;gt; c
array([[0, 1, 2],
       [0, 1, 2],
       [0, 1, 2],
       [0, 1, 2],
       [0, 1, 2]])
&amp;gt;&amp;gt;&amp;gt; d
array([[0.  , 0.  , 0.  ],
       [0.25, 0.25, 0.25],
       [0.5 , 0.5 , 0.5 ],
       [0.75, 0.75, 0.75],
       [1.  , 1.  , 1.  ]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;也就是把&lt;code class=&quot;highlighter-rouge&quot;&gt;meshgrid&lt;/code&gt;的返回值的对应的位置配成坐标的话，可以当成grid来看。&lt;/p&gt;

&lt;h3 id=&quot;npcopya-orderk&quot;&gt;np.copy(a, order=’K’)&lt;/h3&gt;
&lt;p&gt;这个感觉就像是deepcopy，看下面的例子&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a = np.array([1,2,3])
&amp;gt;&amp;gt;&amp;gt; a
array([1, 2, 3])
&amp;gt;&amp;gt;&amp;gt; b = a
&amp;gt;&amp;gt;&amp;gt; a[0] =100
&amp;gt;&amp;gt;&amp;gt; b
array([100,   2,   3])
&amp;gt;&amp;gt;&amp;gt; c = np.copy(a)
&amp;gt;&amp;gt;&amp;gt; c
array([100,   2,   3])
&amp;gt;&amp;gt;&amp;gt; a[1]=200
&amp;gt;&amp;gt;&amp;gt; a
array([100, 200,   3])
&amp;gt;&amp;gt;&amp;gt; b
array([100, 200,   3])
&amp;gt;&amp;gt;&amp;gt; c
array([100,   2,   3])
&amp;gt;&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;它的第二个参数，可以指定是按照C语言的形式来存还是按照其它的方式，default是’K’，意思是 match the layout of a  as closely as possible.&lt;/p&gt;

&lt;h3 id=&quot;npwherea1010-还可以这样用&quot;&gt;np.where(a&amp;gt;10,1,0) 还可以这样用&lt;/h3&gt;

&lt;p&gt;看例子&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import numpy as np
&amp;gt;&amp;gt;&amp;gt; a = np.array(range(20)).reshape(4,5)
&amp;gt;&amp;gt;&amp;gt; a
array([[ 0,  1,  2,  3,  4],
       [ 5,  6,  7,  8,  9],
       [10, 11, 12, 13, 14],
       [15, 16, 17, 18, 19]])
&amp;gt;&amp;gt;&amp;gt; a[:,:]= np.where(a&amp;gt;10,1,0)
&amp;gt;&amp;gt;&amp;gt; a
array([[0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0],
       [0, 1, 1, 1, 1],
       [1, 1, 1, 1, 1]])

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;这在产生mask的时候就比较方便了。&lt;/p&gt;

&lt;h3 id=&quot;nptofilefid-sep-formats-npfromfile&quot;&gt;np.tofile(fid, sep=””, format=”%s”)， np.fromfile()&lt;/h3&gt;

&lt;p&gt;这个可以写入二进制文件，filename可以是.bin结尾的，也可以是.txt结尾的
看下面的例子。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import numpy as np
&amp;gt;&amp;gt;&amp;gt; a = np.random.random((10,10))
&amp;gt;&amp;gt;&amp;gt; a
array([[0.59281993, 0.3239744 , 0.40916617, 0.7026653 , 0.73275024,
        0.21081999, 0.53366245, 0.10331709, 0.32546868, 0.22810421],
       [0.98428505, 0.94670514, 0.63903532, 0.27788937, 0.88567724,
        0.37963127, 0.44450239, 0.69552021, 0.9621253 , 0.03284991],
       [0.42321447, 0.05881487, 0.06551279, 0.2043375 , 0.88272984,
        0.24789873, 0.87246889, 0.10402341, 0.12713767, 0.33367603],
       [0.11624641, 0.75637676, 0.20271353, 0.5155519 , 0.36666372,
        0.39928505, 0.30223019, 0.86106991, 0.17976945, 0.83550575],
       [0.98105967, 0.81721926, 0.77524547, 0.36720453, 0.94334179,
        0.59794438, 0.98942932, 0.0531472 , 0.23519734, 0.7861395 ],
       [0.4437484 , 0.50221219, 0.52620174, 0.62602009, 0.91305105,
        0.98763546, 0.24418486, 0.56778355, 0.79686608, 0.39581413],
       [0.7328945 , 0.57456536, 0.18012771, 0.81519295, 0.01466615,
        0.03374568, 0.40865905, 0.70241457, 0.30494482, 0.63398954],
       [0.46405281, 0.5332063 , 0.18604028, 0.26061367, 0.76300291,
        0.62996246, 0.21587994, 0.32441372, 0.63741871, 0.24269805],
       [0.43275035, 0.41381104, 0.4914946 , 0.47221879, 0.07892972,
        0.53239343, 0.55639538, 0.62165555, 0.48979182, 0.94992944],
       [0.44856271, 0.63900666, 0.1354305 , 0.21257494, 0.93571004,
        0.27395649, 0.68330413, 0.06238116, 0.60970981, 0.23192754]])
&amp;gt;&amp;gt;&amp;gt; a.tofile(&quot;a.bin&quot;)
&amp;gt;&amp;gt;&amp;gt; b = np.fromfile(&quot;a.bin&quot;)
&amp;gt;&amp;gt;&amp;gt; b.shape
(100,)
&amp;gt;&amp;gt;&amp;gt; b.reshape((10,10))
array([[0.59281993, 0.3239744 , 0.40916617, 0.7026653 , 0.73275024,
        0.21081999, 0.53366245, 0.10331709, 0.32546868, 0.22810421],
       [0.98428505, 0.94670514, 0.63903532, 0.27788937, 0.88567724,
        0.37963127, 0.44450239, 0.69552021, 0.9621253 , 0.03284991],
       [0.42321447, 0.05881487, 0.06551279, 0.2043375 , 0.88272984,
        0.24789873, 0.87246889, 0.10402341, 0.12713767, 0.33367603],
       [0.11624641, 0.75637676, 0.20271353, 0.5155519 , 0.36666372,
        0.39928505, 0.30223019, 0.86106991, 0.17976945, 0.83550575],
       [0.98105967, 0.81721926, 0.77524547, 0.36720453, 0.94334179,
        0.59794438, 0.98942932, 0.0531472 , 0.23519734, 0.7861395 ],
       [0.4437484 , 0.50221219, 0.52620174, 0.62602009, 0.91305105,
        0.98763546, 0.24418486, 0.56778355, 0.79686608, 0.39581413],
       [0.7328945 , 0.57456536, 0.18012771, 0.81519295, 0.01466615,
        0.03374568, 0.40865905, 0.70241457, 0.30494482, 0.63398954],
       [0.46405281, 0.5332063 , 0.18604028, 0.26061367, 0.76300291,
        0.62996246, 0.21587994, 0.32441372, 0.63741871, 0.24269805],
       [0.43275035, 0.41381104, 0.4914946 , 0.47221879, 0.07892972,
        0.53239343, 0.55639538, 0.62165555, 0.48979182, 0.94992944],
       [0.44856271, 0.63900666, 0.1354305 , 0.21257494, 0.93571004,
        0.27395649, 0.68330413, 0.06238116, 0.60970981, 0.23192754]])
&amp;gt;&amp;gt;&amp;gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;也可以&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a.tofile(&quot;a.txt&quot;)
&amp;gt;&amp;gt;&amp;gt; b = np.fromfile(&quot;a.txt&quot;)
&amp;gt;&amp;gt;&amp;gt; b.reshape((10,10))
array([[0.59281993, 0.3239744 , 0.40916617, 0.7026653 , 0.73275024,
        0.21081999, 0.53366245, 0.10331709, 0.32546868, 0.22810421],
       [0.98428505, 0.94670514, 0.63903532, 0.27788937, 0.88567724,
        0.37963127, 0.44450239, 0.69552021, 0.9621253 , 0.03284991],
       [0.42321447, 0.05881487, 0.06551279, 0.2043375 , 0.88272984,
        0.24789873, 0.87246889, 0.10402341, 0.12713767, 0.33367603],
       [0.11624641, 0.75637676, 0.20271353, 0.5155519 , 0.36666372,
        0.39928505, 0.30223019, 0.86106991, 0.17976945, 0.83550575],
       [0.98105967, 0.81721926, 0.77524547, 0.36720453, 0.94334179,
        0.59794438, 0.98942932, 0.0531472 , 0.23519734, 0.7861395 ],
       [0.4437484 , 0.50221219, 0.52620174, 0.62602009, 0.91305105,
        0.98763546, 0.24418486, 0.56778355, 0.79686608, 0.39581413],
       [0.7328945 , 0.57456536, 0.18012771, 0.81519295, 0.01466615,
        0.03374568, 0.40865905, 0.70241457, 0.30494482, 0.63398954],
       [0.46405281, 0.5332063 , 0.18604028, 0.26061367, 0.76300291,
        0.62996246, 0.21587994, 0.32441372, 0.63741871, 0.24269805],
       [0.43275035, 0.41381104, 0.4914946 , 0.47221879, 0.07892972,
        0.53239343, 0.55639538, 0.62165555, 0.48979182, 0.94992944],
       [0.44856271, 0.63900666, 0.1354305 , 0.21257494, 0.93571004,
        0.27395649, 0.68330413, 0.06238116, 0.60970981, 0.23192754]])

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;npappend&quot;&gt;np.append()&lt;/h3&gt;

&lt;p&gt;这个相当于数组的拼接，因为有时候都在和numpy打交道，所以这个函数还是很方便的。&lt;/p&gt;

&lt;p&gt;见下面的几个例子&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import numpy as np
&amp;gt;&amp;gt;&amp;gt; a = np.array([1,2,3])
&amp;gt;&amp;gt;&amp;gt; np.append(a,[4])
array([1, 2, 3, 4])
&amp;gt;&amp;gt;&amp;gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a = np.zeros((0,3))
&amp;gt;&amp;gt;&amp;gt; np.append(a, [1,2,3])
array([1., 2., 3.])

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; a = np.append([1,2,3], [[3,4,5], [6,7,8]])
&amp;gt;&amp;gt;&amp;gt; a
array([1, 2, 3, 3, 4, 5, 6, 7, 8])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; np.append([[1,2,3],[4,5,6]], [7,8,9])
array([1, 2, 3, 4, 5, 6, 7, 8, 9])

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; np.append([[1,2,3],[4,5,6]], [[7,8,9]], axis=0)
array([[1, 2, 3],
       [4, 5, 6],
       [7, 8, 9]])

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;npexpand_dims&quot;&gt;np.expand_dims()&lt;/h3&gt;

&lt;p&gt;扩展一个维度，从低维到高维&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a = np.ones((3,4))
&amp;gt;&amp;gt;&amp;gt; a
array([[1., 1., 1., 1.],
       [1., 1., 1., 1.],
       [1., 1., 1., 1.]])
&amp;gt;&amp;gt;&amp;gt; np.expand_dims(a, axis=0)
array([[[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]]])
&amp;gt;&amp;gt;&amp;gt; a = np.expand_dims(a, axis=0)
&amp;gt;&amp;gt;&amp;gt; a.shape
(1, 3, 4)
&amp;gt;&amp;gt;&amp;gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
        <pubDate>Mon, 24 Dec 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/12/numpy/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/12/numpy/</guid>
        
        <category>python</category>
        
        
        <category>python</category>
        
      </item>
    
      <item>
        <title>产生anchor的机制</title>
        <description>&lt;!--more--&gt;

&lt;p&gt;在当前的目标检测算法中，几乎都用到了anchor的思想，所以对于具体如何产生anchor的也要有个清晰的认识，因为这关系到产生多少anchor,产生多大的anchor，这在不同的问题上是不同的，比如如果一个图上面要检测的目标只有一个的话，而且这个目标都在图的中心，那么这时候就没有必要产生那么多的anchor。&lt;/p&gt;

&lt;p&gt;产生anchor有两种思路，一种是一下子产生完，另外一种是先产生一个位置的anchor，然后再产生其它位置的anchor，而产生其它位置的anchor的时候其实就是做一个平移就可以了。下面以第二种办法，来详细地看一下代码。&lt;/p&gt;

&lt;h3 id=&quot;第一步产生一个以00为中心的anchor&quot;&gt;第一步，产生一个以(0,0)为中心的anchor&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
import numpy as np

def generate_anchors(base_size=16, ratios=None, scales=None):
    &quot;&quot;&quot; 
    Generate anchor (reference) windows by enumerating aspect ratios X
    scales w.r.t. a reference window.
    &quot;&quot;&quot;

    if ratios is None:
        ratios = np.array([0.5, 1, 2]) 

    if scales is None:
        scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])

    num_anchors = len(ratios) * len(scales)   # 这是每个地方产生的anchors

    # initialize output anchors
    anchors = np.zeros((num_anchors, 4)) 
    # scale base_size
    anchors[:, 2:] = base_size * np.tile(scales, (2, len(ratios))).T   # 右边是(9,2)的shape， 和左边是一样的，相当于是anchors的大小。

    # compute areas of anchors
    areas = anchors[:, 2] * anchors[:, 3]   # 每个anchor的面积

    # correct for ratios    # 要*上对应的ratio  w/ratio   h*ratio.
    anchors[:, 2] = np.sqrt(areas / np.repeat(ratios, len(scales)))   # 左边是(9,) 右边也是(9,)的。  
    anchors[:, 3] = anchors[:, 2] * np.repeat(ratios, len(scales))

    # 上面得到的是[0,0,w,h]  
    # transform from (x_ctr, y_ctr, w, h) -&amp;gt; (x1, y1, x2, y2)
    # 下面把中心变化到小格子的中心，要减去这个anchor的w/2和h/2.
    anchors[:, 0::2] -= np.tile(anchors[:, 2] * 0.5, (2, 1)).T
    anchors[:, 1::2] -= np.tile(anchors[:, 3] * 0.5, (2, 1)).T
    # 最后得到的是以(0,0)为中心的bbox.其信息是左上角和右下角。
    return anchors


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;里面的每一步已经解释清晰，
最终产生的anchor就是以(0,0)为中心的，函数入口处的参数&lt;code class=&quot;highlighter-rouge&quot;&gt;base_size&lt;/code&gt;是要产生anchor的feature map的大小，由此可见在这时候产生的anchor还只是相对于feature map的大小，而不是原图的大小。&lt;/p&gt;

&lt;h3 id=&quot;以上面产生的anchor为基础进行平移得到整个feature-map上的anchors&quot;&gt;以上面产生的anchor为基础进行平移得到整个feature map上的anchors&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def shift(shape, stride, anchors):   # shape是(10,8)  # 并不是图的shape, 而是要产生多少行和多少列的anchors
    shift_x = (np.arange(0, shape[1]) + 0.5) * stride
    shift_y = (np.arange(0, shape[0]) + 0.5) * stride

    shift_x, shift_y = np.meshgrid(shift_x, shift_y)

    shifts = np.vstack((
        shift_x.ravel(), shift_y.ravel(),
        shift_x.ravel(), shift_y.ravel()
    )).transpose()

    # add A anchors (1, A, 4) to
    # cell K shifts (K, 1, 4) to get
    # shift anchors (K, A, 4)
    # reshape to (K*A, 4) shifted anchors
    A = anchors.shape[0]
    K = shifts.shape[0]
    all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))
    all_anchors = all_anchors.reshape((K * A, 4))

    return all_anchors

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;其中传入的shape是下面的办法产生的，相当于&lt;code class=&quot;highlighter-rouge&quot;&gt;种几行菜&lt;/code&gt;的感觉。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in self.pyramid_levels]

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;«««&amp;lt; HEAD
而且上面的乘以stride之后就转成了相对于原图的位置和中心。所以经过这一步之后最终产生的anchor依然是相对于原图的位置。估计这样做的好处是在测试的时候方便。这一点要注意，虽然中间经过了一些其它的操作，但是最终拿去用的anchor是相对于原图的。
=======
而且上面的乘以stride之后就转成了相对于原图的位置和中心。所以经过这一步之后最终产生的anchor依然是相对于原输入的位置。估计这样做的好处是在测试的时候方便。这一点要注意，虽然中间经过了一些其它的操作，但是最终拿去用的anchor是相对于原输入的。&lt;/p&gt;

&lt;h3 id=&quot;注意上面的原输入指的是输入进网络的并不是原始的图&quot;&gt;注意，上面的原输入指的是输入进网络的,并不是原始的图，&lt;/h3&gt;

&lt;p&gt;注意，产生的anchor最终都会转化为相对于网络的输入的大小，原始标注的gt信息也会在进入网络的时候转成相对于网络输入的大小的，这样这两个信息就一致了，然后就可以去做回归的gt了。&lt;/p&gt;
&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;blockquote&gt;
      &lt;blockquote&gt;
        &lt;blockquote&gt;
          &lt;blockquote&gt;
            &lt;blockquote&gt;
              &lt;p&gt;6d923d60bf3b199fe698abbbc29f0262d41d8345&lt;/p&gt;
            &lt;/blockquote&gt;
          &lt;/blockquote&gt;
        &lt;/blockquote&gt;
      &lt;/blockquote&gt;
    &lt;/blockquote&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

</description>
        <pubDate>Mon, 24 Dec 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/12/anchor/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/12/anchor/</guid>
        
        <category>torch</category>
        
        
        <category>torch</category>
        
      </item>
    
      <item>
        <title>pytorch-cuda和c++的extensions</title>
        <description>&lt;!--more--&gt;

&lt;p&gt;这次是基于最新的pytorch 1.0.0 版本的来看的。&lt;/p&gt;

&lt;p&gt;先以官方的教程为例子来试一下。&lt;/p&gt;

&lt;h3 id=&quot;c-extension&quot;&gt;c++ extension&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;第一步写&lt;code class=&quot;highlighter-rouge&quot;&gt;.cpp&lt;/code&gt;文件&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#include &amp;lt;torch/torch.h&amp;gt;
#include &amp;lt;iostream&amp;gt;
#include &amp;lt;vector&amp;gt;

at::Tensor d_sigmoid(at::Tensor z) {
  auto s = at::sigmoid(z);
  return (1 - s) * s;
}


std::vector&amp;lt;at::Tensor&amp;gt; lltm_forward(
    at::Tensor input,
    at::Tensor weights,
    at::Tensor bias,
    at::Tensor old_h,
    at::Tensor old_cell) {
  auto X = at::cat({old_h, input}, /*dim=*/1);

  auto gate_weights = at::addmm(bias, X, weights.transpose(0, 1));
  auto gates = gate_weights.chunk(3, /*dim=*/1);

  auto input_gate = at::sigmoid(gates[0]);
  auto output_gate = at::sigmoid(gates[1]);
  auto candidate_cell = at::elu(gates[2], /*alpha=*/1.0);

  auto new_cell = old_cell + candidate_cell * input_gate;
  auto new_h = at::tanh(new_cell) * output_gate;

  return {new_h,
          new_cell,
          input_gate,
          output_gate,
          candidate_cell,
          X,
          gate_weights};
}

at::Tensor d_tanh(at::Tensor z) {
  return 1 - z.tanh().pow(2);
}

// elu'(z) = relu'(z) + { alpha * exp(z) if (alpha * (exp(z) - 1)) &amp;lt; 0, else 0}
at::Tensor d_elu(at::Tensor z, at::Scalar alpha = 1.0) {
  auto e = z.exp();
  auto mask = (alpha * (e - 1)) &amp;lt; 0;
  return (z &amp;gt; 0).type_as(z) + mask.type_as(z) * (alpha * e);
}

std::vector&amp;lt;at::Tensor&amp;gt; lltm_backward(
    at::Tensor grad_h,
    at::Tensor grad_cell,
    at::Tensor new_cell,
    at::Tensor input_gate,
    at::Tensor output_gate,
    at::Tensor candidate_cell,
    at::Tensor X,
    at::Tensor gate_weights,
    at::Tensor weights) {
  auto d_output_gate = at::tanh(new_cell) * grad_h;
  auto d_tanh_new_cell = output_gate * grad_h;
  auto d_new_cell = d_tanh(new_cell) * d_tanh_new_cell + grad_cell;

  auto d_old_cell = d_new_cell;
  auto d_candidate_cell = input_gate * d_new_cell;
  auto d_input_gate = candidate_cell * d_new_cell;


auto gates = gate_weights.chunk(3, /*dim=*/1);
  d_input_gate *= d_sigmoid(gates[0]);
  d_output_gate *= d_sigmoid(gates[1]);
  d_candidate_cell *= d_elu(gates[2]);

  auto d_gates =
     at::cat({d_input_gate, d_output_gate, d_candidate_cell}, /*dim=*/1);

  auto d_weights = d_gates.t().mm(X);
  auto d_bias = d_gates.sum(/*dim=*/0, /*keepdim=*/true);

  auto d_X = d_gates.mm(weights);
  const auto state_size = grad_h.size(1);
  auto d_old_h = d_X.slice(/*dim=*/1, 0, state_size);
  auto d_input = d_X.slice(/*dim=*/1, state_size);

  return {d_old_h, d_input, d_weights, d_bias, d_old_cell};
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
  m.def(&quot;forward&quot;, &amp;amp;lltm_forward, &quot;LLTM forward&quot;);
  m.def(&quot;backward&quot;, &amp;amp;lltm_backward, &quot;LLTM backward&quot;);
}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;注意最后的&lt;code class=&quot;highlighter-rouge&quot;&gt;PYBIND11&lt;/code&gt;那些不要忘记了，其作用是告诉在外边该怎么调这个接口，比如最后&lt;code class=&quot;highlighter-rouge&quot;&gt;lltm.forward&lt;/code&gt; 通过这里的指向，就会调用&lt;code class=&quot;highlighter-rouge&quot;&gt;lltm_forward&lt;/code&gt;这个函数，后面加引号的是对其的描述。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;第二步写setup.py&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from setuptools import setup
from torch.utils.cpp_extension import CppExtension, BuildExtension
import setuptools
import torch
setup(name='lltm',
      ext_modules=[CppExtension('lltm', ['lltm.cpp'])],
     cmdclass={'build_ext': BuildExtension})


&quot;&quot;&quot;
setuptools.Extension(
   name='lltm',
   sources=['lltm.cpp'],
   include_dirs=torch.utils.cpp_extension.include_paths(),
   language='c++')
&quot;&quot;&quot;


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;setup.py&lt;/code&gt;有两种方式可以用，上面的一种是会生成一些东西，而下面的不会，先以上面的为例，写好之后&lt;/p&gt;

&lt;p&gt;运行&lt;code class=&quot;highlighter-rouge&quot;&gt;python setup.py install&lt;/code&gt;
这时候结果如下&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;running install
running bdist_egg
running egg_info
writing lltm.egg-info/PKG-INFO
writing dependency_links to lltm.egg-info/dependency_links.txt
writing top-level names to lltm.egg-info/top_level.txt
reading manifest file 'lltm.egg-info/SOURCES.txt'
writing manifest file 'lltm.egg-info/SOURCES.txt'
installing library code to build/bdist.linux-x86_64/egg
running install_lib
running build_ext
building 'lltm' extension
gcc -pthread -B /home/pengkun/anaconda3/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/pengkun/anaconda3/lib/python3.6/site-packages/torch/lib/include -I/home/pengkun/anaconda3/lib/python3.6/site-packages/torch/lib/include/torch/csrc/api/include -I/home/pengkun/anaconda3/lib/python3.6/site-packages/torch/lib/include/TH -I/home/pengkun/anaconda3/lib/python3.6/site-packages/torch/lib/include/THC -I/home/pengkun/anaconda3/include/python3.6m -c lltm.cpp -o build/temp.linux-x86_64-3.6/lltm.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=lltm -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11
cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
In file included from lltm.cpp:1:0:
/home/pengkun/anaconda3/lib/python3.6/site-packages/torch/lib/include/torch/csrc/api/include/torch/torch.h:7:2: warning: #warning &quot;Including torch/torch.h for C++ extensions is deprecated. Please include torch/extension.h&quot; [-Wcpp]
 #warning \
  ^
g++ -pthread -shared -B /home/pengkun/anaconda3/compiler_compat -L/home/pengkun/anaconda3/lib -Wl,-rpath=/home/pengkun/anaconda3/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/lltm.o -o build/lib.linux-x86_64-3.6/lltm.cpython-36m-x86_64-linux-gnu.so
creating build/bdist.linux-x86_64/egg
copying build/lib.linux-x86_64-3.6/lltm.cpython-36m-x86_64-linux-gnu.so -&amp;gt; build/bdist.linux-x86_64/egg
creating stub loader for lltm.cpython-36m-x86_64-linux-gnu.so
byte-compiling build/bdist.linux-x86_64/egg/lltm.py to lltm.cpython-36.pyc
creating build/bdist.linux-x86_64/egg/EGG-INFO
copying lltm.egg-info/PKG-INFO -&amp;gt; build/bdist.linux-x86_64/egg/EGG-INFO
copying lltm.egg-info/SOURCES.txt -&amp;gt; build/bdist.linux-x86_64/egg/EGG-INFO
copying lltm.egg-info/dependency_links.txt -&amp;gt; build/bdist.linux-x86_64/egg/EGG-INFO
copying lltm.egg-info/top_level.txt -&amp;gt; build/bdist.linux-x86_64/egg/EGG-INFO
writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt
zip_safe flag not set; analyzing archive contents...
__pycache__.lltm.cpython-36: module references __file__
creating 'dist/lltm-0.0.0-py3.6-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it
removing 'build/bdist.linux-x86_64/egg' (and everything under it)
Processing lltm-0.0.0-py3.6-linux-x86_64.egg
removing '/home/pengkun/anaconda3/lib/python3.6/site-packages/lltm-0.0.0-py3.6-linux-x86_64.egg' (and everything under it)
creating /home/pengkun/anaconda3/lib/python3.6/site-packages/lltm-0.0.0-py3.6-linux-x86_64.egg
Extracting lltm-0.0.0-py3.6-linux-x86_64.egg to /home/pengkun/anaconda3/lib/python3.6/site-packages
lltm 0.0.0 is already the active version in easy-install.pth

Installed /home/pengkun/anaconda3/lib/python3.6/site-packages/lltm-0.0.0-py3.6-linux-x86_64.egg
Processing dependencies for lltm==0.0.0
Finished processing dependencies for lltm==0.0.0

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;没编译之前文件目录结构是&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.
├── lltm.cpp
└── setup.py

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;编译完成后是&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.
├── build
│   ├── bdist.linux-x86_64
│   ├── lib.linux-x86_64-3.6
│   │   └── lltm.cpython-36m-x86_64-linux-gnu.so
│   └── temp.linux-x86_64-3.6
│       └── lltm.o
├── dist
│   └── lltm-0.0.0-py3.6-linux-x86_64.egg
├── lltm.cpp
├── lltm.egg-info
│   ├── dependency_links.txt
│   ├── PKG-INFO
│   ├── SOURCES.txt
│   └── top_level.txt
└── setup.py


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;这时候可以测试一下&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import torch
&amp;gt;&amp;gt;&amp;gt; import lltm
&amp;gt;&amp;gt;&amp;gt; lltm.forward
&amp;lt;built-in method forward of PyCapsule object at 0x7f8c2bc5ef00&amp;gt;
&amp;gt;&amp;gt;&amp;gt; 

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;注意一定是&lt;code class=&quot;highlighter-rouge&quot;&gt;import torch&lt;/code&gt;在前面！！！&lt;/p&gt;

&lt;p&gt;用的时候可以这样使用&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
import lltm
outputs = lltm.forward(input, weights, bias, old_h, old_cell)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;用第二种方法编译&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;第二种方法没有生成东西&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pengkun@ubuntu:~/torch_learn/lltm_extension$ vim setup.py 
pengkun@ubuntu:~/torch_learn/lltm_extension$ python setup.py install
pengkun@ubuntu:~/torch_learn/lltm_extension$ ls
lltm.cpp  setup.py
pengkun@ubuntu:~/torch_learn/lltm_extension$ 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;建议用第一种，因为第二种教程里面没有怎么说。&lt;/p&gt;

&lt;h3 id=&quot;还可以更方便地采取jit的方式just-in-time的方式&quot;&gt;还可以更方便地采取jit的方式（Just in time）的方式&lt;/h3&gt;

&lt;p&gt;这时候不需要像之前那样&lt;code class=&quot;highlighter-rouge&quot;&gt;python setup.py install&lt;/code&gt;
直接用的时候像下面这样用就可以了，不过第一次用的时候会慢，&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from torch.utils.cpp_extension import load
&amp;gt;&amp;gt;&amp;gt; 
&amp;gt;&amp;gt;&amp;gt; lltm = load(name=&quot;lltm&quot;, sources=[&quot;lltm.cpp&quot;])
&amp;gt;&amp;gt;&amp;gt; lltm.forward
&amp;lt;built-in method forward of PyCapsule object at 0x7f524d32e750&amp;gt;
&amp;gt;&amp;gt;&amp;gt; 

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;lltm.forward&lt;/code&gt; 后面跟上输出的数据就可以用了&lt;/p&gt;

&lt;h3 id=&quot;cuda-extensions&quot;&gt;cuda extensions&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;写核函数
这里把主要部分拿出来，核函数的细节省略了，官方的教程上面是有的，&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// lltm_cuda_kernel.cu 
#include &amp;lt;ATen/ATen.h&amp;gt;
#include &amp;lt;cuda.h&amp;gt;
#include &amp;lt;cuda_runtime.h&amp;gt;
#include &amp;lt;vector&amp;gt;

/*

kernel funciton

*/

std::vector&amp;lt;at::Tensor&amp;gt; lltm_cuda_forward(
    at::Tensor input,
    at::Tensor weights,
    at::Tensor bias,
    at::Tensor old_h,
    at::Tensor old_cell)
{ 
  auto X = at::cat({old_h, input}, /*dim=*/1);
  auto gates = at::addmm(bias, X, weights.transpose(0, 1));
  
  const auto batch_size = old_cell.size(0);
  const auto state_size = old_cell.size(1);
  
  auto new_h = at::zeros_like(old_cell);
  auto new_cell = at::zeros_like(old_cell);
  auto input_gate = at::zeros_like(old_cell);
  auto output_gate = at::zeros_like(old_cell);
  auto candidate_cell = at::zeros_like(old_cell);
  
  const int threads = 1024;
  const dim3 blocks((state_size + threads - 1) / threads, batch_size);
  
  AT_DISPATCH_FLOATING_TYPES(gates.type(), &quot;lltm_forward_cuda&quot;, ([&amp;amp;] {
    lltm_cuda_forward_kernel&amp;lt;scalar_t&amp;gt;&amp;lt;&amp;lt;&amp;lt;blocks, threads&amp;gt;&amp;gt;&amp;gt;(
        gates.data&amp;lt;scalar_t&amp;gt;(),
        old_cell.data&amp;lt;scalar_t&amp;gt;(),
        new_h.data&amp;lt;scalar_t&amp;gt;(),
        new_cell.data&amp;lt;scalar_t&amp;gt;(),
        input_gate.data&amp;lt;scalar_t&amp;gt;(),
        output_gate.data&amp;lt;scalar_t&amp;gt;(),
        candidate_cell.data&amp;lt;scalar_t&amp;gt;(),
        state_size);
  }));



/*

kernel function

*/

std::vector&amp;lt;at::Tensor&amp;gt; lltm_cuda_backward(
    at::Tensor grad_h,
    at::Tensor grad_cell,
    at::Tensor new_cell,
    at::Tensor input_gate,
    at::Tensor output_gate,
    at::Tensor candidate_cell,
    at::Tensor X,
    at::Tensor gate_weights,
    at::Tensor weights)
{

  auto d_old_cell = at::zeros_like(new_cell);
  auto d_gates = at::zeros_like(gate_weights);

  const auto batch_size = new_cell.size(0);
  const auto state_size = new_cell.size(1);

  const int threads = 1024;
  const dim3 blocks((state_size + threads - 1) / threads, batch_size);

  AT_DISPATCH_FLOATING_TYPES(X.type(), &quot;lltm_forward_cuda&quot;, ([&amp;amp;] {
    lltm_cuda_backward_kernel&amp;lt;scalar_t&amp;gt;&amp;lt;&amp;lt;&amp;lt;blocks, threads&amp;gt;&amp;gt;&amp;gt;(
        d_old_cell.data&amp;lt;scalar_t&amp;gt;(),
        d_gates.data&amp;lt;scalar_t&amp;gt;(),
        grad_h.contiguous().data&amp;lt;scalar_t&amp;gt;(),
        grad_cell.contiguous().data&amp;lt;scalar_t&amp;gt;(),
        new_cell.contiguous().data&amp;lt;scalar_t&amp;gt;(),
        input_gate.contiguous().data&amp;lt;scalar_t&amp;gt;(),
        output_gate.contiguous().data&amp;lt;scalar_t&amp;gt;(),
        candidate_cell.contiguous().data&amp;lt;scalar_t&amp;gt;(),
        gate_weights.contiguous().data&amp;lt;scalar_t&amp;gt;(),
        state_size);
  }));

  auto d_weights = d_gates.t().mm(X);
  auto d_bias = d_gates.sum(/*dim=*/0, /*keepdim=*/true);

  auto d_X = d_gates.mm(weights);
  auto d_old_h = d_X.slice(/*dim=*/1, 0, state_size);
  auto d_input = d_X.slice(/*dim=*/1, state_size);

  return {d_old_h, d_input, d_weights, d_bias, d_old_cell, d_gates};
}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;注意这里不像上一个blog写的那样进行编译cuda核函数。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;写一个”.cpp”的连接文件&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#include &amp;lt;torch/torch.h&amp;gt;
#include &amp;lt;vector&amp;gt;
// CUDA forward declarations
std::vector&amp;lt;at::Tensor&amp;gt; lltm_cuda_forward(
    at::Tensor input,
    at::Tensor weights,
    at::Tensor bias,
    at::Tensor old_h,
    at::Tensor old_cell);
std::vector&amp;lt;at::Tensor&amp;gt; lltm_cuda_backward(
    at::Tensor grad_h,
    at::Tensor grad_cell,
    at::Tensor new_cell,
    at::Tensor input_gate,
    at::Tensor output_gate,
    at::Tensor candidate_cell,
    at::Tensor X,
    at::Tensor gate_weights,
    at::Tensor weights);
// C++ interface
#define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x &quot; must be a CUDA tensor&quot;)
#define CHECK_CONTIGUOUS(x) AT_ASSERTM(x.is_contiguous(), #x &quot; must be contiguous&quot;)
#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)
std::vector&amp;lt;at::Tensor&amp;gt; lltm_forward(
    at::Tensor input,
    at::Tensor weights,
    at::Tensor bias,
    at::Tensor old_h,
    at::Tensor old_cell) {
  CHECK_INPUT(input);
 CHECK_INPUT(weights);
  CHECK_INPUT(bias);
  CHECK_INPUT(old_h);
  CHECK_INPUT(old_cell);
  return lltm_cuda_forward(input, weights, bias, old_h, old_cell);
}
std::vector&amp;lt;at::Tensor&amp;gt; lltm_backward(
    at::Tensor grad_h,
    at::Tensor grad_cell,
    at::Tensor new_cell,
    at::Tensor input_gate,
    at::Tensor output_gate,
    at::Tensor candidate_cell,
    at::Tensor X,
    at::Tensor gate_weights,
    at::Tensor weights) {
  CHECK_INPUT(grad_h);
  CHECK_INPUT(grad_cell);
  CHECK_INPUT(input_gate);
  CHECK_INPUT(output_gate);
  CHECK_INPUT(candidate_cell);
  CHECK_INPUT(X);
  CHECK_INPUT(gate_weights);
  CHECK_INPUT(weights);

  return lltm_cuda_backward(
      grad_h,
      grad_cell,
      new_cell,
      input_gate,
      output_gate,
      candidate_cell,
      X,

      gate_weights,
      weights);
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
  m.def(&quot;forward&quot;, &amp;amp;lltm_forward, &quot;LLTM forward (CUDA)&quot;);
  m.def(&quot;backward&quot;, &amp;amp;lltm_backward, &quot;LLTM backward (CUDA)&quot;);
}


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;仍然不要忘记最后让外边用的接口，这里其实就是一个如何调核函数的问题，这里面的return 后面调的就是cuda 核函数里面写的那两个函数，不过这里面并没有include 刚才写的文件，可能也是内部已经实现好了吧。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;写编译文件
```
    &lt;h1 id=&quot;setuppy&quot;&gt;setup.py&lt;/h1&gt;
    &lt;p&gt;from setuptools import setup
from torch.utils.cpp_extension import BuildExtension, CUDAExtension&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;setup(
    name=’lltm_cuda’,
    ext_modules=[
        CUDAExtension(‘lltm_cuda’, [
            ‘lltm_cuda.cpp’,
            ‘lltm_cuda_kernel.cu’,
        ])&lt;br /&gt;
    ],&lt;br /&gt;
    cmdclass={
        ‘build_ext’: BuildExtension
    })&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
运行`python setup.py install`， 和之前结果差不多，不过这时候是这样的，因为名字变了，

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;…
…
…
xtracting lltm_cuda-0.0.0-py3.6-linux-x86_64.egg to /home/pengkun/anaconda3/lib/python3.6/site-packages
lltm-cuda 0.0.0 is already the active version in easy-install.pth&lt;/p&gt;

&lt;p&gt;Installed /home/pengkun/anaconda3/lib/python3.6/site-packages/lltm_cuda-0.0.0-py3.6-linux-x86_64.egg
Processing dependencies for lltm-cuda==0.0.0
Finished processing dependencies for lltm-cuda==0.0.0&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
然后可以这样用

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;blockquote&gt;
      &lt;p&gt;import torch
import lltm_cuda
lltm_cuda.forward&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;&amp;lt;built-in method forward of PyCapsule object at 0x7f669bae2b40&amp;gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### jit

当然这次也可以用jit的方式
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;blockquote&gt;
      &lt;p&gt;from torch.utils.cpp_extension import load&lt;/p&gt;

      &lt;p&gt;lltmCuda = load(name=’lltm_cuda’, sources=[‘lltm_cuda.cpp’, ‘lltm_cuda_kernel.cu’])
lltmCuda.forward&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;&amp;lt;built-in method forward of PyCapsule object at 0x7f63a7049f30&amp;gt;&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;h3 id=&quot;后记&quot;&gt;后记&lt;/h3&gt;

&lt;p&gt;下面就准备用这种方式来重写一下之前的roc计算的。&lt;/p&gt;
</description>
        <pubDate>Sun, 23 Dec 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/12/roc-torch2/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/12/roc-torch2/</guid>
        
        <category>cuda</category>
        
        
        <category>CUDA</category>
        
      </item>
    
      <item>
        <title>梯度clip</title>
        <description>&lt;!--more--&gt;

&lt;p&gt;在训练模型的时候有时候会出现这种情况，即loss已经很小了，几乎接近于0，但是模型并不是太好，我之前就遇到过这个问题。分析了之后觉得是因为loss小的时候梯度也很小，这样就会导致学不到东西。
而梯度clip是一种解决的办法，它是为了防止梯度太大或者梯度太小的时候而做的，就像&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.clamp&lt;/code&gt; 的想法一样，如果小于某个阈值的话，为了不让其太小了，就人为地规定一个值，让其以这个值传下去，&lt;/p&gt;

&lt;p&gt;在torch里面可以这样用。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.nn.utils.clip_grad_norm&lt;/code&gt; 里面指定阈值就可以了。而且可以指定哪些参数。&lt;/p&gt;

</description>
        <pubDate>Sun, 23 Dec 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/12/clip-grid/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/12/clip-grid/</guid>
        
        <category>torch</category>
        
        
        <category>torch</category>
        
      </item>
    
      <item>
        <title>pytorch-cuda-计算roc</title>
        <description>&lt;!--more--&gt;

&lt;p&gt;之前实现了用mxnet调用cuda,而且也画出过roc,现在用pytorch来试一下，感觉过程要复杂一些，代码量也多一些，不过总结一下，为了方便以后写pytorch与cuda的接口时用，并且记录一下出bug的过程。&lt;/p&gt;

&lt;h3 id=&quot;第一步写cuda核函数&quot;&gt;第一步写cuda核函数&lt;/h3&gt;

&lt;p&gt;代码如下，主题部分和上一个blog是一样的。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#ifdef __cplusplus
extern &quot;C&quot; {
#endif

#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;float.h&amp;gt;
#include &quot;roc_kernel.h&quot;
#define NUM 1024

__global__ void roc_kernel(const float *score, const int q_num, const int g_num, float *histo1, float *histo2, const float *labels_q, const float *labels_g){
    __shared__ float temp1[NUM];
    __shared__ float temp2[NUM];

    //initialize
    temp1[threadIdx.x]=0;
    temp2[threadIdx.x]=0;
    //syncthreads
    __syncthreads();

    int j = threadIdx.x + blockIdx.x * blockDim.x;
    int stride = blockDim.x * gridDim.x;
    
    while(j&amp;lt;q_num*g_num){
        atomicAdd(&amp;amp;(temp1[(int)(1.0*(score[j]+1)/2*(NUM-1))]), 1); 
        atomicAdd(&amp;amp;(temp2[(int)(1.0*(score[j]+1)/2*(NUM-1))]), labels_q[j/g_num]==labels_g[j%g_num]);
        j += stride;
    }

    __syncthreads();

    atomicAdd(&amp;amp;(histo1[threadIdx.x]), temp1[threadIdx.x]);
    atomicAdd(&amp;amp;(histo2[threadIdx.x]), temp2[threadIdx.x]);

}

void _roc(const float *score, const int q_num, const int g_num, float *histo1, float *histo2, const float *labels_q, const float *labels_g){
    roc_kernel&amp;lt;&amp;lt;&amp;lt;1024, 1024&amp;gt;&amp;gt;&amp;gt;(score, q_num, g_num, histo1, histo2, labels_q, labels_g);
}


#ifdef __cplusplus
}
#endif


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这个过程写的比较多了，主要是运算搞清晰，我这里传入的参数有些多了，其实不需要那么多的，
不同的是，之前的代码中&lt;code class=&quot;highlighter-rouge&quot;&gt;histo1,histo2,labelq,labelg&lt;/code&gt;是&lt;code class=&quot;highlighter-rouge&quot;&gt;int *&lt;/code&gt;的，而现在是&lt;code class=&quot;highlighter-rouge&quot;&gt;float *&lt;/code&gt;的了，这里就有一个bug,我最初用&lt;code class=&quot;highlighter-rouge&quot;&gt;int *&lt;/code&gt;的时候，就出了bug,好像意思是&lt;code class=&quot;highlighter-rouge&quot;&gt;THCudaTensor&lt;/code&gt;的类型需要是&lt;code class=&quot;highlighter-rouge&quot;&gt;float&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;上面代码中的&lt;code class=&quot;highlighter-rouge&quot;&gt;void _roc&lt;/code&gt;是方便外面调用的。
其中头文件&lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;roc_kernel.h&quot;&lt;/code&gt;里面写的是&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#ifndef _ROC_KERNEL
#define _ROC_KERNEL

#ifdef __cplusplus
extern &quot;C&quot; {
#endif

void _roc(const float *score, const int q_num, const int g_num, float *histo1, float *histo2, const float *labels_q, const float *labels_g);

#ifdef __cplusplus
}
#endif


#endif


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;上面的两个写好之后编译生成可执行文件&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nvcc -c -o roc_kernel roc_kernel.cu -x cu -Xcompiler -fPIC -arch=sm_61&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;其中最后一个参数是计算能力的参数，不同的GPU参数不一样。&lt;/p&gt;

&lt;h3 id=&quot;写cuda与pytorch的连接代码&quot;&gt;写cuda与pytorch的连接代码&lt;/h3&gt;

&lt;p&gt;这在mxnet中是没有这处过程的，可能是mxnet内部已经实现好了吧。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#include &amp;lt;THC/THC.h&amp;gt;
#include &amp;lt;TH/TH.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;

#include &quot;cuda/roc_kernel.h&quot;


extern THCState *state;

void gpu_roc(THCudaTensor *score, THCudaTensor *histo1, THCudaTensor *histo2, THCudaTensor *labels_q, THCudaTensor *labels_g){
    THArgCheck(THCudaTensor_isContiguous(state, score), 0, &quot;must be contiguous&quot;);
    THArgCheck(THCudaTensor_isContiguous(state, histo1), 1, &quot;must be contiguous&quot;);
    THArgCheck(THCudaTensor_isContiguous(state, histo2), 2, &quot;must be contiguous&quot;);
    THArgCheck(THCudaTensor_isContiguous(state, labels_q), 3, &quot;must be contiguous&quot;);
    THArgCheck(THCudaTensor_isContiguous(state, labels_g), 4, &quot;must be contiguous&quot;);

    const int q_num = THCudaTensor_size(state, score, 0); 
    const int g_num = THCudaTensor_size(state, score, 1); 

    printf(&quot;nums of query: %d\n&quot;, q_num);
    printf(&quot;nums of gallery: %d\n&quot;, g_num);


    _roc(THCudaTensor_data(state, score), q_num, g_num,
        THCudaTensor_data(state, histo1),
        THCudaTensor_data(state, histo2),
        THCudaTensor_data(state, labels_q),
        THCudaTensor_data(state, labels_g));



}
~                  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这里面最后传参的时候最后的&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;THCudaTensor_data(state, score)&lt;/code&gt; 可以先用一个&lt;code class=&quot;highlighter-rouge&quot;&gt;float &lt;/code&gt;去接，然后 再传写行&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;float* score_data = THCudaTensor_data(state, score)&lt;/code&gt;， 然后传入&lt;code class=&quot;highlighter-rouge&quot;&gt;score_data&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;其中头文件中就一句话&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;void gpu_roc(THCudaTensor *score, THCudaTensor *histo1, THCudaTensor *histo2, THCudaTensor *labels    _q, THCudaTensor *labels_g);

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;build&quot;&gt;build&lt;/h3&gt;

&lt;p&gt;写一个&lt;code class=&quot;highlighter-rouge&quot;&gt;build.py&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;里面是&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import os
import torch
from torch.utils.ffi import create_extension

if torch.cuda.is_available():
    sources = ['src/roc_cuda.c']
    headers = ['src/roc_cuda.h']
    defines = [('WITH_CUDA', None)]

    with_cuda = True

this_file = os.path.dirname(os.path.realpath(__file__))

print(this_file)
extra_objects = ['src/cuda/roc_kernel']
extra_objects = [os.path.join(this_file, fname) for fname in extra_objects]

ffi = create_extension(
    '_ext.roc',
    headers = headers,
    sources = sources,
    define_macros = defines,
    relative_to = __file__,
    with_cuda=with_cuda,
    extra_objects=extra_objects
    )   

if __name__ == &quot;__main__&quot;:
    ffi.build()


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;至此文件目录是这样的&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.
├── build.py
├── roc11.py
└── src
    ├── cuda
    │   ├── do.sh
    │   ├── roc_kernel
    │   ├── roc_kernel.cu
    │   └── roc_kernel.h
    ├── roc_cuda.c
    └── roc_cuda.h


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;然后运行&lt;code class=&quot;highlighter-rouge&quot;&gt;python build.py&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;出现结果&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/mnt/data1/cuda_roc_pytorch
generating /tmp/tmp2bcmkyqk/_roc.c
setting the current directory to '/tmp/tmp2bcmkyqk'
running build_ext
building '_roc' extension
creating mnt
creating mnt/data1
creating mnt/data1/cuda_roc_pytorch
creating mnt/data1/cuda_roc_pytorch/src
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -m64 -fPIC -fPIC -DWITH_CUDA -I/home/pengkun/anaconda3/envs/py35_tf19/lib/python3.5/site-packages/torch/utils/ffi/../../lib/include -I/home/pengkun/anaconda3/envs/py35_tf19/lib/python3.5/site-packages/torch/utils/ffi/../../lib/include/TH -I/home/pengkun/anaconda3/envs/py35_tf19/lib/python3.5/site-packages/torch/utils/ffi/../../lib/include/THC -I/usr/local/cuda/include -I/home/pengkun/anaconda3/envs/py35_tf19/include/python3.5m -c _roc.c -o ./_roc.o
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -m64 -fPIC -fPIC -DWITH_CUDA -I/home/pengkun/anaconda3/envs/py35_tf19/lib/python3.5/site-packages/torch/utils/ffi/../../lib/include -I/home/pengkun/anaconda3/envs/py35_tf19/lib/python3.5/site-packages/torch/utils/ffi/../../lib/include/TH -I/home/pengkun/anaconda3/envs/py35_tf19/lib/python3.5/site-packages/torch/utils/ffi/../../lib/include/THC -I/usr/local/cuda/include -I/home/pengkun/anaconda3/envs/py35_tf19/include/python3.5m -c /mnt/data1/cuda_roc_pytorch/src/roc_cuda.c -o ./mnt/data1/cuda_roc_pytorch/src/roc_cuda.o
gcc -pthread -shared -L/home/pengkun/anaconda3/envs/py35_tf19/lib -Wl,-rpath=/home/pengkun/anaconda3/envs/py35_tf19/lib,--no-as-needed ./_roc.o ./mnt/data1/cuda_roc_pytorch/src/roc_cuda.o /mnt/data1/cuda_roc_pytorch/src/cuda/roc_kernel -L/home/pengkun/anaconda3/envs/py35_tf19/lib -lpython3.5m -o ./_roc.so

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;此时文件目录变成了&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.
├── build.py
├── _ext
│   ├── __init__.py
│   └── roc
│       ├── __init__.py
│       └── _roc.so
├── roc11.py
└── src
    ├── cuda
    │   ├── do.sh
    │   ├── roc_kernel
    │   ├── roc_kernel.cu
    │   └── roc_kernel.h
    ├── roc_cuda.c
    └── roc_cuda.h



&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;测试&quot;&gt;测试&lt;/h3&gt;

&lt;p&gt;测试代码是&lt;code class=&quot;highlighter-rouge&quot;&gt;roc11.py&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# ===========================================
# --coding:UTF-8 --
# file: roc11.py
# author: ZhengPengkun
# date: 2018-12-22
# email: pkzhengmath@pku.edu.cn
# description: 
# ===========================================
import torch
import numpy as np
import time
import datetime
import matplotlib.pyplot as plt 
import os, sys 
from _ext import roc 
#plt.figure(figsize=(100,80), dpi=100)

num = 1024
def run(file1, file2):
    query = np.load(file1)
    gallery = np.load(file2)
    label_q = query[&quot;label&quot;].astype(np.float32)
    label_g = gallery[&quot;label&quot;].astype(np.float32)
    feature_q = query[&quot;feature&quot;].astype(np.float32)
    feature_g = gallery[&quot;feature&quot;].astype(np.float32)
    assert dim1==dim2
    feature_q = torch.from_numpy(feature_q).cuda()
    feature_g = torch.from_numpy(feature_g).cuda()
    score = torch.matmul(feature_q, torch.t(feature_g))
    label_q = torch.from_numpy(label_q).contiguous().cuda()
    label_g = torch.from_numpy(label_g).contiguous().cuda()
    histo1 = torch.zeros((num,)).contiguous().cuda() 
    histo2 = torch.zeros((num,)).contiguous().cuda()
    roc.gpu_roc(score, histo1, histo2, label_q, label_g) 
    histo1 = histo1.cpu().numpy()
    histo2 = histo2.cpu().numpy()
    num_real_true = histo2.sum()
    num_real_false = q_size*g_size-num_real_true
    print(&quot;num_real_true, false&quot;, num_real_true, num_real_false)
    print(time.time()-start) 
    histo1 = np.cumsum(histo1[::-1])
    histo2 = np.cumsum(histo2[::-1])
    tpr = 1.0*histo2/num_real_true
    fpr = 1.0*(histo1-histo2)/num_real_false
    plt.plot(fpr, tpr, 'b')  
    plt.xlim([1e-8,1.01])
    plt.ylim([-0.01, 1.01])
    plt.xscale(&quot;log&quot;)
    plt.xlabel(&quot;fpr&quot;)
    plt.ylabel(&quot;tpr&quot;)
    plt.grid(True)
    plt.savefig(datetime.datetime.now().strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;)+&quot;.png&quot;)
    #plt.show()
if __name__ == &quot;__main__&quot;:
    start = time.time()
    run(sys.argv[1], sys.argv[2])
                                             

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这里面的bug之前一直没发现，是numpy读出来的时候是&lt;code class=&quot;highlighter-rouge&quot;&gt;float64&lt;/code&gt;的，而需要传入的是&lt;code class=&quot;highlighter-rouge&quot;&gt;float32&lt;/code&gt;的，所以之前的错总是说&lt;code class=&quot;highlighter-rouge&quot;&gt;cuda 越界之类的&lt;/code&gt;， 但是check了好几遍也没有发现代码里面有哪个地方越界了。今天才想到会不会是数据不匹配导致的，然后发现果然是这个问题，从cv里面读出来的图像虽是numpy也不能直接传进去，也是这个道理。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;#plt.figure()&lt;/code&gt; 注释掉的那里，虽然显示的图较大，但是会降低速度，而且不是一点点，有好几秒。&lt;/p&gt;

&lt;p&gt;算&lt;code class=&quot;highlighter-rouge&quot;&gt;fpr和tpr&lt;/code&gt;的也可以放在&lt;code class=&quot;highlighter-rouge&quot;&gt;roc_cuda.c&lt;/code&gt;里面完成，但是numpy的&lt;code class=&quot;highlighter-rouge&quot;&gt;np.cumsum&lt;/code&gt;比较高效也比较方便。
最终测试的结果&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;num_real_true, false 10934.0 212054770.0
2.1989805698394775

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;上个blog中再算的时候，我是直接把&lt;code class=&quot;highlighter-rouge&quot;&gt;score&lt;/code&gt;存下来然后算的，这次也计算了&lt;code class=&quot;highlighter-rouge&quot;&gt;score&lt;/code&gt;.
这个结果和&lt;code class=&quot;highlighter-rouge&quot;&gt;mxnet&lt;/code&gt;的结果差不多。&lt;/p&gt;

&lt;h3 id=&quot;其它&quot;&gt;其它&lt;/h3&gt;

&lt;p&gt;上面的是基于&lt;code class=&quot;highlighter-rouge&quot;&gt;pytorch=0.4.0&lt;/code&gt;的来写的，最新的&lt;code class=&quot;highlighter-rouge&quot;&gt;1.0.0&lt;/code&gt;的还没有试过，这是下一步的目标。
当然上面的过程也可以用于pytorch的c++的连接方式。&lt;/p&gt;

</description>
        <pubDate>Sat, 22 Dec 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/12/roc-torch/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/12/roc-torch/</guid>
        
        <category>cuda</category>
        
        
        <category>cuda</category>
        
      </item>
    
  </channel>
</rss>
